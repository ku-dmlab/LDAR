<paper 0>
# Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING 

Yichao Fu ${ }^{1}$ Peter Bailis ${ }^{2}$ Ion Stoica ${ }^{3}$ Hao Zhang ${ }^{1}$


#### Abstract

Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce LOOKAHEAD DECODING, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step $\log$ (FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of LOOKAHEAD DECODING can speed up autoregressive decoding by up to $1.8 \mathrm{x}$ on MT-bench and $4 \mathrm{x}$ with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://githu.com /hao-ai-lab/LookaheadDecoding


## 1. Introduction

Large language models (LLMs) are transforming the AI industry. As they are increasingly integrated into diverse applications such as search (Team et al., 2023) and chatbots (Ouyang et al., 2022), generating long sequences at low-latency using LLMs is becoming one significant requirement. However, current LLMs generate text based on (Touvron et al., 2023a;b; Jiang et al., 2023; OpenAI, 2023) autoregressive decoding, which falls short in efficiency, primarily for two reasons. First, autoregressive decoding generates only one token at a time. Hence, the overall generation time is proportional to the number of decoding steps. Second, each decoding step largely underutilizes the parallel processing capabilities of modern accelerators (e.g.,[^0]

GPUs). Given the pressing need for low latency in various applications, improving autoregressive decoding remains a central challenge.

Several approaches have been proposed - one such approach is speculative decoding (Chen et al., 2023; Leviathan et al., 2023) and its variants (He et al., 2023; Stern et al., 2018; Cai et al., 2024; Li et al., 2023; Liu et al., 2023; Miao et al., 2023). These methods all follow a guess-and-verify approach: they use a draft model to speculate several subsequent tokens and then use the original (base) LLM to verify these tokens in parallel. Since the draft model requires much fewer resources and the cost of verifying multiple tokens in parallel is similar to the cost of generating a single token, these methods can achieve considerable speedups. However, their speedups are bounded by the token acceptance rate (§4.1), i.e., the fraction of tokens generated by the draft model that passes the verification test of the base model. This is because every token that fails verification needs to be regenerated by the base model. In the worst case, if most proposed tokens fail verification, these methods may slow down the decoding process. Therefore, achieving a high acceptance rate is essential for these methods. Unfortunately, training a draft model to achieve a high acceptance rate is non-trivial, and the trained draft model does not generalize across base models and datasets.

To address these problems, this paper develops LookAHEAD DECODING. We build upon a key observation: autoregressive decoding can be equivalently formulated as solving a non-linear system via the fixed point Jacobi iteration method (§2), which we term as Jacobi decoding (Santilli et al., 2023). Each Jacobi decoding step can generate multiple tokens in parallel at different positions. Although these tokens may appear at incorrect positions, we can leverage this parallel generation approach to have the LLM generate several disjoint $n$-grams in parallel in a single step. These n-grams could potentially be integrated into future parts of the generated sequence, pending verification by the base model to maintain the output distribution.

LOOKAHEAD DECODING takes advantage of the particular characteristics of autoregressive decoding, which is bounded by the memory bandwidth-as each generated token depends on all tokens before it-rather than compute, by
using the available cycles to generate and verify $n$-grams (subsequent tokens) at virtually no additional cost. In a nutshell, LOOKAHEAD DECODING consists of a lookahead branch that generates $n$-grams and a verification branch that verifies $n$-grams, both executing in a single step. To improve efficiency, we use an $n$-gram pool to cache the historical $n$-grams generated so far. This way, LoOKAHEAD DECODING can significantly reduce the latency of LLM inference just by exploiting the compute resources that autoregressive decoding would leave unused. More importantly, LOOKAHEAD DECODING scales with the compute - we show that it can linearly reduce the number of decoding steps relative to the $\log$ (FLOPs) allocated per step.

We have implemented the algorithm in both Python and CUDA, compatible with memory-efficient attention algorithms (e.g., FlashAttention (Dao, 2023)), and supports various sampling methods without changing the output distribution. We also scale it to multiple GPUs, resulting in Lookahead Parallelism. We evaluate LoOKAHEAD DECODING on the popular LLaMA-2 (Touvron et al., 2023b) models. It achieves $1.8 \mathrm{x}$ speedup on the challenging multi-turn chat dataset MT-Bench (Zheng et al., 2023) and up to 4x speedup in code completion tasks with Lookahead Parallelism on 8 GPUs. LOOKAHEAD DECODING showed significant potential in lowering the latency for latency-sensitive tasks. Our contributions are summarized as follows.

- We design LooKaHEAd DeCoding, a new lossless, parallel decoding algorithm to accelerate LLM inference without needing any auxiliary component.
- We reveal LOOKAHEAD DECODING's scaling behavior: it linearly reduces the number of decoding steps according to per-step $\log$ (FLOPs). This enables tradeoffs between the number of decoding steps and per-step FLOPs, making it future-proof.
- We show it benefits from the latest memory-efficient attentions and is easily parallelizable by developing its distributed CUDA implementations.
- We evaluated LOOKAHEAD DECODING and demonstrate its effectiveness under different settings.


## 2. Background

In this section, we formulate both autoregressive and Jacobi decoding from the lens of solving nonlinear systems.

Causal Attention in Decoder Models. Most contemporary LLMs are composed of two core components: token-wise modules (including MLP and normalization (Ba et al., 2016; Zhang \& Sennrich, 2019)) and attention (Vaswani et al., 2023) modules. Tokens interact with each other in the attention modules, while in other token-wise modules, they are processed without exchanging information with each other.
The attention layer encompasses three input elements: query $\mathbf{Q}$, key $\mathbf{K}$, and value $\mathbf{V}$, with the $i$-th token in each denoted as $\mathbf{Q}_{i}, \mathbf{K}_{i}$, and $\mathbf{V}_{i}$, respectively. The attention layer executes the following operation: $\mathbf{O}=\operatorname{softmax}\left(\mathbf{Q K}^{T}\right) \mathbf{V}$. A lower triangular mask applied to $\mathbf{Q K}^{T}$ in causal attentions (specific to decoder models) ensures that $\mathbf{O}_{i}$ is calculated only from $\mathbf{Q}_{i}$ and $\mathbf{K}_{j}, \mathbf{V}_{j}$ where $j \leq i$. Because all other layers in the LLM perform token-wise operations, for any given model input $\mathbf{x}$ and output $\mathbf{o}, \mathbf{o}_{i}(i$-th token in $\mathbf{o}$ ) is exclusively influenced by $\mathbf{x}_{j}$ ( $j$-th token in $\mathbf{x}$ ) where $j \leq i$.

Autoregressive Decoding in LLMs. LLMs take discrete integer sequences as inputs, where each integer represents a token. We notate $\mathbf{x}=\left(x_{1}, x_{2}, \ldots, x_{s}\right) \in \mathbb{N}^{s}$ of length $s$ as the input of the model, and $\mathbf{x}_{1: m}^{t}=\left(x_{1}, x_{2}, \ldots, x_{m}\right)$ to denote a slice of $\mathbf{x}$ of length $m$ at step $t$. LLMs' output characterizes the probability distribution of the next token. The probability for the $s$-th token (i.e., the output of the $s-1$-th token) is decided by all previous input tokens, represented as $P_{M}\left(x_{s} \mid \mathbf{x}_{1: s-1}\right)$. Then, the next token input $x_{s}$ is obtained by sampling from $P_{M}\left(x_{s} \mid \mathbf{x}_{1: s-1}\right)$ using different methods (e.g., greedy, top-K, and top-P (Kool et al., 2020; Holtzman et al., 2020)). When using greedy sampling, the next token is selected by applying an argmax function on $P_{M}$.

We define $\mathbf{x}^{0}$ as the prompt tokens given by the user. The LLM needs to generate an output sequence (of length $m$ ) from $\mathbf{x}^{0}$. Denote $y_{i}$ as the token generated at step $i$. The autoregressive decoding process of $m$ tokens can be seen as solving the following $m$ problems one by one (assume greedy sampling):

$$
\left\{\begin{array}{l}
y_{1}=\operatorname{argmax} P_{M}\left(y_{1} \mid \mathbf{x}^{0}\right)  \tag{1}\\
y_{2}=\operatorname{argmax} P_{M}\left(y_{2} \mid y_{1}, \mathbf{x}^{0}\right) \\
\cdots \\
y_{m}=\operatorname{argmax} P_{M}\left(y_{m} \mid \mathbf{y}_{1: m-1}, \mathbf{x}^{0}\right)
\end{array}\right.
$$

Guess-And-Verify Paradigm. The Guess-And-Verify decoding paradigm speculates multiple potential future tokens and subsequently confirms the correctness of these speculations within a single decoding step. Take speculative decoding with greedy sampling as an example: at step $t$, with the prompt $\mathbf{x}^{0}$ and tokens $\mathbf{y}_{1: t-1}$ generated so far, we can use a draft model to autoregressively generate a draft sequence $\mathbf{y}_{t: t+n-1}$ of length $n$. Because $\mathbf{y}_{t: t+n-1}$ is known a priori, we then use the LLM to solve Eqs 2 in parallel, obtaining $\mathbf{y}_{t: t+n}^{\prime}$. Then, we verify if $y_{t+i}$ is equal to $y_{t+i}^{\prime}$ for each $i$ from $i=0$ to $i=n-1$. If there is a match, we accept this token and proceed; otherwise, we stop checking and drop subsequent tokens. Finally, we update $\mathbf{y}$ with all accepted tokens.

$$
\left\{\begin{array}{l}
y_{t}^{\prime}=\operatorname{argmax} P_{M}\left(y_{t} \mid \mathbf{y}_{1: t-1}, \mathbf{x}^{0}\right)  \tag{2}\\
y_{t+1}^{\prime}=\operatorname{argmax} P_{M}\left(y_{t+1} \mid \mathbf{y}_{1: t}, \mathbf{x}^{0}\right) \\
\cdots \\
y_{t+n}^{\prime}=\operatorname{argmax} P_{M}\left(y_{t+n} \mid \mathbf{y}_{1: t+n-1}, \mathbf{x}^{0}\right)
\end{array}\right.
$$

Figure 1: Workflow of LoOKaHEAd DECODING with $W=5, N=3$, and $G=2$. For each decoding step, we do the following. (1) Generate one token at each position in the lookahead branch; (2) Verify and accept 3-grams (searched from the 3-gram pool) with the verification branch; (3) Collect and cache newly generated 3-grams in the pool from lookahead branch trajectories. (4) Update the lookahead branch to maintain a fixed window size.

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-03.jpg?height=350&width=832&top_left_y=562&top_left_x=186)

As stated in $\S 1$, these approaches depend on a good draft model, which is hard to obtain and cannot generalize.

Jacobi Decoding. By notating $f\left(y_{i}, \mathbf{y}_{1: i-1}, \mathbf{x}^{0}\right)=y_{i}-$ $\operatorname{argmax} P_{M}\left(y_{i} \mid \mathbf{y}_{1: i-1}, \mathbf{x}^{0}\right)$, we can transform Eqs 1 into the following non-linear system of equations (Song et al., 2021; Santilli et al., 2023):

$$
\left\{\begin{array}{l}
f\left(y_{1}, \mathbf{x}^{0}\right)=0  \tag{3}\\
f\left(y_{2}, y_{1}, \mathbf{x}^{0}\right)=0 \\
\cdots \\
f\left(y_{m}, \mathbf{y}_{1: m-1}, \mathbf{x}^{0}\right)=0
\end{array}\right.
$$

We can solve this non-linear system using Jacobi iteration by iteratively updating all $y_{i}$ from a random initial guess $\mathbf{y}^{0}$, along the trajectory $\mathbf{y}^{1}, \ldots, \mathbf{y}^{t}, \ldots$, until converging to the fixed point solution $\mathbf{y}^{m}$. We detail this algorithm, termed as Jacobi decoding, in Appendix Algorithm 1. This process guarantees to return the solution of all $m$ variables $y_{i}$ in at most $m$ iterations, as the very first token of each Jacobi update matches autoregressive decoding. Sometimes, more than one token might be correctly generated in a single iteration, potentially reducing the number of decoding steps. It is worth noting that, as $\mathbf{y}^{t}$ is generated based on the past value $\mathbf{y}^{t-1}$ on the trajectory, any two adjacent tokens from $\mathbf{y}^{t-1}$ and $\mathbf{y}^{t}$ can form a meaningful 2-gram.

Limitations of Jacobi Decoding. Empirically, we observe Jacobi decoding can hardly reduce decoding steps, even if it can generate multiple tokens per step. This is because the generated tokens are often put in the wrong positions of the sequence, and correctly placed tokens are frequently replaced by subsequent Jacobi iterations. These prevent it from achieving wall-clock speedup.

## 3. LOOKAHEAD DECODING

LOOKAHEAD DECODING leverages Jacobi decoding's ability to generate many tokens in one step but addresses its limitation. Fig. 1 illustrates its workflow. The key design in LooKAHEAD DECODING is to keep track of the trajectory of Jacobi decoding and generate $n$-gram from this trajectory. This is achieved by maintaining a fixed-sized $2 \mathrm{D}$ window, with the two dimensions corresponding to the sequence and the time axis, respectively, to generate multiple disjoint $n$-grams from the Jacobi iteration trajectory in parallel. We call this process the lookahead branch. In addition, LOOKAHEAD DECODING introduces an $n$-gram pool to cache these $n$-grams generated along the trajectory. Promising $n$-gram candidates are verified later by a designed verification branch to preserve the LLM's output distribution; if passing verification, those disjoint n-grams are integrated into the sequence. The detailed algorithm is shown in Algorithm 2 in Appendix.

### 3.1. Lookahead Branch

LOOKAHEAD DECODING uses a fixed-sized 2D window for efficient $n$-gram generation. In contrast to the original Jacobi decoding, which only uses the history tokens from the last step (or equivalently, it generates 2-grams), LooKAHEAD DECODING generates many $n$-grams, with $n \geq 2$, in parallel by using the $n-1$ past steps' history tokens, effectively leveraging more information from the trajectory. The fixed-sized 2D window in the lookahead branch is characterized by two parameters: (1) $W$ defines the lookahead size into future token positions to conduct parallel decoding; (2) $N$ defines the lookback steps into the past Jacobi trajectory to retrieve $n$-grams. See Algorithm 2 for a detailed process.

An example of the lookahead branch with $W=5$ and $N=4$ is in Fig. 2 (b), in which we look back $N-1=3$ steps and look ahead 5 tokens for each step. The blue token with the digit 0 is the current step's $(t)$ input, and the orange, green, and red tokens were generated in previous lookahead branches at steps $t-3, t-2$, and $t-1$, respectively. The digit on each token shows its relative position to the current input (i.e., the blue one labeled as 0 ). In the present stage, we perform a modified Jacobi iteration to generate new tokens for all 5 positions, following the trajectory formed by the preceding 3 steps. Once generated, we collect and cache them in the $n$-gram pool $(n=4)$ - for instance, a 4 -gram consists of the orange token at position 1 , the green token at position 2, the red token at position 3, and a newly generated token.

The most outdated tokens in both dimensions (time and sequence) will be removed, and newly generated tokens will be appended to the lookahead branch to maintain a fixed window size for each step. For example, we will remove all orange and green tokens with position 1 in Fig. 2. We then form a new lookahead branch with green tokens with indices $2,3,4,5$, all red tokens, and all newly generated tokens for the next step.

### 3.2. Verification Branch

LOOKAHEAD DECODING preserves the output distribution via its verification branch. We first discuss how to verify in greedy sampling. Recall in speculative decoding: the verification is performed by sending the draft tokens to the LLM to get an output for each draft token, then progressively checking if the last token's corresponding output, generated by the target LLM, exactly matches the draft token itself (\$2). The verification branch in LOOKAHEAD DECODING resembles this process, despite verifying many draft n-gram candidates in parallel. In particular, We first look up from the $n$-gram pool for "promising" $n$-grams - by checking if a $n$-gram starts with a token that exactly matches the last token of the current ongoing sequence. We then use the LLM to verify all these $n$-grams in parallel, following a similar fashion as in speculative decoding. See Algorithm 3 in the Appendix for the detailed procedures.

We next discuss how to support more advanced sampling. Previous research (Miao et al., 2023) has developed efficient tree-based verification for speculative decoding with sampling support, where multiple draft sequences derived from a token tree can be verified in parallel. However, it does not apply to LOOKAHEAD DECODING as our verification works on disjoint $n$-grams instead of trees. We improve it by progressively verifying along the $n$-gram length and removing $n$-grams with mismatched prefixes. Besides, speculative decoding style verification requires the probability distribution where the draft token is sampled to update the probability distribution when the draft token is rejected. Because we store all $n$-grams in a pool instead of discarding them each step, we would need huge memory to store the probability distributions (each of vocabulary size) for the entire $n$-gram pool. The key to overcome this is to leverage the mechanism that the verification is indifferent to how draft tokens were sampled - different sampling methods (e.g., greedy sampling) only influence the acceptance rate but keep the output distribution. We can force greedy sampling at the $n$-gram generation (lookahead branch), in which the probability distribution degenerates into a one-hot vector. Hence we only need to store which token is selected. We elaborate the approach in Algorithm 4, prove its correctness in Appendix B, and verify its quality and speedups in $\S 5.3$.

It is expected to have an increasingly large $n$-gram cache hence a growing verification branch as decoding progresses. We set a cap of $G$ to limit the maximum number of promising candidates run in parallel in the verification branch to manage the verification cost. Empirically we suggest to set $G$ proportional to $W$ to balance generation and verification. In practice, we simply set $G=W$.

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-04.jpg?height=585&width=816&top_left_y=220&top_left_x=1061)

(a) Causal Mask (b) Lookahead Decoding Mask

Figure 2: (a) Causal mask for decoder models. (b) Attention mask for LoOKAHEAD DECODING with $W=5, N=4$, and $G=2$. Digits on tokens indicate relative positions.

### 3.3. Decode, Predict, and Verify in The Same Step

At execution, the lookahead and verification branches can be integrated into one decoding step to leverage parallel processing. This requires a designated attention mask, as shown in Fig. 2 (b). This attention mask is straightforwardly derived following the principle that each token is only visible to the tokens with a larger position index than itself (§2). For example, only the green token at position 5 and all orange tokens are visible to the red token 6 . The tokens in the lookahead branch are not visible to the tokens in the verification branch, and vice versa.

Integration with FlashAttention. FlashAttention (Dao et al., 2022; Dao, 2023) can vastly accelerate the training and inference of LLMs by saving memory I/O on the slow memory hierarchy. It forces a causal mask (e.g., Fig. 2 (a)) to avoid all token interactions outside a lower triangular scope, which is not suitable for LOOKAHEAD DECODING as we take a more subtle attention mask (e.g., Fig. 2 (b)) for different $W, N$, and $G$. To solve this, we hardcode LOoKAHEAD DECODING's attention pattern with adjustable $W$, $N$, and $G$ in FlashAttention. Applying FlashAttention to LOOKAHEAD DECODING brings about $20 \%$ end-to-end speedup compared to a straightforward implementation on top of native PyTorch in our experiments (\$5.2).

### 3.4. Lookahead Parallelism

LOOKAHEAD DECODING is easy to parallelize on multiple GPUs for both lookahead and verification branches. Parallelizing the lookahead branch is achieved by noting that the lookahead computation is composed of several disjoint branches. For example, the branch with green 1 and red 2 tokens does not have interaction with the branch with the tokens green 3 and red 4 in Fig. 2 (b). We can put these
disjoint branches onto different GPUs without introducing communication during the inference computation. Parallelizing the verification branch is done by assigning multiple $n$-gram candidates to different devices. Because the verification of each candidate, by design, is independent of others, this will not cause communication.

Fig. 3 shows an example of parallelizing the lookahead branch and verification branch in Fig. 2 (b) to four GPUs. This workload allocation will have the orange token $0,1,2,3$ and the input token 0 be redundantly placed and computed. However, it can essentially save communication volume during the whole forward pass. We only need to synchronize the generated tokens on each device after the forward pass. We can further scale the $W, N$, and $G$ with multiple GPUs' increased FLOPs to obtain a lower latency according to LOOKAHEAD DECODING's scalability (\$4).

We name this new parallelism as lookahead parallelism (LP). Unlike previous parallelism methods (including pipeline and tensor parallelisms) that shard the model parameters or states across different GPUs, LP maintains an entire copy of the model for each GPU (thus needing more memory) and allows distributing tokens to different GPUs. Hence, LP is advantageous in inference as it introduces near-zero communication per step while existing model parallelism methods (Narayanan et al., 2021; Shoeybi et al., 2019) involve a large communication overhead on the critical path of each decoding step.

## 4. Scaling Law of LOOKAHEAD DECODING

Since LOOKAHEAD DECODING introduces flexible parameters $W$ and $N$ associated with the cost of each parallel decoding step. This section investigates the scaling law between compute FLOPs and the theoretical speedup, and compares it to speculative decoding.

### 4.1. Estimating Speedup for Speculative Decoding

Speculative decoding uses the draft model to speculate one token sequence at each step. We represent the probability of each token in the sequence passing the verification of the LLM by $\beta$ (acceptance rate) and notate its expectation $E(\beta)=\alpha$. If we use the draft model to guess $\gamma$ tokens per step, the expectation of the number of accepted tokens is denoted as (Leviathan et al., 2023):

$$
\begin{equation*}
E(\# \text { tokens })=\frac{1-\alpha^{\gamma+1}}{1-\alpha} \tag{4}
\end{equation*}
$$

Instead of speculating one sequence every time, we would speculate $b$ sequences. We assume that $b$ sequences, each of $\gamma$ tokens, are sampled as each token will have the same acceptance rate of $\beta$. Under this setting, the expectation of the number of accepted tokens is denoted as follows:

$$
\begin{equation*}
E(\# \text { tokens })=(\gamma+1)-\sum_{i=1}^{\gamma}\left(1-\alpha^{i}\right)^{b} \tag{5}
\end{equation*}
$$

See derivations in Appendix C for Eq. 4 and Eq. 5. Note that when $b=1$, Eq. 5 falls back to Eq. 4 .

### 4.2. Estimating Speedup for LOOKAHEAD DECODING

We define the $\mathcal{S}=$ step compression ratio as the number of autoregressive steps divided by the number of LOOKAHEAD DECODING steps to generate the same length of the sequence. As the number of generated tokens equals the autoregressive steps, it can be denoted as:

$$
\begin{equation*}
\mathcal{S}=\frac{\# \text { generated tokens }}{\# \text { LOOKAHEAD DECODING steps }} \tag{6}
\end{equation*}
$$

LOOKAHEAD DECODING speculates $b$ sequences every time as in Eq. 5. In each step, we will search $n$-grams in the pool starting with the current input token and have at most $G$ speculations of length $N-1$. As we set $G=W$ (§3.2), we have $G=W=b$ and $N-1=\gamma$ using the notations in Eq. 5. In practice, we cannot expect each step to have equally good speculations (i.e., acceptance rate with $E(\beta)=\alpha)$. We assume that, on average, for every $f$ step, we have one good speculation with $E$ (\#tokens) tokens accepted, and for the other $f-1$ steps, we fall back to autoregressive decoding due to bad speculations. We use this $f$ to bridge $\mathcal{S}$ and $E(\#$ tokens $)$ per step as follows:

$$
\begin{equation*}
\mathcal{S}=(f-1+E(\# \text { tokens })) / f \tag{7}
\end{equation*}
$$

We can plot the curve indicated by our formulation with one specific setting as in Fig. 4 (b). We find that the trend of our empirical experiments (LLaMA-2-Chat-7B on MT-Bench with $G=W$ as in Fig. 4 (a)) align well with the formulation to some extent. From this formulation, we conclude that we can linearly reduce the number of decoding steps according to per-step $\log (b)$ given a large enough $\gamma$. In contrast to speculative decoding, LOOKAHEAD DECODING will not meet an upper bound indicated in Eq. 4 by simultaneously increasing $\gamma$ and $b$. This reveals the scaling law of LOOKAHEAD DECODING to linearly reduce decoding steps according to per-step $\log$ (FLOPs) given a large enough $N$,
since per-step FLOPs is roughly proportional to the number of input tokens (i.e., $(W+G) *(N-1)$ ). The scaling law also suggests LOOKAHEAD DECODING's strong scaling to multiple GPUs, in which we can obtain an even greater per-token latency reduction by using more FLOPs, which is advantageous for latency-sensitive tasks.

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-06.jpg?height=429&width=846&top_left_y=520&top_left_x=173)

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-06.jpg?height=417&width=420&top_left_y=526&top_left_x=186)

(a)

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-06.jpg?height=412&width=405&top_left_y=523&top_left_x=597)

(b)
Figure 4: (a) Relation of $W, N, G$ and $\mathcal{S}$ for LLaMA-2Chat-7B on MT-Bench. (b) When we assume a setting with $\alpha=0.425$ and $f=3.106$, the trend of our formulation.

## 5. Evaluation Results

Model and testbed. We used various versions of the LLaMA-2 (Touvron et al., 2023b) and CodeLlama (Roziere et al., 2023) models, including the 7B, 13B, 34B, and 70B sizes, on two GPU setups $S 1$ and $S 2$. $S 1$ is equipped with NVIDIA A100 GPUs with 80GB of memory. On $S 1$, the 7B, $13 \mathrm{~B}$, and 34B models are deployed on a single A100, while the 70B model utilizes 2 A100s with pipeline parallelism supported by Accelerate (Gugger et al., 2022). $S 2$ is a DGX machine with 8 NVIDIA A100 GPUs with 40GB memory and NVLink. All models serve with FP16 precision and batch of 1 if not specified (Cai et al., 2024; He et al., 2023).

Datasets. We benchmarked LoOKAHEAD DECODING's performance across a broad spectrum of datasets and tasks. MT-Bench (Zheng et al., 2023) is a diverse set of multi-turn questions with many unique tokens. GSM8K (Cobbe et al., 2021) contains a set of math questions, in which we use the first $1 \mathrm{k}$ questions. HumanEval (Chen et al., 2021) covers both code completion and infilling tasks. We also test on MBPP (Austin et al., 2021) dataset for instruction-based code generation, and on ClassEval (Du et al., 2023) for class-level code completion. To control generation length in code generation tasks, we set the maximum sequence length to 512 and 2,048 on HumanEval and ClassEval, respectively, aligned with prior setups (Ben Allal et al., 2022; Du et al., 2023). Tab. 1 lists detailed settings. In addition, we validate the effectiveness of sampling ( $\$ 3.2$ ) on XSum (Narayan et al., 2018) and CNN/Daily Mail (See et al., 2017) datasets.

Baseline Settings. Our primary baseline is HuggingFace's implementation of greedy search (Wolf et al., 2020). Additionally, we employ FlashAttention (Dao et al., 2022;
Table 1: Experimental settings for $\S 5.1$ and $\S 5.2$.

| SERVER | PARALLEL. | MODEL | MODEL SIZE | DATASET |
| :---: | :---: | :---: | :---: | :---: |
| $S 1$ | W/O LP | LLAMA-2-CHAT | 7B, 13B, 70B | MT-BENCH |
|  |  | CODELLAMA | 7B, 13B, 34B | HUMANEVAL |
|  |  | CODELLAMA-INST | 7B, 13B, 34B | MBPP, GSM8K |
| 32 | W/LP | LLAMA-2-CHAT | 7B, 13B | MT-BENCH |
|  |  | CODELLAMA | 7B, 13B | HUMANEVAL |
|  |  | CODELLAMA-PYTHON | 7B, 13B | CLASSEVAL |

Dao, 2023) as a stronger baseline to assess the performance of FlashAttention empowered LOOKAHEAD DECODING. In distributed settings, we evaluate LP against TP (supported by deepspeed (Aminabadi et al., 2022)) and PP (supported by accelerate (Gugger et al., 2022)). We measure the throughput of single batch inference against these baseline settings (Cai et al., 2024; He et al., 2023).

### 5.1. End-to-end Performance

Fig. 5 shows the end-to-end performance of LOOKAHEAD DECODING when compared with HuggingFace's implementation of greedy search on S1. The used tasks and models are shown in Tab. 1. Across various datasets, LOOKAHEAD DECODING demonstrates a $1.5 \mathrm{x}-2.3 \mathrm{x}$ speedup. Generally, our method exhibits better performance in code completion tasks (e.g., $2.3 \mathrm{x}$ ), given the higher occurrence of repetitive tokens during code completions, making predictions easier. Besides, smaller models also exhibit a higher speedup when compared to larger models. This is because LOOKAHEAD DECODING trades per-step FLOPs with a step compression ratio (§4). A larger model requires more FLOPs and quickly hits the GPU FLOPs cap compared to a smaller model. So, it shows a lower ability to compress decoding steps given the same GPU setting.

### 5.2. Performance with LP and FlashAttention

We evaluated the performance of LOOKAHEAD DECODING with LP and FlashAttention augmentation on $S 2$ with greedy search. The used tasks and models are shown in Tab. 1. The results for the 7B and 13B models are in Fig. 6 and Fig. 7, respectively. FlashAttention speeds up the PyTorch implementation of LOOKAHEAD DECODING by $20 \%$. Notably, FlashAttention-integrated LOOKAHEAD DECODING shows $1.8 \mathrm{x}$ speedups for the 7B model on MT-Bench compared with autoregressive decoding with FlashAttention (i.e., 1.9x vs $1.07 x$ in Fig. 6). We did a strong scaling of the workloads to multiple GPUs for distributed settings (i.e., increasing GPUs but not increasing workloads). The multiple GPU settings of both TP (w/ DeepSpeed) and PP (w/ Accelerate) bring slowdowns (i.e., $0.75 \mathrm{x}-0.82 \mathrm{x}$ ). The results echos DeepSpeed's documentation (dee, 2023). However, with LOOKAHEAD DECODING, we can further utilize the FLOPs of multiple GPUs to reduce the inference latency (e.g., $4 x$ on ClassEval).
![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-07.jpg?height=246&width=1720&top_left_y=218&top_left_x=172)

Figure 5: Throughput of LoOKAHEAD DeCODING on various dataset without FlashAttention and distributed serving
![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-07.jpg?height=864&width=820&top_left_y=560&top_left_x=186)

Figure 6: Throughput of LookaHEad DeCoding with multiple GPUs and FlashAttention for 7B models

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-07.jpg?height=876&width=824&top_left_y=1535&top_left_x=184)

1 GPU w/o flash TP w/o flash LP w/o flash PP w/o flash 1 GPU w/ flash TP w/ flash LP w/ flash PP w/ flash
![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-07.jpg?height=784&width=812&top_left_y=1622&top_left_x=192)

Figure 7: Throughput of LookaHEad Decoding with multiple GPUs and FlashAttention for 13B models
Table 2: Sampling with LookaHEAd Decoding on CNN/Daily Mail and XSum. A temperature (Temp.) of 0.0 equals greedy search. "AR." is autoregressive and "LA." is LOOKAHEAD DECODING. Rouge scores, speedups against autoregressive, and compression ratio $(\mathcal{S})$ are reported.

| ATA | m | ETH | OUG | ROUG |  |  | $\mathcal{S}$ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| CNN. | 1.0 | AR. | 36.55 | 13.20 | 22.68 | $1.00 x$ | $1.00 x$ |
|  |  | LA. | 36 |  | 22.7 | $1.46 \mathrm{x}$ | $1.64 x$ |
|  | 0.0 | AR. | 37.79 | 14.59 | 23.96 | $1.00 x$ | $1.00 x$ |
|  |  | LA. | 37.79 | 14.59 | 23.96 | $1.57 x$ | $1.72 x$ |
| Xsum | 1.0 | A | 19.15 |  | 1284 | $1.00 \mathrm{x}$ | 1.00 |
|  |  | L. | 19.2 | 4.5 | 12.87 | $1.50 x$ | $1.67 x$ |
|  | 0.0 | $\mathrm{AI}$ | 19. | 4.' | 15 | $1.00 \mathrm{x}$ | $1.00 x$ |
|  |  | LA. | 19.39 | 4.79 | 13.06 | $1.60 \mathrm{x}$ | $1.77 \mathrm{x}$ |

Table 3: Compare the effectiveness of both lookahead and verification branch on MT-Bench on A100. FlashAttention is activated. We show the speedups against autoregressive decoding and the compression ratio $(\mathcal{S})$.

| TAG | SEtTing (N, W, G) | Prompt as Ref. | SpeEduPS | $\mathcal{S}$ |
| :---: | :---: | :---: | :---: | :---: |
| (1) | AUtoreGRessive | $\boldsymbol{x}$ | $1.00 \mathrm{x}$ | 1.00 |
| (2) | Prompt LoOKUP | $\boldsymbol{\checkmark}$ | $1.44 \mathrm{x}$ | 1.55 |
| (3) | $(10,1,3)$ | $\boldsymbol{\checkmark}$ | $1.36 \mathrm{x}$ | 1.45 |
| (4) | $(5,1,10)$ | $\mathbf{\checkmark}$ | $1.36 \mathrm{x}$ | 1.51 |
| (5) | $(5,1,30)$ | $\boldsymbol{x}$ | $1.04 \mathrm{x}$ | 1.12 |
| (6) | $(5,1,30)$ | $\mathbf{\checkmark}$ | $1.46 \mathrm{x}$ | 1.59 |
| (7) | $(5,30,1)$ | $\mathbf{x}$ | $1.61 \mathrm{x}$ | 1.79 |
| (8 | $(5,15,15)$ | $\mathbf{x}$ | $1.78 \mathrm{x}$ | 1.96 |
| (9) | $(5,15,15)$ | $\boldsymbol{\checkmark}$ | $\mathbf{1 . 8 8 x}$ | $\mathbf{2 . 0 5}$ |

### 5.3. Generation Quality of LOOKAHEAd DECODING

We assess the generation quality of LOOKAHEAD DECODING on LLaMA-2-7B-Chat model with the prompts in Appendix D on summarization datasets (Chen et al., 2023; Leviathan et al., 2023) in Tab. 2. Whether the sampling is activated, LookAHEAD DECODING can reserve the output distribution quality, which is evaluated in rouge-1, rouge2, and rouge-L (Lin, 2004), while achieving 1.46x-1.60x speedups compared with autoregressive decoding. Using sampling gives smaller speedups as the acceptance ratio is lower according to the sampling verification algorithm 4, which aligns with the results in the previous research (Chen et al., 2023; Leviathan et al., 2023). We further verify that using greedy sampling and advanced integrations will not change the generation quality in Appendix E.

### 5.4. Ablation Study

In this section, we study the importance of the lookahead and verification branch in achieving a high speedup. We experiment on LLaMA-2-7B-Chat and MT-Bench on S1 with various settings. The results are shown in Tab. 3.

We ablate the importance of lookahead branch by comparing the performance of using a lookahead branch to the recent methods of using prompts as reference (Yang et al., 2023; Saxena, 2023). This comparison assumes that LOOKAHEAD DECODing does not use the prompt to build the n-gram pool. We use the implementation in transformers v4.37 of prompt lookup as a baseline (2), with prompt_lookup_num_tokens=10). We also use prompt to build n-gram pool to augment LOOKAHEAD DECODING (3)(4)(6). The results show that although using a minimal lookahead branch ( $W=1$ ) with various $N, G$ settings (3)(4)(5)(6) can obtain a decent speedup on MT-Bench, it is still not as good as using balanced branches (8). We can find that prompt lookup can surpass prompt as reference implementation in LOOKAHEAD DECODING. This is because our method checks if $n$-gram starts with one token that exactly matches the last generated token while prompt lookup in transformers v4.37 checks several starting tokens for a better speculation.

We ablate the importance of verification branch by reporting the speedup of using a tiny verification branch and a large lookahead branch (7), $G=1$ ). It shows lower performance due to lower potential in accepting speculations compared with a balanced branches (8).

Besides, our evaluation shows that using prompt as reference can further boost LOOKAHEAD DECODING (8) and (9)). We have integrated them in our implementation.

### 5.5. Discussion and Limitation

The main limitation of LOOKAHEAD DECODING is that it requires extra computations. Our experimental results show that on A100, the config-

Table 4: Good Config. of uration in Tab. 4 works LOOKAHEAD DECODING on A100 GPUs with $G=W$.

| MODEl | WINDOW Size $(W)$ | N-GRAM Size $(N)$ |
| :---: | :---: | :---: |
| 7B | 15 | 5 |
| 13B | 10 | 5 |
| 34B | 7 | 5 |

near optimally in most cases for single batch serving. Because the per-step FLOPs are roughly proportional to the number of per-step input tokens, which is $(W+G) *(N-1)$. If we ignore the attention cost's increase with sequence length, the 7B, 13B, and 34B models require 120x, 80x, and 56x extra FLOPs per step, respectively. Since the LLM decoding is memory bandwidth-bound rather than computebound, these extra FLOPs only turn into a limited wall-clock slowdown for each step.

Given this, LOOKAHEAD DECODING needs large surplus

![](https://cdn.mathpix.com/cropped/2024_06_04_232f8a75b4ba6ca3f0d7g-08.jpg?height=220&width=829&top_left_y=215&top_left_x=1057)

Figure 8: Compression $\operatorname{ratio}(\mathcal{S})$ and speedups of LooKAHEAD DECODING on RTX 3090 and A100 with $N=5$, all with FlashAttention. The blue and orange curves of $\mathcal{S}$ overlap as the device does not affect the ratio.

FLOPs to obtain high speedups. Running in compute-bound environments (e.g., serving with a large batch size) may cause slowdowns. Another example is shown in Fig. 8, where lower speedup is observed when the GPU's cap FLOPs is smaller (e.g., on RTX 3090 GPUs).

Based on $\S 4$, we need to exponentially increase the per-step FLOPs to obtain a linear reduction in decoding steps. Hence, the setting in Tab. 4 faces a diminishing return. However, when FLOPs are not rich, we see that a gentle speedup (e.g., $30 \%$ on RTX 3090 and $>50 \%$ on A100) on MT-Bench easily achievable, as in Fig. 8, which is a free lunch that requires no extra model, training, or changing the output distribution.

## 6. Related Work

Speculative decoding (Chen et al., 2023; Leviathan et al., 2023) pioneer in speedup autoregressive decoding with a draft model. Different methods for obtaining speculations are researched. Specinfer (Miao et al., 2023) uses many draft models obtained from distillation, quantization, and pruning to conduct speculations together. Medusa (Cai et al., 2024), OSD (Liu et al., 2023), and EAGLE (Li et al., 2023) use training to obtain a draft model. REST (He et al., 2023) uses the finetuning dataset itself as a datastore to lookup speculations, while other works (Yang et al., 2023; Saxena, 2023) uses prompt as a reference for speculations. Different from these methods, LOOKAHEAD DECODING uses LLM's parallel generation ability for speculations. Sampling methods are also researched. Specinfer maintains output distribution by a tree-based sampling algorithm. Medusa uses a typical acceptance scheme to accelerate when the temperature is large but does not persist on an exact output distribution. LookaHEAD DeCoding follows Specinfer to maintain output distribution but with multiple disjoint $n$-grams.

## 7. Conclusion

In this paper, we present LOOKAHEAD DECODING to parallelize the autoregressive decoding of LLMs without changing the output distribution. It shows notable speedup without a draft model and can linearly decrease the decoding steps with exponential investment in per-step FLOPs.

## References

Automatic tensor parallelism for huggingface models, 2023. URL https://www.deepspeed.ai/tutoria ls/automatic-tensor-parallelism.

Aminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., et al. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-15. IEEE, 2022.

Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and Sutton, C. Program synthesis with large language models, 2021.

Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

Ben Allal, L., Muennighoff, N., Kumar Umapathi, L., Lipkin, B., and von Werra, L. A framework for the evaluation of code generation models. https://github.com /bigcode-project/bigcode-evaluation-h arness, 2022.

Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., and Dao, T. Medusa: Simple llm inference acceleration framework with multiple decoding heads, 2024.

Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.

Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021.

Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.

Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.

Du, X., Liu, M., Wang, K., Wang, H., Liu, J., Chen, Y., Feng, J., Sha, C., Peng, X., and Lou, Y. Classeval: A manuallycrafted benchmark for evaluating llms on class-level code generation, 2023.

Gugger, S., Debut, L., Wolf, T., Schmid, P., Mueller, Z., Mangrulkar, S., Sun, M., and Bossan, B. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface / accelerate, 2022.

He, Z., Zhong, Z., Cai, T., Lee, J. D., and He, D. Rest: Retrieval-based speculative decoding. arXiv preprint arXiv:2311.08252, 2023.

Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration, 2020.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Kool, W., van Hoof, H., and Welling, M. Ancestral gumbeltop-k sampling for sampling without replacement. Journal of Machine Learning Research, 21(47):1-36, 2020. URL http:// jmlr.org/papers/v21/19-985 . html.

Leviathan, Y., Kalman, M., and Matias, Y. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023.

Li, Y., Zhang, C., and Zhang, H. Eagle: Lossless acceleration of llm decoding by feature extrapolation, December 2023. URL https://sites.google.com/vie w/eagle-llm.

Lin, C.-Y. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: / / www. ac lweb.org/anthology/W04-1013.

Liu, X., Hu, L., Bailis, P., Stoica, I., Deng, Z., Cheung, A., and Zhang, H. Online speculative decoding, 2023.

Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Wong, R. Y. Y., Zhu, A., Yang, L., Shi, X., Shi, C., Chen, Z.,

Arfeen, D., Abhyankar, R., and Jia, Z. Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification, 2023.

Narayan, S., Cohen, S. B., and Lapata, M. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018.

Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V. A., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., and Zaharia, M. Efficient large-scale language model training on gpu clusters using megatron-lm, 2021.

OpenAI. Gpt-4 technical report, 2023.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.

Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

Ruan, J. T., Sabir, F., and Chopra, P. Best prompting practices for using the llama 2 chat llm through amazon sagemaker jumpstart, November 2023. URL https: / /aws.amazon.com/cn/blogs/machine-lea rning/best-prompting-practices-for-u sing-the-llama-2-chat-llm-through-a mazon-sagemaker-jumpstart/.

Santilli, A., Severino, S., Postolache, E., Maiorca, V., Mancusi, M., Marin, R., and Rodola, E. Accelerating transformer inference for translation via parallel decoding. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12336-12355, Toronto, Canada, July 2023. Association for Computational Linguistics. URL https : / /aclanthology.org/2023.acl-long. 689 .

Saxena, A. Prompt lookup decoding, November 2023. URL https: / github. com/apoorvumang/promp t-lookup-decoding/.

See, A., Liu, P. J., and Manning, C. D. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1073-1083, 2017.
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.

Song, Y., Meng, C., Liao, R., and Ermon, S. Accelerating feedforward computation via parallel nonlinear equation solving, 2021.

Stern, M., Shazeer, N., and Uszkoreit, J. Blockwise parallel decoding for deep autoregressive models, 2018.

Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.

Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023.

Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State-ofthe-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. URL https: / /www. aclweb.org/a nthology/2020.emnlp-demos. 6 .

Yang, N., Ge, T., Wang, L., Jiao, B., Jiang, D., Yang, L., Majumder, R., and Wei, F. Inference with reference: Lossless acceleration of large language models, 2023.

Zhang, B. and Sennrich, R. Root mean square layer normalization, 2019.

Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
</end of paper 0>


<paper 1>
# Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting 

Fangcheng Liu ${ }^{\dagger}$ Yehui Tang ${ }^{\dagger}$ Zhenhua Liu ${ }^{\dagger}$<br>Yunsheng $\mathrm{Ni}^{\dagger}$ Kai Han ${ }^{\star, \dagger}$ Yunhe Wang ${ }^{\star, \dagger}$<br>${ }^{\dagger}$ Huawei Noah's Ark Lab * Corresponding Author<br>\{liufangcheng3,kai.han, yunhe.wang\}@huawei.com


#### Abstract

Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models while maintaining a consistent sampling distribution. However, the conventional approach of training a separate draft model to achieve a satisfactory token acceptance rate can be costly. Drawing inspiration from early exiting, we propose a novel self-speculative decoding framework Kangaroo, which uses a fixed shallow sub-network as a self-draft model, with the remaining layers serving as the larger target model. We train a lightweight and efficient adapter module on top of the sub-network to bridge the gap between the sub-network and the full model's representation ability. It is noteworthy that the inference latency of the self-draft model may no longer be negligible compared to the large model, necessitating strategies to increase the token acceptance rate while minimizing the drafting steps of the small model. To address this challenge, we introduce an additional early exiting mechanism for generating draft tokens. Specifically, we halt the small model's subsequent prediction during the drafting phase once the confidence level for the current token falls below a certain threshold. Extensive experiments on the Spec-Bench demonstrate the effectiveness of Kangaroo. Under single-sequence verification, Kangaroo achieves speedups up to $1.68 \times$ on Spec-Bench, outperforming Medusa-1 with $88.7 \%$ fewer additional parameters (67M compared to 591M). The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo


## 1 Introduction

Large Language Models (LLMs) [1, 2, 3, 4, 5, 6] have undeniably showcased remarkable performance across a myriad of natural language tasks. However, constrained by the bottleneck of memory bandwidth [7], the primary latency

![](https://cdn.mathpix.com/cropped/2024_06_04_7d8d0ed1b0af69488765g-02.jpg?height=596&width=1227&top_left_y=434&top_left_x=449)

![](https://cdn.mathpix.com/cropped/2024_06_04_7d8d0ed1b0af69488765g-02.jpg?height=404&width=513&top_left_y=454&top_left_x=470)

(a) The token acceptance rate on the mathematical reasoning subtask in Spec-Bench. Token position " 2 " represents the next-nexttoken prediction task.

![](https://cdn.mathpix.com/cropped/2024_06_04_7d8d0ed1b0af69488765g-02.jpg?height=406&width=631&top_left_y=453&top_left_x=1018)

(b) End-to-end speedup ratio comparison on four subtasks in Spec-Bench. "Math" and "RAG" denote generation, respectively.

Figure 1: Comparison of various self-drafting speculative decoding methods on Spec-Bench [14] for Vicuna-7B [8]. Kangaroo outperforms all other methods w.r.t. end-to-end speedup ratio across all the four subtasks. For more detailed comparison on full Spec-Bench, see Table 1 .

for autoregressive decoding of LLMs stems from memory read/write operations of model weights rather than arithmetic computations. For instance, decoding with Vicuna-33B [8] on four NVIDIA V100 GPUs yields a throughput of only seven new tokens per second. To address this challenge, Speculative Decoding (SD) techniques $[9,10]$ have been developed, aiming to accelerate autoregressive decoding by verifying multiple tokens generated by a draft model in parallel. Given $\gamma$ draft tokens, SD can generate 1 to $\gamma+1$ new tokens within each forward pass of the large LLM. The effectiveness of SD relies on two primary factors: 1) the gap between the draft model and the target LLM. Researchers often train a tiny draft model from scratch on a large corpus to accelerate large LLMs from the same series, e.g., LLaMA-68M [11] for LLaMA7B [2]. However, the training of such task-specific models can be costly [12, 13], limiting its application in real-world scenarios; 2) the inference latency of the draft model. If the inference cost of the small model is negligible compared to the target large LLM, the end-to-end speedup ratio is directly proportional to the consistent token acceptance rate as defined in Eq (2).

To address the aforementioned issues, several studies have proposed selfdrafting methods that do not rely on external drafter models. LLMA [15] and REST [16] generate draft tokens by selecting text spans from reference or retrieving relevant tokens from the database. Notably, Medusa [17] trains multiple time-independent FFN heads on top of the last decoder layer. However, these approaches still present some challenges. While Medusa can efficiently generate multiple draft tokens at adjacent positions, its token acceptance rate is not yet satisfactory (see Figure 1(a)). Additionally, focusing exclusively on the token acceptance rate without considering the latency of generating draft
tokens can lead to suboptimal end-to-end acceleration. For instance, Lookahead [18] achieves a token acceptance rate comparable to Kangaroo in the mathematical reasoning subtask, significantly outperforming Medusa. However, due to its lower efficiency in generating draft tokens compared to Medusa, its end-to-end speedup ratio is slightly lower than that of Medusa (see Figure 1 .

In response to these challenges, we design an autoregressive self-draft model by training a lightweight and efficient adapter module on top of a fixed shallow sub-network of the original large LLM. As shown in Figure2, the adapter network architecture consists of only one multi-head attention |19] and two normalization layers [20]. Surprisingly, we find this simple design efficient but powerful, with only $11.3 \%$ of the parameters of the Medusa's heads ${ }^{1}$. To further reduce the inference latency of the self-draft model, we introduce an additional early exiting mechanism for generating draft tokens, aiming to avoid unnecessary costs on more difficult tokens.

To summarize, our main contributions are:

- We propose a novel self-speculative decoding framework based on a double early-exit mechanism, named Kangaroo. Firstly, the equivalent selfdraft small model exits early from the fixed shallow layers of the large LLM and connects to an adapter network to generate draft tokens. Secondly, during the drafting phase, Kangaroo uses early exiting at suitable points to avoid unnecessary computational overhead on more challenging tokens.
- Kangaroo offers a low-cost approach to train a lightweight small model. Since the self-speculative draft model and the large LLM share some KV cache and computation, the only additional deployment requirement in practice is a small adapter network.
- Experiments on the Spec-Bench [14] validate the effectiveness of Kangaroo. Under single-sequence verification, Kangaroo achieves speedups up to $1.7 \times$ on Spec-Bench, outperforming Medusa-1 with $88.7 \%$ fewer additional parameters, i.e., $67 \mathrm{M}$ compared to $591 \mathrm{M}$.

This paper is structured as follows: Section 2 reviews related works, and Section 3 introduces our framework, Kangaroo. The experimental section, Section 4. provides analysis and comparisons with various self-drafting methods, along with ablation studies to identify Kangaroo's key components. The conclusion is presented in Section 5 .[^0]

## 2 Related work

Inference Acceleration of Large Language Models With the rapid development of large language models, significant research effort has been dedicated to accelerating their inference speed [21]. Techniques such as knowledge distillation [22], model compression [23] and quantization [24] have also been widely applied in this area. However, these approaches often require additional training of the backbone or substantial modifications to the model architecture. Recent efforts have explored early exiting on models like the T5 series [25, 26, 27] and decoder-only architectures [28]. However, since early exiting accelerates inference by saving subsequent computations, it inevitably incurs the issue of performance degradation [25].

Speculative Decoding Speculative Decoding (SD) has gained significant attention due to its ability to accelerate the inference of LLMs while maintaining the same sampling distribution. Generally, SD [9. 10] involves finding or training [12, 29] a small draft model closely aligned with the target LLM. Consequently, recent research has focused on more convenient self-drafting methods. For instance, approaches like blockwise parallel decoding [30] and Medusa [17] expedite the generation of draft tokens by training multiple time-independent Feedforward Neural Networks (FFNs) at the second-top-layer. Several selfdrafting acceleration techniques are inspired by early exiting. Draft \& Verify [31], for instance, generates draft tokens by skipping intermediate redundant layers of the target LLM. While this approach could achieve a high token acceptance rate, the inference latency of the "small model" is exceptionally high, which can hinder end-to-end acceleration efficiency. SPEED [32] adapts early exiting to pipelined speculative execution for transformer decoders that employ parameter sharing. Concurrently, we have learned that there are also several works [33, 34, 35] that make improvement on Medusa by introducing time dependency among the draft tokens. For more detailed summarization, we refer readers to a recent survey [14] on speculative decoding.

## 3 Kangaroo

In this section, we first delve into an in-depth analysis of token acceptance rate, compression rate, and speedup ratio for several self-drafting algorithms. Subsequently, we introduce our framework, Kangaroo, which employs selfspeculative decoding by sharing a fixed shallow sub-network of the large LLM. To further reduce the inference latency of the self-draft model, we introduce an additional early exiting mechanism when generating draft tokens.

Notation. We use $x^{t}$ to denote the discrete token sequence $\left(x_{1}, \cdots, x_{t}\right)$ and $x^{i: j}$ to represent sequence $\left(x_{i}, \cdots, x_{j}\right)$. Let $\mathcal{V}$ be a discrete space over all possible tokens in the LLM's vocabulary, we model the autoregressive process of
a language model $\mathcal{M}$ by the conditional distributions $\mathcal{M}\left(\cdot \mid x^{t}\right) \in \mathbb{R}^{|\mathcal{V}|}$ where $|\mathcal{V}|$ is the vocabulary size. We use subscript $\mathcal{M}_{n}\left(\cdot \mid x^{t}\right)$ to denote the $n$-th entry of the probability distribution. We denote the large target language model and the speculative small model as $\mathcal{M}^{b}$ and $\mathcal{M}^{s}$, respectively.

Token Acceptance Rate Decays along Speculative Direction Speculative decoding is often evaluated using two primary metrics: walltime speedup ratio and compression rate. Given a speculative decoding algorithm, we execute it to generate $N$ new tokens and record the accepted tokens per forward of the large model as a list $S=\left[s_{1}, s_{2}, \cdots, s_{|S|}\right]$ where $\sum_{k} s_{k}=N$. The compression rate (CR) is defined as

$$
\begin{equation*}
\mathrm{CR}=\frac{1}{|S|} \sum_{k} s_{k} \tag{1}
\end{equation*}
$$

Note that during the verification of speculative sampling, once a draft token is rejected by the large model $\mathcal{M}^{b}$, all subsequent tokens will be discarded regardless of their quality. Compression rate does not accurately reflect the acceptance levels of the drafting algorithm for tokens at varying distances. Thus, we propose a new evaluation metric called consistent token acceptance rate:

Definition 1. The consistent token acceptance rate $\operatorname{CTAR}(w)$, given a prefix and a following window with size $w$, is the probability that the $w$ guessed tokens from the draft model $\mathcal{M}^{s}$ are all accepted by the target model $\mathcal{M}^{b}$.

For the greedy decoding setting, $\operatorname{CTAR}\left(x^{t}, w\right)$ is 0 if there is at least one inconsistent top-1 prediction between $\mathcal{M}^{s}$ and $\mathcal{M}^{b}$ within the window, otherwise 1 . Similar to the compression rate, the consistent token acceptance rate could be calculate as:

$$
\begin{equation*}
\operatorname{CTAR}(w)=\frac{1}{|S|} \sum_{k} \mathbb{I}\left(s_{k}-w>0\right) \tag{2}
\end{equation*}
$$

which is a decreasing function w.r.t. the window size $w$. We plot the empirical CTARs (for $w=1,2, \cdots, 6$ ) of several self-drafting speculative decoding algorithms on the mathematical reasoning subtask of Spec-Bench [14] in Figure 1(a). It can be seen that in addition to the token acceptance rate, the speed of generating draft tokens also has a significant impact on the final end-to-end speedup ratio.

### 3.1 Early Exiting as Self-Drafting Model

Training an additional small model from scratch is often costly, thus it is worth considering sharing a portion of the parameters with the target LLM. Inspired by the concept of early exiting, we directly extract hidden states from a fixed shallow sub-network of the target LLM and learn a mapping from the shallow layer to the final layer. Specifically, We train a lightweight and efficient adapter $\mathcal{A}$ to bridge the gap between the self-draft model $\mathcal{M}^{s}=\mathcal{A} \circ \mathcal{M}^{b}[: l]$ and the target model $\mathcal{M}^{b}$, where the early exit layer $l \in\{1,2, \cdots, L\}$ and $\mathcal{A}$ denotes the
![](https://cdn.mathpix.com/cropped/2024_06_04_7d8d0ed1b0af69488765g-06.jpg?height=610&width=1176&top_left_y=427&top_left_x=452)

Figure 2: The framework of Kangaroo. The adapter network $\mathcal{A}$ consists of only one multi-head attention [19] and two normalization layers [20]. The self-draft model $\mathcal{M}^{s}=\mathcal{A} \circ \mathcal{M}^{b}[: l]$ will reuse the LM Head of the target LLM $\mathcal{M}^{b}$, where $l$ denotes the early exit layer. To avoid unnecessary costs on more difficult tokens, $\mathcal{M}^{s}$ stops drafting once the confidence level of the current token falls below a certain threshold, e.g., $\mathcal{M}^{s}\left(x_{3}^{\prime}\right) \leq \eta$. Note that we will concatenate the stopped token's next early feature $f_{3}$ with all previous exited features into a parallel compute unit $\left[f_{0}, f_{1}, \cdots, f_{3}\right]$, which will be verified by the remaining layers $\mathcal{M}^{b}[l:]$ in parallel. Once all drafted tokens are accepted $\left(x_{i}^{\prime}=x_{i}\right.$ for $i=1,2,3$ ), we could start the next round with $x_{4}$ rather than $x_{3}$ if we have not calculated $f_{3}$ in advance. The decoding on parallel compute unit $\left[f_{3}, f_{4}\right]$ could save the latency for a single forward pass of the adapter network $\mathcal{A}$.

adapter network. As shown in Figure 2 the architecture of the adapter $\mathcal{A}$ consists of only one multi-head attention [19] and two normalization layers [20].

Training Loss A trivial method for training the adapter network is to maximize the token acceptance rate across each position, while we find that the cross-entropy loss exhibits faster convergence rate, i.e.,

$$
\begin{equation*}
\mathcal{A}^{*}=\underset{\mathcal{A}}{\arg \min } \sum_{t} \sum_{n}-\mathcal{M}_{n}^{b}\left(x_{t}\right) \log \mathcal{M}_{n}^{s}\left(x_{t}\right) . \tag{3}
\end{equation*}
$$

### 3.2 Dynamic Drafting Steps with Early-Exiting

Speculative decoding typically employs a fixed drafting step during the drafting phase, but this often leads to local optima. On one hand, the difficulty of

Table 1: Speedup comparison of various self-drafting speculative decoding methods on Spec-Bench [14] for Vicuna [8]. Speedup is the walltime speedup ratio and CR denotes the compression rate.

| Size | Method | $\frac{\text { Translation }}{\text { CR Speedup }}$ |  | $\frac{\mathrm{QA}}{\mathrm{CR} \text { Speedup }}$ |  | Summarization |  | Math |  | RAG |  | MT Bench |  | Avg. |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  |  |  | $\overline{\mathrm{CR}}$ | Speedup | $\overline{\mathrm{C}}$ | $\mathrm{S}$ | $\overline{\mathrm{C}}$ | Spe | $\overrightarrow{\mathrm{CF}}$ | p |  |
| $7 \mathrm{~B}$ | Lool | 1.24 | $115 x$ |  |  | 1.56 | $121 \vee$ | 1.53 | $127 x$ | 196 | $1.51 \times$ | 1.49 | 1.1 | 1.70 | 14 | $1.29 x$ |
| $7 \mathrm{~B}$ | Mec | 1.58 | 1.4 | 1.50 | 1 | 1.49 | 1 | 1 |  | 1.51 |  | 1.76 |  |  |
| $7 \mathrm{~B}$ | REST | 1.54 | $1.26 \times$ | 1.91 | 1.6 | 1.64 | 1. | 1.53 | $1.23 \times$ | 1.92 | 1. | 2.00 | 1. | $1.43 \times$ |
| $7 \mathrm{~B}$ | Kangaroo | 1.41 | $1.24 \times$ | 1.87 | $1.43 \times$ | 1.87 | $1.50 \times$ | 2.14 | $1.61 \times$ | 2.05 | $1.52 \times$ | 2.22 | $1.68 \times$ | $1.50 \times$ |
| $13 \mathrm{~B}$ | L | 1. . |  |  |  | $1-0$ |  |  |  |  |  | 1.68 |  |  |
| 13B | REST 16 | 1.53 | $1.07 \times$ | 1.92 | $1.41 \quad$ | 1.66 |  | 1.55 | $1.06 \times$ | 1.87 |  | 1.98 |  | $1.23 \times$ |
| 13B | Medusa 17 | 1.61 | $1.33 \times$ | 1.49 | $1.25 \times$ | 1.53 | $1.25 \times$ | 1.80 | $1.48 \times$ | 1.53 | $1.23 \times$ | 1.82 | $1.48 \times$ | $1.34 \times$ |
| $13 \mathrm{~B}$ | Kangaroo | 1.45 | $1.18 \times$ | 1.79 | $1.34 \times$ | 2.00 | $1.41 \times$ | 2.42 | $1.63 \times$ | 2.16 | $1.40 \times$ | 2.44 | $1.66 \times$ | $1.44 \times$ |

predicting the next token varies across different contextual scenarios. Therefore, it is highly likely to waste time on more challenging samples or miss opportunities to speculate on simpler tokens further. On the other hand, the inference of the small model used in this approach still incurs a certain cost, and timely termination can save a considerable amount of latency. Therefore, we stop drafting once the top-1 confidence on the self-draft model is below a predefined threshold $\eta$, i.e.,

$$
\begin{equation*}
\max _{n} \mathcal{M}_{n}^{s}(x) \leq \eta \tag{4}
\end{equation*}
$$

## 4 Experiments

### 4.1 Implementation Details

We conduct experiments on Vicuna [8] models with size of 7B and 13B. We select three self-drafting speculative decoding approaches for comparison, i.e., Lookahead [18], Medusa [17] and REST [16]. We utilize the compression rate and the walltime speedup ratio metric. For fail comparison, we benchmark the performance of the selected self-drafting methods with the recently proposed Spec-Bench [14]. All models are evaluated on NVIDIA V100 GPUs. For Kangaroo, we train the adapter network for 10 epochs with the AdamW [36] optimizer on the ShareGPT dataset following Medusa [17].

### 4.2 Ablation Studies

The Depth of Shallow Sub-Network. The capacity of the self-draft model $\mathcal{M}^{s}$ highly depends on the depth of the shared shallow sub-network. However, selecting deeper early exiting layers, such as half layers of $\mathcal{M}^{b}$, would result in excessively high inference latency. Therefore, the the early exitlayer $l$ controls a trade-off between token acceptance rate and drafting efficiency. As shown in Figure 3(a), we set $\ell=2$ for Vicuna-7B and $\ell=3$ for Vicuna-13B.

![](https://cdn.mathpix.com/cropped/2024_06_04_7d8d0ed1b0af69488765g-08.jpg?height=450&width=1217&top_left_y=434&top_left_x=454)

![](https://cdn.mathpix.com/cropped/2024_06_04_7d8d0ed1b0af69488765g-08.jpg?height=366&width=293&top_left_y=451&top_left_x=469)

![](https://cdn.mathpix.com/cropped/2024_06_04_7d8d0ed1b0af69488765g-08.jpg?height=365&width=284&top_left_y=449&top_left_x=755)

(a) Optimal exit-layer $l$.

![](https://cdn.mathpix.com/cropped/2024_06_04_7d8d0ed1b0af69488765g-08.jpg?height=366&width=569&top_left_y=451&top_left_x=1079)

(b) Optimal threshold $\eta$.

Figure 3: Ablation studies on hyper-parameters. The compression rate and walltime speedup is averaged across all sub-benchmarks in Spec-Bench.

The Architecture of the Adapter Module. In a transformer block, the FFN component counts for $67 \%$ of the whole parameters. As shown in Table 2, we find that removing the FFN component and sharing the LM Head of the target LLM is extremely effective.

Table 2: Ablation studies on the architecture of the adapter module $\mathcal{A}$ for Vicuna-7B. "Speedup" denotes the average speedup ratio on Spec-Bench [14].

| Architecture | Input LN | Attention | Post LN | FFN | Linear | Last LN | Head | \# Parameters | Speedup |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Medusa | $x$ | $x$ | $x$ | $x$ | $\times 4$ | $x$ | $\times 4$ | $591 \mathrm{M}$ | $1.41 \times$ |
| Kangaroo | $\checkmark$ | $\checkmark$ | $x$ | $x$ | $x$ | $\checkmark$ | $x$ | $67 \mathrm{M}$ | $1.50 \times$ |
| Kangaroo + Head | $\checkmark$ | $\checkmark$ | $x$ | $x$ | $x$ | $\checkmark$ | $\checkmark$ | 198M | $1.44 \times$ |
| 1-Layer Transformer | $\checkmark$ | $\checkmark$ | $\checkmark$ | $\checkmark$ | $x$ | $\checkmark$ | $x$ | $202 \mathrm{M}$ | $1.37 \times$ |
| MLP Only | $\checkmark$ | $x$ | $x$ | $x$ | $\times 2$ | $\checkmark$ | $\checkmark$ | $165 \mathrm{M}$ | $1.22 \times$ |

Dynamic Exiting v.s. Fixed Step Drafting. To validate the effectiveness of our dynamic drafting steps with fixed threshold, we plot the comparison for various $\eta$ in Figure 3(b) The fixed step strategy $(\eta=0)$ achieves the maximum compression rate, however, leading to sub-optimal end-to-end walltime speedup. Overall, the optimal threshold $\eta$ is consistent across different maximum different steps. For Kangaroo, we set $\gamma=6$ and $\eta=0.6$.

## 5 Conclusion

In this paper, we introduced Kangaroo, a novel self-speculative decoding framework tailored for accelerating the inference of large language models. Kangaroo uses a fixed shallow sub-network to formulate a self-draft model, with the remaining layers serving as the larger target model. To reduce the inference latency of the self-draft model, we introduce an additional early exiting mechanism for generating draft tokens, aiming to avoid unnecessary costs on more difficult tokens. Under single-sequence verification, Kangaroo achieves speedups up to $1.7 \times$ on Spec-Bench, outperforming Medusa-1 with $88.7 \%$ fewer additional parameters.

## References

[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[3] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

[4] Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan Bai, Yun Wang, et al. Pangu- $\pi$ : Enhancing language model architectures via nonlinearity compensation. arXiv preprint arXiv:2312.17276, 2023.

[5] Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, and Yunhe Wang. Rethinking optimization and architecture for tiny language models. arXiv preprint arXiv:2402.02791, 2024.

[6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.

[7] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019.

[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%^{*}$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023.

[9] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023.

[10] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274-19286. PMLR, 2023.

[11] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023.

[12] Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461, 2023.

[13] Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. Multi-candidate speculative decoding. arXiv preprint arXiv:2401.06706, 2024.

[14] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851, 2024.

[15] Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. Inference with reference: Lossless acceleration of large language models. arXiv preprint arXiv:2304.04487, 2023.

[16] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D Lee, and Di He. Rest: Retrievalbased speculative decoding. arXiv preprint arXiv:2311.08252, 2023.

[17] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024.

[18] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of $1 l \mathrm{~m}$ inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024.

[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[20] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019.

[21] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. A survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294, 2024.

[22] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations, 2023.

[23] Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu, and Dacheng Tao. A survey on transformer compression. arXiv preprint arXiv:2402.05964, 2024.

[24] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087-38099. PMLR, 2023.

[25] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:17456-17472, 2022.

[26] Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5910-5924, 2023.

[27] Shengkun Tang, Yaqing Wang, Zhenglun Kong, Tianchi Zhang, Yao Li, Caiwen Ding, Yanzhi Wang, Yi Liang, and Dongkuan Xu. You need multiple exiting: Dynamic early exiting for accelerating unified vision language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1078110791,2023

[28] Neeraj Varshney, Agneet Chatterjee, Mihir Parmar, and Chitta Baral. Accelerating llama inference by enabling intermediate layer decoding via instruction tuning with lite. arXiv e-prints, pages arXiv-2310, 2023.

[29] Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. Spectr: Fast speculative decoding via optimal transport. arXiv preprint arXiv:2310.15141, 2023.

[30] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, $31,2018$.

[31] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft \& verify: Lossless large language model acceleration via selfspeculative decoding. arXiv preprint arXiv:2309.08168, 2023.

[32] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, and Sophia Shao. Speed: Speculative pipelined execution for efficient decoding. arXiv preprint arXiv:2310.12072, 2023.

[33] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024.

[34] Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, and William Brandon. Hydra: Sequentiallydependent draft heads for medusa decoding. arXiv preprint arXiv:2402.05109, 2024.

[35] Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, and Yunfei Cheng. Recurrent drafter for fast speculative decoding in large language models. arXiv preprint arXiv:2403.09919, 2024.

[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.


[^0]:    ${ }^{1}$ For detailed ablation studies on the architecture of the adapter, see Table 2

</end of paper 1>


<paper 2>
# Mistral 7B 


#### Abstract

Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed


![](https://cdn.mathpix.com/cropped/2024_05_29_c1e08f80a92ab8fdeda7g-1.jpg?height=366&width=1073&top_left_y=749&top_left_x=534)


#### Abstract

We introduce Mistral 7B, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B - Instruct, that surpasses Llama 2 13B - chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.

Code: https://github.com/mistralai/mistral-src

Webpage: https://mistral.ai/news/announcing-mistral-7b/


## 1 Introduction

In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model performance often necessitates an escalation in model size. However, this scaling tends to increase computational costs and inference latency, thereby raising barriers to deployment in practical, real-world scenarios. In this context, the search for balanced models delivering both high-level performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that a carefully designed language model can deliver high performance while maintaining an efficient inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [26]) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [25]) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [20], without sacrificing performance on non-code related benchmarks.

Mistral 7B leverages grouped-query attention (GQA) [1], and sliding window attention (SWA) [6, 3]. GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms collectively contribute to the enhanced performance and efficiency of Mistral 7B.

Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation ${ }^{1}$ facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM [17] inference server and SkyPilot ${ }^{2}$. Integration with Hugging Face ${ }^{3}$ is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B - Chat model.

Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.

## 2 Architectural details

![](https://cdn.mathpix.com/cropped/2024_05_29_c1e08f80a92ab8fdeda7g-2.jpg?height=504&width=1326&top_left_y=816&top_left_x=384)

Effective Context Length

Figure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher latency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window attention: each token can attend to at most $W$ tokens from the previous layer (here, $W=3$ ). Note that tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by $W$ tokens. Hence, after $k$ attention layers, information can move forward by up to $k \times W$ tokens.

Mistral 7B is based on a transformer architecture [27]. The main parameters of the architecture are summarized in Table 1. Compared to Llama, it introduces a few changes that we summarize below.

Sliding Window Attention. SWA exploits the stacked layers of a transformer to attend information beyond the window size $W$. The hidden state in position $i$ of the layer $k, h_{i}$, attends to all hidden states from the previous layer with positions between $i-W$ and $i$. Recursively, $h_{i}$ can access tokens from the input layer at a distance of up to $W \times k$ tokens, as illustrated in Figure 1. At the last layer, using a window size of $W=4096$, we have a theoretical attention span of approximately $131 K$ tokens. In practice, for a sequence length of $16 \mathrm{~K}$ and $W=4096$, changes made to FlashAttention [11] and xFormers [18] yield a 2x speed improvement over a vanilla attention baseline.

Rolling Buffer Cache. A fixed attention span means that we can limit our cache size using a rolling buffer cache. The cache has a fixed size of $W$, and the keys and values for the timestep $i$ are stored in position $i \bmod W$ of the cache. As a result, when the position $i$ is larger than $W$, past values in the cache are overwritten, and the size of the cache stops increasing. We provide an illustration in Figure 2 for $W=3$. On a sequence length of $32 \mathrm{k}$ tokens, this reduces the cache memory usage by $8 \mathrm{x}$, without impacting the model quality.[^0]

| Parameter | Value |
| :--- | ---: |
| dim | 4096 |
| n_layers | 32 |
| head_dim | 128 |
| hidden_dim | 14336 |
| n_heads | 32 |
| n_kv_heads | 8 |
| window_size | 4096 |
| context_len | 8192 |
| vocab_size | 32000 |

Table 1: Model architecture.

![](https://cdn.mathpix.com/cropped/2024_05_29_c1e08f80a92ab8fdeda7g-3.jpg?height=225&width=1396&top_left_y=251&top_left_x=362)

Figure 2: Rolling buffer cache. The cache has a fixed size of $W=4$. Keys and values for position $i$ are stored in position $i \bmod W$ of the cache. When the position $i$ is larger than $W$, past values in the cache are overwritten. The hidden state corresponding to the latest generated tokens are colored in orange.

Pre-fill and Chunking. When generating a sequence, we need to predict tokens one-by-one, as each token is conditioned on the previous ones. However, the prompt is known in advance, and we can pre-fill the $(k, v)$ cache with the prompt. If the prompt is very large, we can chunk it into smaller pieces, and pre-fill the cache with each chunk. For this purpose, we can select the window size as our chunk size. For each chunk, we thus need to compute the attention over the cache and over the chunk. Figure 3 shows how the attention mask works over both the cache and the chunk.

![](https://cdn.mathpix.com/cropped/2024_05_29_c1e08f80a92ab8fdeda7g-3.jpg?height=393&width=963&top_left_y=928&top_left_x=581)

Figure 3: Pre-fill and chunking. During pre-fill of the cache, long sequences are chunked to limit memory usage. We process a sequence in three chunks, "The cat sat on", "the mat and saw", "the dog go to". The figure shows what happens for the third chunk ("the dog go to"): it attends itself using a causal mask (rightmost block), attends the cache using a sliding window (center block), and does not attend to past tokens as they are outside of the sliding window (left block).

## 3 Results

We compare Mistral 7B to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:

- Commonsense Reasoning (0-shot): Hellaswag [28], Winogrande [21], PIQA [4], SIQA [22], OpenbookQA [19], ARC-Easy, ARC-Challenge [9], CommonsenseQA [24]
- World Knowledge (5-shot): NaturalQuestions [16], TriviaQA [15]
- Reading Comprehension (0-shot): BoolQ [8], QuAC [7]
- Math: GSM8K [10] (8-shot) with maj@ 8 and MATH [13] (4-shot) with maj@4
- Code: Humaneval [5] (0-shot) and MBPP [2] (3-shot)
- Popular aggregated results: MMLU [12] (5-shot), BBH [23] (3-shot), and AGI Eval [29] (3-5-shot, English multiple-choice questions only)

Detailed results for Mistral 7B, Llama 27B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama $134 \mathrm{~B}^{4}$ in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.[^1]![](https://cdn.mathpix.com/cropped/2024_05_29_c1e08f80a92ab8fdeda7g-4.jpg?height=440&width=1390&top_left_y=257&top_left_x=365)

Figure 4: Performance of Mistral 7B and different Llama models on a wide range of benchmarks. All models were re-evaluated on all metrics with our evaluation pipeline for accurate comparison. Mistral 7B significantly outperforms Llama 2 7B and Llama 2 13B on all benchmarks. It is also vastly superior to Llama 1 34B in mathematics, code generation, and reasoning benchmarks.

| Model | Modality | MMLU | HellaSwag | WinoG | PIQA | Arc-e | Arc-c | NQ | TriviaQA | HumanEval | MBPP | MATH | GSM8K |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| LLaMA 2 7B | Pretrained | $44.4 \%$ | $77.1 \%$ | $69.5 \%$ | $77.9 \%$ | $68.7 \%$ | $43.2 \%$ | $24.7 \%$ | $63.8 \%$ | $11.6 \%$ | $26.1 \%$ | $3.9 \%$ | $16.0 \%$ |
| LLaMA 2 13B | Pretrained | $55.6 \%$ | $\mathbf{8 0 . 7 \%}$ | $72.9 \%$ | $80.8 \%$ | $75.2 \%$ | $48.8 \%$ | $\mathbf{2 9 . 0 \%}$ | $69.6 \%$ | $18.9 \%$ | $35.4 \%$ | $6.0 \%$ | $34.3 \%$ |
| Code-Llama 7B | Finetuned | $36.9 \%$ | $62.9 \%$ | $62.3 \%$ | $72.8 \%$ | $59.4 \%$ | $34.5 \%$ | $11.0 \%$ | $34.9 \%$ | $31.1 \%$ | $\mathbf{5 2 . 5 \%}$ | $5.2 \%$ | $20.8 \%$ |
| Mistral 7B | Pretrained | $60.1 \%$ | $81.3 \% \quad$ | $75.3 \%$ | $\mathbf{8 3 . 0 \%}$ | $\mathbf{8 0 . 0 \%}$ | $\mathbf{5 5 . 5 \%}$ | $\mathbf{2 8 . 8 \%}$ | $69.9 \%$ | $30.5 \%$ | $47.5 \%$ | $13.1 \%$ | $\mathbf{5 2 . 2 \%}$ |

Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.

Size and Efficiency. We computed "equivalent model sizes" of the Llama 2 family, aiming to understand Mistral 7B models' efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than $3 x$ its size. On the Knowledge benchmarks, Mistral 7B's performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.

Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.

## 4 Instruction Finetuning

To evaluate the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on the Hugging Face repository. No proprietary data or training tricks were utilized: Mistral 7B - Instruct model is a simple and preliminary demonstration that the base model can easily be fine-tuned to achieve good performance. In Table 3, we observe that the resulting model, Mistral 7B - Instruct, exhibits superior performance compared to all 7B models on MT-Bench, and is comparable to $13 \mathrm{~B}$ - Chat models. An independent human evaluation was conducted on https://llmboxing.com/leaderboard.

| Model | Chatbot Arena <br> ELO Rating | MT Bench |
| :--- | :---: | :--- |
| WizardLM 13B v1.2 | 1047 | 7.2 |
| Mistral 7B Instruct | $\mathbf{1 0 3 1}$ | $\mathbf{6 . 8 4}+/ \mathbf{0 . 0 7}$ |
| Llama 2 13B Chat | 1012 | 6.65 |
| Vicuna 13B | 1041 | 6.57 |
| Llama 2 7B Chat | 985 | 6.27 |
| Vicuna 7B | 997 | 6.17 |
| Alpaca 13B | 914 | 4.53 |

Table 3: Comparison of Chat models. Mistral 7B Instruct outperforms all 7B models on MT-Bench, and is comparable to $13 \mathrm{~B}-$ Chat models.

In this evaluation, participants were provided with a set of questions along with anonymous responses from two models and were asked to select their preferred response, as illustrated in Figure 6. As of October 6, 2023, the outputs generated by Mistral 7B were preferred 5020 times, compared to 4143 times for Llama 2 13B.
![](https://cdn.mathpix.com/cropped/2024_05_29_c1e08f80a92ab8fdeda7g-5.jpg?height=682&width=1000&top_left_y=244&top_left_x=560)

Figure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for Mistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the amount of knowledge it can compress).

## 5 Adding guardrails for front-facing applications

The ability to enforce guardrails when it comes to AI generation is important for front-facing applications. In this section, we highlight how to leverage system prompting to optionally enforce output constraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform fine-grained content moderation, which can be useful to enforce quality content in applications.

### 5.1 System prompt to enforce guardrails

We introduce a system prompt (see below) to guide the model to generate answers within specified guardrails, similar to the work done with Llama 2. Using this prompt allows the user to move on the Pareto front of model utility / guardrails enforcement, as indicated in Table 4.

Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.

We use a set of 175 unsafe prompts for evaluating safety. With the recommended system prompt, the model properly declines to answer $100 \%$ of the harmful questions.

As an illustration, we provide in Table 5 the answers of both Mistral 7B - Instruct and Llama 2 Chat 13B to the question How to kill a linux process with system prompts activated. We observe that Mistral 7B provides a correct response while Llama 2 declines to answer. Note that on this specific question, both models answer correctly when system prompts are deactivated.

| Guardrails | MT Bench |
| :---: | :---: |
| No system prompt | $6.84 \pm 0.07$ |
| Llama 2 system prompt | $6.38 \pm 0.07$ |
| Mistral system prompt | $6.58 \pm 0.05$ |

Table 4: System prompts. Mean official MT Bench score over 10 iterations with standard deviation for Mistral 7B - Instruct. For reference, Llama 2 13B - Chat reports official results of 6.65 .

### 5.2 Content moderation with self-reflection

Mistral 7B - Instruct can be used as a content moderator: the model itself is able to accurately classify a user prompt or its generated answer as being either acceptable or falling into one of the following categories: Illegal activities such as terrorism, child abuse or fraud; Hateful, harassing or violent content such as discrimination, self-harm or bullying; Unqualified advice for instance in legal, medical or financial domains.

| Model | Answer |
| :--- | :--- | | To kill a Linux process, you can use the ` kill` command followed by the process ID (PID) |
| :--- |
| of the process you want to terminate. For example, to kill process with PID 1234, you |
| would run the command `kill 1234`. It's important to note that killing a process can |
| have unintended consequences, so it's generally a good idea to only kill processes that |
| you are certain you want to terminate. Additionally, it's generally a good idea to use the |
| `kill`command with caution, as it can be used to terminate critical system processes that |
| are necessary for the operation of the system. |

Table 5: Comparison between Mistral and Llama system prompts. Mistral provides the right answer whereas Llama 2 declines to answer to the question How to kill a linux process.

To do so, we designed a self-reflection prompt that makes Mistral 7B classify a prompt or a generated answer. We evaluated self-reflection on our manually curated and balanced dataset of adversarial and standard prompts and got a precision of $99.4 \%$ for a recall of $95.6 \%$ (considering acceptable prompts as positives).

The use cases are vast, from moderating comments on social media or forums to brand monitoring on the internet. In particular, the end user is able to select afterwards which categories to effectively filter based on their particular use-case.

## 6 Conclusion

Our work on Mistral 7B demonstrates that language models may compress knowledge more than what was previously thought. This opens up interesting perspectives: the field has so far put the emphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as in [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and much remains to be explored to obtain the best performance with the smallest possible model.

## Acknowledgements

We are grateful to CoreWeave for their $24 / 7$ help in marshalling our cluster. We thank the CINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help. We thank the maintainers of FlashAttention, vLLM, xFormers, Skypilot for their precious assistance in implementing new features and integrating their solutions into ours. A huge thanks to Tri Dao and Daniel Haziza for helping include Mistral related changes to FlashAttention and xFormers on a tight schedule. We thank the teams of Hugging Face, AWS, GCP, Azure ML for their intense help in making our model compatible everywhere.

# Can you recommend a foundational and accessible book about quantum physics? 

\author{

| Copy Link to Question | Next Round (Enter) $\rightarrow$ |
| :--- | :--- |

}

Round Winner: mistral-7b-instruct-v0.1

![](https://cdn.mathpix.com/cropped/2024_05_29_c1e08f80a92ab8fdeda7g-7.jpg?height=309&width=301&top_left_y=995&top_left_x=923)

mistral-7b-instruct-v0.1 Round Winner

A great book for a foundational understanding of quantum physics is "The Quantum Universe: Everything that Can Happen Does Happen" by Brian Cox and Jeff Forshaw. This book is accessible and easy to understand, making it a great choice for beginners. It covers all the basics of quantum mechanics, including superposition, entanglement, and the uncertainty principle. The authors also explore the implications of quantum physics for our understanding of the universe, including the nature of time and space. Overall, "The Quantum Universe" is an excellent choice for anyone looking to gain a foundational understanding of this fascinating field
Llama-2-13b-chat

Certainly! For a foundational and accessible introduction to quantum physics, I highly recommend "The Feynman Lectures on Physics" by Richard Feynman. This classic text provides a clear and engaging overview of the principles and concepts of quantum mechanics, with minimal mathematical formalism and plenty of illustrations and examples. It's a great starting point for anyone looking to gain a deeper understanding of quantum physics without getting bogged down in too much technical detail.

Figure 6: Human evaluation of Mistral 7B - Instruct vs Llama 2 13B - Chat Example. An example of human evaluation from llmboxing. com. The question asks for recommendations of books in quantum physics. Llama 2 13B - Chat recommends a general physics book, while Mistral 7B - Instruct recommends a more relevant book on quantum physics and describes in the contents in more detail.

## References

[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.

[2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.

[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

[4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, 2020.

[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

[6] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

[7] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. Quac: Question answering in context. arXiv preprint arXiv:1808.07036, 2018 .

[8] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.

[9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.

[10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

[11] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.

[12] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.

[13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.

[14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems, volume 35, 2022.

[15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.

[16] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466, 2019.

[17] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.

[18] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular and hackable transformer modelling library. https://github.com/ facebookresearch/xformers, 2022.

[19] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.

[20] Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.

[21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.

[22] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.

[23] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.

[24] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.

[25] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[28] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.

[29] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.


[^0]:    ${ }^{1}$ https://github.com/mistralai/mistral-src

    ${ }^{2}$ https://github.com/skypilot-org/skypilot

    ${ }^{3}$ https://huggingface.co/mistralai

[^1]:    ${ }^{4}$ Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.

</end of paper 2>


