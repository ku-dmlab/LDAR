<paper 0>
# Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation 

Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin,<br>Zicheng Liu, Lijuan Wang<br>Microsoft Azure AI<br>\{zhengyang, jianfw, lindsey.li,keli, chungching.lin, zliu,lijuanw\}@microsoft.com<br>https://idea2img.github.io/


#### Abstract

We introduce "Idea to Image," a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries. Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model's characteristics. The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities. The user preference study validates the efficacy of multimodal iterative selfrefinement on automatic image design and generation.


## 1. Introduction

"Image design and generation" aims to create an image from a high-level user idea. This input IDEA can contain interleaved reference images, such as "the dog looks like the one in the image," or with instructional texts specifying the intended design usage, such as "a logo for the Idea2Img system." To convert IDEA into an image, humans may first draft detailed descriptions of the imagined image, and then use text-to-image (T2I) models [ $31,33,34,36,54]$ to gener-[^0]

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-01.jpg?height=856&width=810&top_left_y=892&top_left_x=1075)

Figure 1. Idea2Img framework enables LMMs to mimic humanlike exploration to use a T2I model, enabling the design and generation of an imagined image specified as a multimodal input IDEA. The iterative process involves LMMs functioning in different roles to refine the image creation. Specifically, LMMs will (1) generate and revise text prompts for the T2I model, (2) select the best draft images, and (3) provide feedback on the errors and revision directions. This multimodal iterative self-refinement process requires LMMs to memorize the T2I model's characteristics observed in previous iterations as humans and adjust T2I prompts accordingly.

ate the image. This manual process for users to search for an ideal detailed description (i.e., T2I prompt) that fits the T2I model typically involves iterative exploration $[44,58]$. As shown in Figure 1, humans may first design and draft an initial T2I prompt based on their imagined IDEA to generate. Then, they can obtain multiple draft images with a T2I model, select the most promising draft, write text feedback,

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-02.jpg?height=545&width=591&top_left_y=232&top_left_x=171)

Concept customization \& visual pointing

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-02.jpg?height=242&width=874&top_left_y=811&top_left_x=170)

Style transfer
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-02.jpg?height=206&width=660&top_left_y=1057&top_left_x=168)

Image manipulation

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-02.jpg?height=220&width=876&top_left_y=1283&top_left_x=169)

In-context entity and style transfer

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-02.jpg?height=494&width=876&top_left_y=1536&top_left_x=169)

Figure 2. Overview of the image design and generation scenarios enabled by Idea2Img. In each sub-figure, the image and text in the left green box are the user input IDEA. The center image is the baseline results directly generated by the same T2I model with a human-written T2I prompt, and the image on the right is generated with the T2I prompt discovered by Idea2Img's iterative self-refinement exploration.

and further revise the T2I prompt. As this iteration progresses, we humans can swiftly grasp the characteristics of a specific T2I model, such as words that the model can not understand, finally producing a good image generated by a suitable T2I prompt. Given the remarkable capabilities of large multimodal models (LMMs) [13, 26, 48], we explore if we can build systems based on LMMs to develop similar iterative self-refinement ability, thereby relieving humans from the tedious process of converting ideas to images.

Iterative self-refinement is one intrinsic ability humans possess when exploring unknown environments and solving complicated problems. Large language models (LLMs) agent systems $[9,23,40]$ have demonstrated the effectiveness of self-refinement in better addressing natural language
processing tasks, such as acronym generation, sentiment retrieval, text-based environment exploration, etc. Transitioning from text-only tasks to multimodal environments poses new challenges of improving, assessing, and verifying multimodal contents, such as multiple interleaved image-text sequences. For example, when learning to use T2I models, LMMs need to improve the generation with revised T2I prompts, assess multiple images in detail to select the best draft, and verify the draft image with the multimodal IDEA to provide text feedback. These steps, each requiring different multimodal understanding capabilities, jointly enable the intriguing multimodal iterative self-refinement ability. Such an LMM framework can automatically learn to tackle various real-world problems [48] via self-exploration, such as navigating GUI to use electronic devices, exploring unknown physical environments via an embodied agent, engaging in electronic games, and so on. In this study, we focus on "image design and generation" as the task to study the multimodal iterative self-refinement ability.

To this end, we introduce Idea2Img, a multimodal iterative self-refinement framework for automatic image design and generation. As illustrated in Figure 1, Idea2Img involves an LMM, GPT-4V(ision) [1,26-28], interacting with a T2I model to probe its usage and find an effective T2I prompt. The LMM will act in different roles to analyze the return signal from the T2I model (i.e., draft images) and design the next round's queries (i.e., text T2I prompts). The three roles of generating T2I prompts, selecting draft images, and reflecting feedback together enable the multimodal iterative self-refinement ability. Specifically, (1) Prompt generation: GPT-4V generates $N$ text prompts that correspond to the input multimodal user IDEA, conditioned on the previous text feedback and refinement history; (2) Draft image selection: GPT-4V carefully compares $N$ draft images for the same IDEA and select the most promising one; (3) Feedback reflection: GPT-4V examines the discrepancy between the draft image and the IDEA. GPT-4V then provides feedback on what is incorrect, the plausible causes, and how T2I prompts may be revised to obtain a better image. Furthermore, Idea2Img is enhanced with a memory module that stores all prompt exploration histories, including previous draft images, text prompts, and feedback. The Idea2Img framework iterates among these three steps with GPT-4V for automatic image design and generation.

To users, Idea2Img functions as an enhanced image design and generation assistant. Compared with T2I models, Idea2Img can handle design instructions instead of requiring detailed image description, support the multimodal IDEA input, and generate images of better semantic and visual qualities. We overview representative image design and generation scenarios in Figure 2. For example, Idea2Img can incorporate the visual design and intended usage description in IDEA, extract arbitrary visual infor- mation from the input image, and process IDEA with arbitrarily interleaved image-text sequences. Built upon these new functionalities and scenarios of interest, we develop an evaluation IDEA set with 104 samples, containing complicated queries that humans may fail in their first trials. We perform user preference studies on Idea2Img with different T2I models. The consistent user preference score improvements on different image generation models, e.g., $+26.9 \%$ with SDXL [31], indicate the effectiveness of Idea2Img in image design and generation.

Our contributions are summarized as follows.

- We study "automatic image design and generation," which aims to create an image from an input IDEA. This high-level idea may contain interleaved reference images, and instructional texts for intended designs.
- We explore the multimodal iterative self-refinement ability in GPT-4V-based systems, showcasing its effectiveness in improving, assessing, and verifying multimodal contents.
- We propose Idea2Img, a multimodal iterative selfrefinement framework that enhances any T2I model for image design and generation, enabling various new image creation functionalities with better image qualities.
- We present an evaluation set with 104 challenging multimodal IDEA. The consistent user preference score gains, when experimented on different image generation models, indicate Idea2Img's effectiveness in automatic image design and generation.


## 2. Related Work

LLM-based self-refinement. Idea2Img is inspired by the effectiveness of iterative self-refinement in LMM-based agent systems [23,29, 40] in exploring unknown environments and tasks, built upon the successful LLM agents [14, 30,32,37,47, 52, 57]. Self-refine [23] takes the same LLM to iteratively critique its outputs, and leverage this feedback to enhance its predictions. This framework shows effectiveness across various NLP tasks, including sentiment reversal, dialogue response, math reasoning, and more. Reflexion [40] explores a self-reflective LLM system on the text-based environment exploration task [41] and multi-hop QA [50]. Despite the success, LLM-based self-refinement naturally can not understand multimodal inputs. Consequently, the explored tasks and environments are limited to the natural language description, such as AlfWorld [41]. Idea2Img delves into the potential of an LMM-based iterative self-refinement system for multimodal environment exploration, from a simple T2I model to other more complicated environments.

Multimodal agents. Our Idea2Img is related to multimodal agents [15,20,22,38,43,45,49,55] that chain external tools
such as T2I or vision-language models with LLMs for multimodal tasks. For instance, MM-ReAct [49] integrates an LLM with multiple vision tools for multimodal reasoning and action, enabling it to solve various complicated visual understanding tasks, ranging from multi-hop document reasoning to open-world video interpretation. Visual ChatGPT [45] empowers ChatGPT to allocate various image generation models, such as Stable Diffusion [34], img2img model [24], ControlNet [56], enabling multi-step visual editing and generation. The primary difference between Idea2Img and existing multimodal agent studies [45,49] lies in the approach to understand the tool usage. Existing studies assume the knowledge of how to use each tool, and provide such information to LLMs via text instructions or incontext examples. In contrast, the optimal usage of the tool remains unknown in Idea2Img and requires iterative exploration. Another minor distinction is that Idea2Img utilizes LMMs instead of LLMs, thereby does not require general visual understanding tools such as a caption model [46].

Extensions of base T2I models. Idea2Img provides a more natural way for users to design and produce their desired visual content. This framework, which extends T2I models for new functionalities, is related to various works in improving base T2I models [31,33,34,36,54]. These studies include extending the base T2I model to better follow user prompts $[5,7,10,11]$, finding magic words in T2I prompts for better visual quality $[44,58]$, supporting extra image input for image manipulation $[6,16,17,24]$, style transfer [12], visual concept customization [3, 8, 18,35,39], and so on. While specialized T2I extensions can address a single specific functionality, Idea2Img offers a more unified and widely applicable framework. That is, a single Idea2Img framework can handle various generation scenarios, ranging from style transfer to attribute customization, without requiring separate models or task-specific model design and finetune. More importantly, Idea2Img effectively collaborates with those enhanced generative models, consistently improving them by exploring suitable text prompts.

## 3. Idea2Img Framework

Figure 3 illustrates the Idea2Img framework. Idea2Img framework involves two core pre-trained models, i.e., the GPT-4V(ision) as the LMM $\mathcal{M}$ and a T2I model $^{2}$ to explore $\mathcal{G}$. Idea2Img also contains a memory $m$ that stores insights on $\mathcal{G}$ discovered by $\mathcal{M}$ during previous iterations.

Execution flow. We begin with an overview of the key steps in $\mathcal{M}$ iteratively exploring the use of $\mathcal{G}$. Starting from the top-left of Figure 3, "initial prompt generation" converts the input multimodal user IDEA into T2I text prompts, later producing multiple draft images with T2I model $\mathcal{G}$. "Draft[^1]

image selection" then selects the best draft image among them for the current iteration. The selected image is either output as the final prediction or continues for further refinement, depending on the stop condition. For the latter, "feedback reflection" compares the current best draft image with the multimodal IDEA, and summarizes the major discrepancy as text feedback. With the iteration history and text feedback, "revised prompt generation" then drafts revised T2I prompts and continues the iterative self-refinement with the new set of draft images.

(1) Initial prompt generation. This step generates $N$ initial T2I prompts $\left\{y_{0}^{0}, \ldots, y_{0}^{N-1}\right\}$ following the input user IDEA $x$, by prompting $\mathcal{M}$ with LMM prompt $p_{g e n}$ :

$$
\begin{equation*}
\left\{y_{0}^{0}, \ldots, y_{0}^{N-1}\right\}=\mathcal{M}\left(x, p_{\text {gen }}\right) \tag{1}
\end{equation*}
$$

The "initial prompt generation" requires $\mathcal{M}$ to understand the multimodal user IDEA $x$ and convert the design IDEA into descriptive T2I prompts that the T2I model could understand. LMM prompt $p_{g e n}$ is a zero-shot prompt without in-context examples.

With the "initial prompt generation" step, Idea2Img can understand user IDEA with interleaved image-text sequences, instead of the text-only T2I prompts containing the image description. Specifically, (1) IDEA can be a highlevel design or usage instead of the detailed image description, such as "a car image for a children's painting competition"; and (2) IDEA can take multiple images and use interleaved text instruction to extract arbitrary visual information of interest, including image style, visual entity, object attributes, etc. Then, in iteration $t=0$ as well as future iterations $t=t$, each T2I prompt $y_{t}^{n}$ is separately sent to the T2I model $\mathcal{G}$, resulting in $N$ draft images $i_{t}^{n}=\mathcal{G}\left(y_{t}^{n}\right), n=0, \ldots, N-1$.

(2) Draft image selection. With the $N$ draft images in iteration $t$, "draft image selection" selects the best draft image $i_{t}^{*}$ by prompting $\mathcal{M}$ with LMM prompt $p_{\text {select }}$ :

$$
\begin{equation*}
i_{t}^{*}=\mathcal{M}\left(i_{t}^{0}, \ldots, i_{t}^{N-1}, x, p_{\text {select }}\right) \tag{2}
\end{equation*}
$$

The design of a "draft image selection" step is motivated by the observation that T2I models could generate bad images with good prompts. This step is designed to filter out low-quality images, and avoid the quality perturbation to dominate the iterative refinement.

The task of selecting the best image requires $\mathcal{M}$ to compare and grade both the semantics and visual quality of $N$ similar draft images. We find such a "spot the difference" task very challenging for LMMs, and only the very recent model [26] is capable of performing the selection reliably. Qualitative examples are shown in Figure 10.

(3) Feedback reflection. After obtaining the selected image $i_{t}^{*}$, the framework checks the stop condition, such as if

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-05.jpg?height=753&width=1726&top_left_y=255&top_left_x=167)

Figure 3. The framework overview of Idea2Img, which takes an LMM [26,27] to explore a T2I model via multimodal iterative selfrefinement, leading to an effective T2I prompt for the input user IDEA. The rounded rectangle shape indicates a GPT-4V call.

the current iteration $t$ exceeds the maximum $T$. Idea2Img then outputs $i_{t}^{*}$ as the output image or proceeds the refinement process to the "feedback reflection" step accordingly.

"Feedback reflection" aims to provide text feedback $f_{t}$ that describes the direction to improve for draft image $i_{t}^{*}$. The steps prompts $\mathcal{M}$ with LMM prompt $p_{f b}$, conditioned on the draft image $i_{t}^{*}$ and memory $m$ :

$$
\begin{equation*}
f_{t}=\mathcal{M}\left(i_{t}^{*}, m, x, p_{f b}\right) \tag{3}
\end{equation*}
$$

"Feedback reflection" takes $\mathcal{M}$ to compare an image $i_{t}^{*}$ with the multimodal user IDEA $x$, and summarize the gap as text feedback $f_{t}$. The step not only requires $\mathcal{M}$ to identify the discrepancy between image $i_{t}^{*}$ and IDEA $x$, but also benefits from writing the major errors to make the iteration effective. In practice, we find it helpful to explicitly specify the aspects to check, such as style, entity, attributes, appearance, etc., via text instructions or in-context examples in LMM prompt $p_{f b}$. Furthermore, we add text instructions to $p_{f b}$ to have $\mathcal{M}$ "focus on one thing to improve in each feedback," and "provide a high-level explanation of how to modify prompts to address the given feedback." Example feedback is shown in Figure 8.

(1) Revised prompt generation. Finally, "prompt generation" takes text feedback $f_{t}$ and memory $m$ to draft $N$ revised prompt $\left\{y_{t+1}^{0}, \ldots, y_{t+1}^{N-1}\right\}$, by prompting $\mathcal{M}$ with LMM prompt $p_{\text {revise }}$ :

$$
\begin{equation*}
\left\{y_{t+1}^{0}, \ldots, y_{t+1}^{N-1}\right\}=\mathcal{M}\left(f_{t}, m, x, p_{\text {revise }}\right) \tag{4}
\end{equation*}
$$

Generating revised prompts requires $\mathcal{M}$ to understand the property of $\mathcal{G}$ stored in memory $m$, thereby drafting new T2I prompts that could most likely address the issue identified in $f_{t}$. We empirically find Idea2Img generates
T2I prompts that work better for $\mathcal{G}$ during iterative selfrefinement, with examples shown in Figure 9.

Memory module. Memory $m$ is one important design in Idea2Img. $m$ has the format of interleaved image-text sequences that store all previous iterations' feedback, selected draft image, and the corresponding text prompts:

$$
\begin{equation*}
m_{t}=\left[y_{0}^{*}, i_{0}^{*}, f_{0}, \ldots, y_{t-1}^{*}, i_{t-1}^{*}, f_{t-1}\right] \tag{5}
\end{equation*}
$$

It allows LMM $\mathcal{M}$ to understand the properties and capabilities of the T2I model $\mathcal{G}$ in use, such as a keyword that $\mathcal{G}$ may not understand or a complicated scene that $\mathcal{G}$ fail to generate, and incorporate such knowledge in generating the revised T2I prompts $y$. For example, it may describe the appearance of a yoga pose in detail, instead of only mentioning the pose name in $y$. Examples are shown in Figures 5-7, when comparing initial and refined $\mathrm{T} 2 \mathrm{I}$ prompts $y_{0}$ and $y_{T}$.

## 4. Experiments

### 4.1. Experiment Settings

Compared model variants. We mainly compare the following three models in image generation.

- "Initial-round manual prompt" is the baseline T2I prompt written by humans with minor prompt engineering. It serves as the baseline of a T2I prompt that merely contains key information in IDEA.
- "Initial-round Idea2Img prompt" is the LMMgenerated T2I prompt in the initial round. Specifically, the max iteration $T=1$, and LMM $\mathcal{M}$ is only used for initial prompt generation and draft image selection, but not feedback reflection nor revised prompt

| Method (\%) | SDXL v1.0 | DeepFloyd IF | SD v2.1 | SD v1.5 | SDXL-img2img | IF-img2img |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| Initial-round manual prompt | 13.5 | 14.4 | 13.5 | 8.6 | 8.6 | 8.6 |
| Initial-round Idea2Img prompt | 29.8 | 34.6 | 40.4 | 43.3 | 34.6 | 38.5 |
| Iterative self-refined Idea2Img prompt | $\mathbf{5 6 . 7}$ | $\mathbf{5 1 . 0}$ | $\mathbf{4 6 . 2}$ | $\mathbf{4 8 . 1}$ | $\mathbf{5 6 . 7}$ | $\mathbf{5 2 . 9}$ |
| $\Delta_{\text {iteration }}(\uparrow)$ | +26.9 | +16.3 | +5.8 | +4.8 | +16.3 | +14.4 |

Table 1. User preference scores when applying Idea2Img onto different image generation models. We observe that "Iterative self-refined Idea2Img prompt" is consistently favored when applied to all experimented image generation models.

generation. This Idea2Img variant is used to ablate Idea2Img's gain from prompt generation and selection, $v s$. the further iterative refinement.

- "Iterative self-refined Idea2Img prompt" is complete Idea2Img pipeline with the max iteration $T=3$.

Evaluation samples and metrics. For the quantitative evaluation, we collect a dataset of 104 user IDEA as input queries. Among them, 33 queries contain text only, 43 queries contain an image-text sequence with a single image, and the remaining 28 contains a sequence with two or more images. The text in most IDEA contains not only descriptive content text that describes the scene to generate, but also instructional text such as "a logo for commercial advertising" or "generate the pointed dog in the blue color." All test queries are manually composed.

We then perform the user preference study as the main quantitative metric. Users are presented with the IDEA and multiple images to select the best one for each IDEA. The evaluation script automatically shuffles the order during evaluation to prevent the influence of image orders.

Experimented T2I models. We experiment Idea2Img on a wide variety of T2I model $\mathcal{G}$ with diverse model capacities and functionalities. Specifically, we study Stable Diffusion (SD) v1.5 [34], SD v2.1, SDXL v1.0 with refiner [31], and DeepFloyd IF (IF-I-XL and IF-II-L) [19]. Other than T2I models, we also consider the img2img pipeline (i.e., SDEdit [24]) for SDXL and DeepFloyd IF, as a demonstration of using Idea2Img for the text-conditioned image-toimage generation. The default strength $t_{0}$ in the img2img pipeline is 1.00. SDXL-img2img and IF-img2img are the same as SDXL and IF (i.e., T2I) when IDEA contains text only, and condition on the first image with IDEA contains multiple images. LMM prompts $p_{\text {gen }}, p_{\text {select }}, p_{f b}, p_{\text {revise }}$ are kept the same for all experimented T2I models.

### 4.2. Image Generation Results

User preference evaluation. Table 1 compares the user preference when selecting from the three images generated by "initial-round manual prompt," "initial-round Idea2Img prompt," and "iterative self-refined Idea2Img prompt," using the same T2I model, for each user IDEA. Among T2I models with different model sizes and functionalities, Idea2Img leads to consistent improvements in user preference. The initial-round Idea2Img prompt already improves the initial-round manual prompt, by effectively un-

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-06.jpg?height=431&width=789&top_left_y=543&top_left_x=1080)

Figure 4. User preference scores among T2I models before and after iterative self-refinement. We observe that the initially favored T2I model, SDXL, benefits more from the Idea2Img iteration.

derstanding the multimodal user IDEA and selecting the best draft images. The full Idea2Img framework further improves from the initial-round Idea2 Img results with the multimodal iterative self-refinement. For example, when using SDXL v1.0, users prefer the images generated by Idea2Img $59 / 104=56.7 \%$ times, compared with the baseline of $14 / 104=13.5 \%$. Similar improvements are observed on all experimented T2I models, as shown in the bolded row of "iterative self-refined Idea2Img prompt."

Furthermore, we examine which T2I model benefits the most from the LMM iterative self-refinement. By comparing the $\Delta_{\text {iteration }}$ in Table 1 that represents the difference between first-round and iterative Idea2Img user preference, we observe that stronger T2I models tend to benefit more from LMM refinements. For example, SDXL and IF become more favored $26.9 \%$ and $16.3 \%$ times, compared with SD v2.1 and SD v1.5's $5.8 \%$ and $4.8 \%$. The trend that stronger T2I models benefit more from Idea2Img is also observed in Figure 4's analysis, where users pick their preferred image generated by different T2I models. After Idea2Img's iterative refinement, the initially favored model SDXL benefits more from the iteration, resulting in an even higher user preference rate. We conjecture that the better language understanding ability in stronger T2I models enables them to better follow revised T2I prompts. They also have a better image generation capability that makes it possible to generate challenging scenes, when given a good T2I prompt optimized by Idea2Img. We are eager to see how this trend evolves as we experiment with even stronger T2I models [2,25] in the future. Nonetheless, Idea2Img is effective across T2I models of varying capacities, consistently leading to a higher user preference score.

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-07.jpg?height=2049&width=1740&top_left_y=252&top_left_x=152)

Figure 5. The comparisons among the initial-round manual prompts, initial-round Idea2Img prompts, and the iterative self-refined Idea2Img prompts, with the SDXL v1.0 [31] used as the T2I model.

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-08.jpg?height=1971&width=1726&top_left_y=245&top_left_x=167)

Figure 6. The comparisons among the initial-round manual prompts, initial-round Idea2Img prompts, and the iterative self-refined Idea2Img prompts, with the SDXL-img2img [24,31] used as the image generation model. Instead of random noise, the image generation starts from the input image with added noise [24], showing the effectiveness of Idea2Img on text-conditioned image-to-image pipelines.

Iterative self-refined

Idea2Img prompt
Input user IDEA

SD v1.5

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=211&width=421&top_left_y=388&top_left_x=169)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=190&width=404&top_left_y=623&top_left_x=183)

SD v2.1
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=414&width=418&top_left_y=863&top_left_x=170)

DeepFloyd IF
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=422&width=422&top_left_y=1320&top_left_x=168)

DeepFloyd IF img2img
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=434&width=422&top_left_y=1788&top_left_x=168)

Initial-round manual prompt
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=1844&width=446&top_left_y=388&top_left_x=596)
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=900&width=864&top_left_y=384&top_left_x=1029)

black shirt.
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-09.jpg?height=428&width=644&top_left_y=1320&top_left_x=1244)

Figure 7. The comparisons among the initial-round manual prompts, initial-round Idea2Img prompts, and the iterative self-refined Idea2Img prompts, with other image generation models, including SD v1.5, SD v2.1 [34], DeepFloyd IF, and IF-img2img [19].

Qualitative comparisons. Figures 5-7 show the qualitative results of the comparison in Table 1 to probe the improvements made by Idea2Img. Specifically, Figure 5 presents examples of Idea2Img explores the use of SDXL, a representative T2I model. Figure 6 examines SDXL-img2img, a representative text-conditioned image-to-image model that adds noise to the input image and then performs textconditioned denoising [24]. Figure 7 contains the results of Idea2Img working with other image generation models.

Idea2Img could help users generate images that better follow IDEA, such as the one with correct object counts and rendered scene texts in Figures 5(a,b). Idea2Img enables visual content design, in addition to generation only that requires a detailed visual content description. For example in Figure 5(c), Idea2Img designs visual logo based on the user IDEA of "having a logo for a 2024 conference in Seattle." Idea2Img can also understand user IDEA to search for images with high aesthetic scores and great visual details, or its opposite direction with "minimal face details" in Figure 5(d). The power of LMMs allows Idea2Img to extract arbitrary information from the input image for visual generation. This could be any specific object in the image, such as "the dog on the left" or "the dog pointed to via a red circle" in Figure 5(e). Figure 5(f) shows an example of extracting the image style that requires domain knowledge for humans to accurately describe. The image input can even be an in-context example that defines the desired image transformation, such as the visual style transfer shown in Figure 5(g). The ability to extract arbitrary information from the input image can be seamlessly extended to compose multiple visual and text conditions, such as composing the camera angle and image style in Figure 5(h), and two objects in Figure 5(i).

Idea2Img is also effective in finding T2I prompts for the text-conditioned image-to-image model SDXL-img2img, as shown in Figure 6. Figures 6(c) and (d) illustrate generating images that follow and differ from the reference image style of watercolor and impressionist, respectively. Idea2Img can design visual contents with the inspiration of input images, e.g., a cartoon drawing that blends the llama and the person in suits in Figure 6(f), and composing the coke with the table in an imaginative way in Figure 6(g). Figure 6(h) illustrates representing the anticipated action of rolling the sushi with a generated image. Furthermore, Figure 7 shows the Idea2Img results on other T2I models, including SD v1.5, v2.1, DeepFloyd IF, and IFimg2img. Despite the variance in the base T2I models' capacity, Idea2Img consistently helps design and generate better images.

How Idea2Img may assist humans? We use selected qualitative results to highlight the scenarios where humans might find Idea2Img most helpful in image design and generation, compared with conventional T2I generation.

- Multimodal IDEA inputs. Idea2Img provides a more natural way for human interaction, where users do not have to describe their desired image solely through texts and painstakingly search for the right terminology. Instead, the multimodal IDEA allows Idea2Img to precisely extract specific elements from one or multiple input images, such as the dog breed and color, pointed objects, artist style, camera view, and more, as shown in Figure 5. Finding proper words that the T2I model can understand for such visual concepts could be tedious for humans, e.g., the art style "with bold lines and bright colors, with a cubist and surrealistic style, with a focus on geometric shapes and patterns." in Figure 5(f). Idea2Img automates this process via Idea2Img iterative self-refinement.
- Instructional texts for intended designs and usages. Vanilla T2I models struggle to understand T2I prompts that describe the intended visual design or purpose of the generated image, such as "a logo for a 2024 conference held in Seattle" or "a car image for a children's painting competition." Instead, the prompts need to be a comprehensive description of the image to generate, e.g., "a modern and minimalist logo design, featuring the Seattle skyline in the center and the conference title below it with a blue and green color scheme" (Figure 5(c)). This demands extra drafting effort from users. In contrast, as shown in Figures 5-7, Idea2Img effectively understands the instructional texts in IDEA and creates images accordingly.

Finally, the GPT-4V iterative refinement allows Idea2Img to generate images with better semantic and visual qualities, leading to an effective automatic image creation assistant.

### 4.3. LMM Feedback, Revision, and Selection

One may wonder about the behavior and performance of LMM playing each role in Idea2Img's iterative selfrefinement pipeline, i.e., "feedback reflection," "revised prompt generation," and "draft image selection." We show corresponding qualitative results as follows.

Feedback reflection. Figure 8 shows the text feedback generated by GPT-4V for the user IDEA and the draft image and T2I prompt. Idea2Img can effectively check if the generated image is correct, such as the number of oranges in (a) and the misspelled scene text "ALURE RESEACHE" in (b). In addition to text descriptions in IDEA, Idea2Img can verify if the draft image corresponds to the visual descriptions in IDEA. This includes the color and breed of the dog in (e), the exact art style in (f), and the same cloth and pose in (g). Furthermore, Idea2Img can understand and verify the IDEA containing interleaved image-text pairs, as shown in Figures 8(h,i).

In addition to identifying the discrepancy, Idea2Img also

Input user IDEA

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-11.jpg?height=214&width=545&top_left_y=283&top_left_x=194)

(b)

a whole cake on the table with words Azure Research written on the cake

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-11.jpg?height=209&width=545&top_left_y=752&top_left_x=194)

(d)

an image of a car perfect for a childrens painting competition
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-11.jpg?height=1128&width=550&top_left_y=1210&top_left_x=190)

## Draft prompt and image to revise

## Text feedback

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-11.jpg?height=220&width=526&top_left_y=280&top_left_x=755)

The image generated by the $\mathrm{Al}$ art generation model does not follow the user imagined IDEA of the scene as there are only 6 whole oranges in the image instead of 7 . To address this, the sentence prompt can be modified to specify that there should be 7 whole oranges on the table, such as in one row with exactly 7 oranges

The words on the cake in the generated image are misspelled as "ALURE RESEACHE" instead of "Azure Research". To address this, the sentence prompt should be modified to specifically mention that the words on the cake should be spelled correctly as "Azure Research".

The person in the image is not practicing yoga mountain pose, but in a different yoga pose. Because the prompt has already mention mountain pose, the Al art model might not understand what is mountain pose. To address this, the sentence prompt should be modified to specifically mention what is a mountain pose: posture where one stands with feet hip-width apart, arms stretched overhead, and gaze forward.

The car in the generated image is not perfect for a children's painting competition as it is too detailed and realistic. To address this, the sentence prompt should be modified to specifically mention that the car should be simple and cartoon-like, with less details and more vibrant colors.

The dog in the generated image is not in the blue color, and does not look like the one in the given image. The dog in the given image is a pug, while the dog in the generated image is a Labrador Retriever. To address this issue, the sentence prompt should be modified to specifically mention the breed of the dog as a pug, and the dog is in the blue color.

The style of the painting in the generated image is still not similar to the one in the given image. The given image has a cubist style with faces and objects broken up into geometric shapes and reassembled abstractly, while the generated image has a more realistic style. To address this, the sentence prompt should be modified to specifically mention the style in the given image is 'with fragmented geometric shapes that are reassembled abstractly to form objects'.

The pose of Mr Bean in the generated image is different from the one in the given image. This is because the prompt mentions 'as the given image' but the Al art model can not understand image inputs. To address this, the sentence prompt should be modified to specifically mention the pose in the given image is 'with his right arm stretched out and his left arm bent, holding the tennis racket with both hands'.

The image generated by the Al art generation model does not follow the user imagined IDEA of the scene as the image style is not similar to Claude Monet's impressionist paintings. To address this, the sentence prompt can be modified to specify that the image style should be similar to Claude Monet's impressionist paintings, with a bright and sunny atmosphere.

The dog in the generated image is not the same breed as the one in the given image. The dog in the given image is a Cavalier King Charles Spaniel, while the dog in the generated image is a husky. To address this issue, the sentence prompt should be modified to specifically mention the breed of the dog as a Cavalier King Charles Spaniel.

Figure 8. Examples of the generated text feedback. The left column shows the multimodal input user $I D E A$, and the center column shows the draft image to process as well as its corresponding text prompts. The right column shows the text feedback generated by GPT-4V. The dark blue color highlights the identified discrepancies.

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-12.jpg?height=258&width=436&top_left_y=256&top_left_x=152)

Draft prompt and image to revise
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-12.jpg?height=468&width=1288&top_left_y=260&top_left_x=172)

Revised prompt and image
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-12.jpg?height=428&width=1284&top_left_y=514&top_left_x=172)

The image generated by the Al art generation model does no follow the userimagined JEA of the scene as there are
bananas on the plate and orange juice in the glass. To address this, the sentence prompt can be modified to specify
that there should be no bananas on the plate and no orange juice in the glass.
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-12.jpg?height=838&width=1720&top_left_y=304&top_left_x=172)

The image generated by the Al art generation model does not not ready to take a screenshot. To address this, the sentence
prompt can be modified to specify that the fingers should be pressing the side button and the volume up button at the
same time.
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-12.jpg?height=1500&width=1722&top_left_y=732&top_left_x=170)

Figure 9. Examples of the revised prompts. The four columns, from left to right, show the input user IDEA, the draft image to be revised, generated text feedback, and the revised T2I prompt and image. The dark blue color highlights the identified discrepancies in text feedback, and how they are addressed in the revised T2I prompt. We note that the example only shows a single round of self-refinement. Therefore, the revised T2I prompt may have remaining issues to be further addressed.

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=255&width=421&top_left_y=260&top_left_x=169)

(b) an image of a hand holding an iphone 12 to
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=862&width=422&top_left_y=745&top_left_x=169)

(g)

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=149&width=419&top_left_y=1663&top_left_x=173)
textual description that keeps the content
in the people waving hand image, with the
correct style pattern.
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=418&width=422&top_left_y=1828&top_left_x=168)

Draft images to select
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=1984&width=230&top_left_y=300&top_left_x=600)
![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=1984&width=462&top_left_y=300&top_left_x=820)

Text selection Image 0: The cake is present, and the words Happy Birthday are written on the cake.
However, the words are not written correctly, and there are extra decorations on the However, the words are not written correctly, and ther
cake that are not mentioned in the IDEA. Overall score: 6 . Image 1: The cake is present, and the words Happy Birthday are written on the cake.
However the However, there are extra decorations on the cake that are not mentioned in the IDEA.
Overall score 7 . Overall score: 7

Image 2: The cake is present, and the words Happy Birthday are written on the cake
The cake is also on a tabo The cake is also on a table, as mentioned in the IDEA. However, there are extra
decorations on the cake that are not mentioned in the IDEA. Overall score: 8 . 2 - This image has the highest overall score and best follows the content in the IDEA.

![](https://cdn.mathpix.com/cropped/2024_06_04_84622e890cb78b254827g-13.jpg?height=206&width=588&top_left_y=531&top_left_x=1294)

Image 0: The image has two whole red apples next to bananas. However, there are multiple bananas instead of a single banana as described in the IDEA. Overall score: 8 . Image 1: The image has red apples and bananas, but there are multiple bananas and
three apples are clearly visible. This does not strictly follow the content in the IDEA. three alpples are
Overall score: 6 .

Image 2: The image has two whole red apples next to a single banana, which strictly follows the content in the IDEA. Overall score: 10 . Image 2 strictly follows the content in the IDEA by having two whole red apples next
to a single banana.

Image 0 : The dog in the generated image is a husky, which is the same as the given
image. The dog is also playing with a rellow tall image. The dog is also playing with a yellow ball and running on the street. Overa
score: 9 . Image 1: image. However, the ball is pink instead of yellow. Overall score: 8 . Image 2: The dog in the generated image is a husk, which is the same as the given image. However, the dog is not running on the street, but instead is biting a ball.
Overall score: 7 . Overall score: 7.
0 is the image $w$

is the mage with the best overall quality, as it strictly follows the content in IDEA and
has the highest score.

Image 0: The generated image is a hand drawing of a room with a desk and bookshelf, but it does not have a bed for people to sleep. Overall score: 7 . Image 1: The generated image is a hand drawing of a room with a bed and a desk,
which follows the content in IDEA Overall score: 9 . Image 2: The generated image is a hand drawing of a room with a bed and a lamp. The
style is similar to style is similar to the given image, but there is no study table. Overall score: 6 . 1 is the image with the best overall quality, as it follows bed for people to sleep

Image 0 : The portrait of Bill Gates is present, but the style is not similar to the one in the given image. The generated image has a more realistic style, while the given Image 1: The portrait of Bill Gates is present, but the style is not similar to the one in the given image. The generated image has a more realistic style, while the given image has a more impressionistic style. Additionally, the background the generate
image is different from the given image. Overall score: 4 image is different from the given image. Overall score: 4 . Image 2: The portrait of Bill Gates is present and the style is similar to the one in the
given image. The generated image has a more impressionistic style, which is what the given image. The generated im
user imagined. Overall score: 9 .
2.

2 is the image with the best overall quality.

Image 0 : The generated image has the correct style pattern, but the content is not Overall score: 4 .

Image 1: The generated image has the correct style pattern and the content is correc. The image has a person waving hand. Overall score: 9 . Image 2: The generated image has the correct style pattern, but the content is not
correct. The image has a dog, while the correct content is 'people wavin Overall score: 4 .
1 is the image with the best overall quality.

Image 0 : The image follows the content in IDEA, with the correct objects and placement. However, the Coca Cola bottle and the table is not the same as the
the given image Overall score: 8 . the given image. Overall score: 8 .
Image 1: The image follows the
.

Image 1: The image follows the content in IDEA, with the correct objects and
placement The Coca Cola bottle is the same as the one in the given image Over score: 9 .
II
mos . placed on the table but is instead blended into the table. Overall score: 4 . 1 is the image with the best overall quality, as it follows the content in IDEA and has the correct objects and placement.

Image 0 : The dog in the generated image is in blue color and the image style is similar to the one in the IDEA. However, the dog is not running on the grass as described in
the IDEA. Overall score: 7 . Image 1: The dog in the The image style is also similar to the one in the IDEA. Overall score: 9 . Image 2: The dog in the generated image is in blue color and is running on the grass. However, the image style is not similar to the one in the IDEA. Overall score: 6 . has a higher score than the other two images.

Figure 10. Examples of the draft image selection. The right column shows justification ratings and the final selection. The dark blue color highlights the identified problems and justifications for the draft image selection. The selected draft image in the round is visualized with the blue box shown in the middle column.
points to the plausible directions that may improve the T2I prompt in the text feedback. For example, in Figure 8(c), GPT-4V mentions that "the person is not in yoga mountain pose, but the T2I prompt has already mentioned mountain pose," "the AI model might not understand what mountain pose is, and prompt should be modified to specifically mention what mountain pose is." Similarly, in Figure 8(d), Idea2Img provides guidance to have generated images better follow the user intention of "an image for a children's painting competition," by "specifically mentioning that the car should be simple and cartoon-like."

Revised prompt generation. Figure 9 compares the T2I prompts before and after revision, and visualizes how text feedback helps the revision. For example, (a) the revised T2I prompt includes a detailed description of the "yoga dolphin pose" to generate the correct body pose; (b) the revised T2I prompt mentions "an empty plate with no other objects" to avoid the T2I model misunderstand the prompt "no bananas;" (c) T2I model generates the correct hand gesture with Idea2Img providing text description on how to take a screenshot. Idea2Img also effectively addresses the identified errors in text feedback and improves the prompts for multimodal input IDEA, including the dog color in Figure $9(\mathrm{~d})$, the llama design in Figure 9(e), the study area in Figure 9(f), the human gesture in Figure $9(\mathrm{~g})$, the dog breed and human clothing in Figure 9(h), and the color of the two cats in Figure 9(i).

Draft image selection. Figure 10. T2I models may generate low-quality images even with good T2I prompts. To enable refinement in each iteration, it is critical to reduce such generation noise by selecting from multiple draft images. Performing such selection requires GPT-4V to compare multiple similar draft images and pick the one with the best overall quality. Figure 10 shows the selection results generated by GPT-4V. The LMM prompt is designed such that GPT-4V gives justifications and scores for each draft image, in addition to the final selection index. Such intermediate thoughts not only help humans interpret the selection process, but also serve as the chain of thought to improve the selection performance. We observe that GPT-4V can compare different aspects mentioned in the IDEA and give reasonable scores and selection index. For example, checking the scene text spelling in Figure 10(a); verifying the phone screen and model in Figure 10(b); counting the number of apples and bananas in Figure 10(c); verifying the ball color and dog action in Figure 10(d); finding the image with both sleep and study area in Figure 10(e); selecting the image that best fits the given image style in Figure 10(f); verifying the image content and style in Figure 10(g); locating the best blended image in Figure 10(h); and finding the image with correct dog color and image style in Figure 10(i).

## 5. Limitation and Discussion

Tasks beyond image generation. Idea2Img explores the emergent ability of multimodal self-refinement in LMMbased systems, through the image design and generation task. Specifically, Idea2Img views the T2I model to use as an unknown multimodal environment to explore, and iteratively refines T2I prompts to find its optimal usage. This concept mirrors the intrinsic human approach of iterative problem-solving when faced with unknown environments or complex challenges. We leave its extension to other intriguing tasks [48], e.g., GUI navigation, embodied agents, and complicated visual reasoning, for future exploration.

From a single image generation model to multiple tools. Idea2Img explores the usage of a single image generation model, including a text-to-image model [34] or a textconditioned image-to-image model [24]. When needed, other specialized generative models like ControlNet [56], inpainting [4], region-controlled T2I generation [21, 51], customized generation [8,35], and video generation [42,53] can be seamlessly switched as replacements. That is, Idea2Img could broadly boost different image generation models with diverse specialties by exploring their optimal text description or instruction prompts.

Beyond a single generation model, Idea2Img can also be used to allocate multiple tools as in multimodal agent studies [45,49]. In this case, Idea2Img isn't limited to optimizing the use of individual tools but also investigates their effective collaboration when used together, such as generator selection and multi-step visual generation.

Consolidating explored knowledge. We have shown the effectiveness of LMM iterative self-refinement in automatic image design and generation. Going beyond merely preserving the probed T2I model's properties in memory, Idea2Img can also help to consolidate or distill the explored knowledge into T2I model parameters, such that no inference-time iterative refinement is needed when encountering similar generation scenarios. For example, one could collect a dataset using Idea2Img for a scenario of interest, and fine-tune a T2I model with the explored selfrefinement trajectory. Storing the probed knowledge as sample-agnostic prompt for each image generation model is another promising direction $[14,32,57]$.

## 6. Conclusion

We have presented Idea2Img, a multimodal iterative selfrefinement framework that leverages GPT- $4 \mathrm{~V}$ (ision) for image design and generation. Idea2Img delves into the emergent capabilities of iterative self-refinement in LMM-based systems, showcasing its effectiveness in improving, assessing, and verifying the generated multimodal content. The user preference study demonstrates Idea2Img's capability in assisting humans to find the optimal usage of a T2I model for automatic image design and generation.

## Acknowledgment

We are deeply grateful to OpenAI for providing access to their exceptional tool $[1,26-28]$. We also extend heartfelt thanks to our Microsoft colleagues for their insights, with special acknowledgment to Faisal Ahmed, Ehsan Azarnasab, and Lin Liang for their constructive feedback.

## References

[1] Chatgpt can now see, hear, and speak. https: / openai. com/blog/chatgpt-can-now-see-hear-andspeak, 2023. 3, 15

[2] Midjourney. https://www.midjourney.com/, 2023. 6

[3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel CohenOr, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from a single image. arXiv preprint arXiv:2305.16311, 2023. 4

[4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18208-18218, 2022. 14

[5] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 4

[6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392-18402, 2023. 4

[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. arXiv preprint arXiv:2301.13826, 2023. 4

[8] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William W Cohen. Subject-driven text-to-image generation via apprenticeship learning. arXiv preprint arXiv:2304.00186, 2023. 4, 14

[9] Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.2

[10] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381, 2023. 4

[11] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Trainingfree structured diffusion guidance for compositional text-toimage synthesis. In The Eleventh International Conference on Learning Representations, 2022. 4

[12] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015. 4

[13] Google. Bard. https://bard.google.com, 2023. Accessed: 2023-07-17. 2
[14] Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, and Nan Duan. Learning to program with natural language. arXiv preprint arXiv:2304.10464, 2023. 3,14

[15] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953-14962, 2023. 3

[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2022. 4

[17] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007-6017, 2023. 4

[18] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931-1941, 2023. 4

[19] DeepFloyd Lab. Deepfloyd if. https://github.com/ deep-floyd/IF, 2023. 6, 9

[20] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 2023. 3

[21] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511-22521, 2023. 14

[22] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023. 3

[23] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. 2, 3

[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 4, 6, 8, 10, 14

[25] OpenAI. Dall.e 3 system card. https://cdn.openai. com/papers/DALL_E_3_System_Card.pdf, 2023. 6

[26] OpenAI. Gpt-4 technical report, 2023. 2, 3, 4, 5, 15

[27] OpenAI. Gpt-4v(ision) system card. 2023. 3, 5, 15

[28] OpenAI. Gpt-4v(ision) technical work and authors. https: / / cdn.openai. com / contributions / gpt-4v . pdf, 2023. 3, 15

[29] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188, 2023. 3

[30] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023. 3

[31] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 1, 3, 4, 6, 7, 8

[32] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495, 2023. 3, 14

[33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1,4

[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022. 1, 4, 6, 9, 14

[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510, 2023. 4, 14

[36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 1, 4

[37] Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. 3

[38] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 3

[39] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testtime finetuning. arXiv preprint arXiv:2304.03411, 2023. 4

[40] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. 2,3

[41] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht.
Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. 3

[42] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 14

[43] Ddac Surs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023. 3

[44] Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. Diffusiondb: A large-scale prompt gallery dataset for text-toimage generative models. arXiv preprint arXiv:2210.14896, 2022. 1,4

[45] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 3, 4, 14

[46] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280, 2022. 4

[47] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409, 2023. 3

[48] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023. 2, 3, 14

[49] Zhengyuan Yang*, Linjie Li*, Jianfeng Wang*, Kevin Lin*, Ehsan Azarnasab*, Faisal Ahmed*, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 3, 4, 14

[50] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. 3

[51] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14246-14255, 2023. 14

[52] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. 3

[53] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 14

[54] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. 1, 4

[55] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 3

[56] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. 4, 14

[57] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. arXiv preprint arXiv:2308.10144, 2023. 3,14

[58] Wanrong Zhu, Xinyi Wang, Yujie Lu, Tsu-Jui Fu, Xin Eric Wang, Miguel Eckstein, and William Yang Wang. Collaborative generative ai: Integrating gpt-k for efficient editing in text-to-image generation. arXiv preprint arXiv:2305.11317, 2023. 1,4


[^0]:    ${ }^{1}$ Short for "Idea2Img." Logo design assisted by Idea2Img.

[^1]:    ${ }^{2}$ We will show image generation models other than T2I later in experiments. For clarity, we use T2I as a representative for method introduction.

</end of paper 0>


<paper 1>
# DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual Design 

Kevin Lin ${ }^{*}$, Zhengyuan Yang ${ }^{*}$, Linjie Li, Jianfeng Wang, Lijuan Wang* ${ }^{\star}$<br>Microsoft Corporation<br>${ }^{*}$ Equal Contribution * Project Lead<br>\{keli, zhengyang,lindsey.li,jianfw,lijuanw\}@microsoft.com<br>https://design-bench.github.io/


#### Abstract

We introduce DEsignBench, a text-to-image (T2I) generation benchmark tailored for visual design scenarios. Recent T2I models like DALL-E 3 [8, 67, 66] and others, have demonstrated remarkable capabilities in generating photorealistic images that align closely with textual inputs. While the allure of creating visually captivating images is undeniable, our emphasis extends beyond mere aesthetic pleasure. We aim to investigate the potential of using these powerful models in authentic design contexts. In pursuit of this goal, we develop DEsignBench, which incorporates test samples designed to assess T2I models on both "design technical capability" and "design application scenario." Each of these two dimensions is supported by a diverse set of specific design categories. We explore DALL-E 3 together with other leading T2I models on DEsignBench, resulting in a comprehensive visual gallery for side-by-side comparisons. For DEsignBench benchmarking, we perform human evaluations on generated images in DEsignBench gallery, against the criteria of image-text alignment, visual aesthetic, and design creativity. Our evaluation also considers other specialized design capabilities, including text rendering, layout composition, color harmony, 3D design, and medium style. In addition to human evaluations, we introduce the first automatic image generation evaluator powered by GPT-4V. This evaluator provides ratings that align well with human judgments, while being easily replicable and cost-efficient. A high-resolution version is available at this link.


## Contents

List of Figures ..... 2
1 Introduction ..... 5
1.1 Motivation and Overview ..... 5
2 DALL-E 3 Basics and DEsignBench Settings ..... 7
2.1 DALL-E 3's Working Modes ..... 7
2.2 T2I Generation Capability Overview ..... 7
3 Design Technical Capability ..... 13
3.1 Text Rendering and Typography ..... 13
3.2 Layout and Composition ..... 13
3.3 Color Harmony ..... 14
3.4 Medium and Style ..... 14
3.5 3D and Cinematography ..... 14
4 Design Scenario ..... 31
4.1 Infographics Design ..... 31
4.2 Animation/Gaming Design ..... 31
4.3 Product Design ..... 32
4.4 Visual Art Design ..... 32
5 DEsignBench and Evaluation Results ..... 56
5.1 Evaluation Method and Metric ..... 56
5.2 Compared T2I Models ..... 57
5.3 Evaluation Results ..... 58
5.4 Limitations of DALL-E 3 ..... 69
6 Conclusions ..... 73
A DEsignBench Gallery:
Comparisons among SDXL, Midjourney, Ideogram, Firefly2, and DALL-E 3 ..... 80
List of Figures
1 DEsignBench overview ..... 6
2 ChatGPT prompt expansion ..... 9
3 prompt following: detailed descriptions ..... 10
4 prompt following: uncommon scenes ..... 11
5 other challenge prompts ..... 12
6 text rendering: stylized text ..... 15
7 text rendering: low-frequency words ..... 16
8 text rendering: long text ..... 17
9 layout and composition: diagram, chart, table, calendar ..... 18
10 layout and composition: multi-panel layout ..... 19
11 color harmony: impression sunrise ..... 20
12 color harmony: starry night ..... 21
13 medium and style: cats 1 ..... 22
14 medium and style: cats 2 ..... 23
15 medium and style: cats 3 ..... 24
$163 \mathrm{D}$ and cinematography: shape and lighting ..... 25
17 3D and cinematography: lighting effect ..... 26
18 3D and cinematography: camera view points ..... 27
19 3D and cinematography: camera settings and lens ..... 28
20 3D and cinematography: crowded scene 1 ..... 29
21 3D and cinematography: crowded scene 2 ..... 30
22 infographics design: storybook, poster, and menu ..... 33
23 infographics design: industrial drafts, floorplans, and GUI ..... 34
24 infographics design: ads, marketing posters, and book covers ..... 35
25 infographics design: movie poster, ads ..... 36
26 infographics design: logo and postcards . ..... 37
27 infographics design: greeting cards ..... 38
28 infographics design: coloring book ..... 39
29 product design: sticker ..... 40
30 animation design: cinematic scenes ..... 41
31 animation design: six-panel comic strip ..... 42
32 animation design: six-panel comic strip ..... 43
33 animation design: six-panel comic strip ..... 44
34 animation design: storyboard ..... 45
35 animation design: cartoon, emoji, anime ..... 46
36 gaming design: gaming 1 ..... 47
gaming design: gaming 2 ..... 48
38 product design: product and jewellery 1 ..... 49
product design: product and jewellery 2 ..... 50
product design: fashion ..... 51
product design: change clothes ..... 52
visual art design: 3D sculpture and historical art ..... 53
visual art design: historical art, time-space travel ..... 54
visual art design: knolling ..... 55
Human evaluation results on DEsignBench. ..... 59
comparison between GPT-4V and human judgments on DEsignBench ..... 60
GPT-4V evaluation on DEsignBench ..... 61
GPT-4V evaluation on DEsignBench ..... 62
failure cases: uncommon scenes ..... 70
failure cases: document design ..... 71
failure cases: image generation ..... 72
text rendering comparisons ..... 81
text rendering comparisons ..... 82
layout and document comparisons ..... 83
layout and document comparisons ..... 84
color comparisons ..... 85
color comparisons ..... 86
artistic medium comparisons ..... 87
artistic medium comparisons ..... 88
60 style and 3D comparisons ..... 89
61 style and 3D comparisons ..... 90
62 camera settings comparisons ..... 91
63 color comparisons ..... 92
64 crowded scene comparisons ..... 93
65 crowded scene comparisons ..... 94
66 storybooks, academic posters, and menus comparisons ..... 95
67 storybooks, academic posters, and menus comparisons ..... 96
68 industrial drafts, floorplans, and GUI comparisons ..... 97
69 industrial drafts, floorplans, and GUI comparisons ..... 98
70 ads, posters, and book cover comparisons ..... 99
71 ads, posters, and book cover comparisons ..... 100
72 movie posters and ads comparisons ..... 101
73 movie posters and ads comparisons ..... 102
74 infographics design comparisons ..... 103
75 infographics design comparisons ..... 104
76 cinematic scene comparisons ..... 105
77 cinematic scene comparisons ..... 106
78 comic strip comparisons ..... 107
79 comic strip comparisons ..... 108
80 storyboard comparisons ..... 109
81 storyboard comparisons ..... 110
82 cartoon comparisons ..... 111
83 cartoon comparisons ..... 112
84 game design comparisons ..... 113
85 game design comparisons ..... 114
86 product design comparisons ..... 115
87 product design comparisons ..... 116
88 product design comparisons ..... 117
89 product design comparisons ..... 118
90 fashion design comparisons ..... 119
91 fashion design comparisons ..... 120
92 camera settings comparisons ..... 121
93 color comparisons ..... 122
94 3d art comparisons ..... 123
95 3d art comparisons ..... 124
96 historical art comparisons ..... 125
97 historical art comparisons ..... 126
98 knolling design comparisons ..... 127
99 knolling design comparisons ..... 128
100 DEsignBench logo design by DALL-E 3. ..... 129

## 1 Introduction

### 1.1 Motivation and Overview

Advancements in text-to-image (T2I) generation [1-3, 30, 37, 89, 24, 76, 96, 14, 85, 86, 78, 73, 8, $67,42]$ have shown remarkable capabilities in generating high-fidelity images that follow the user input text prompts. Many known challenges [58, 80, 26, 39], such as the prompt "A horse riding an astronaut" to test prompt following, text rendering, and distortions in the human face and hands generation, have been significantly improved by recent advancements, with examples postponed in Section 2. The rapid advancement naturally raises a question: what is the next goal to make T2I generation even more practically valuable? In this work, we focus on designing scenarios, and examine how the state-of-the-art T2I models can assist visual design [82, 72, 50, 51, 38, 52, 70, 100], in addition to merely generating visually pleasant results.

To this end, we present a new evaluation benchmark named DEsignBench to examine T2I models' capabilities in assisting visual design. In addition to the base T2I capabilities in standard T2I benchmarks $[44,80,96,34,21,39]$, DEsignBench evaluates visual design from two unique perspectives, i.e., the core design technical capability and the design application scenarios. We then collect evaluation prompts organized into each category and aspect. We collect the results of the state-of-the-art T2I models [73,3,2,1, 8, 67] into our DEsignBench gallery, and perform both human and GPT-4V $[68,69,93]$ evaluations on the DEsignBench. Figure 1 overviews the DEsignBench structure, with each component detailed as follows.

DEsignBench topology. DEsignBench categorizes the visual design abilities to examine into two categories, namely the design technical capability and the design application scenario. The design technical capability separately zooms into each core technical capability required for visual design, including text rendering and typography [55, 11], layout and composition [81, 71], color harmony [4, 63], medium and artistic style [56], and 3D and cinematography [60, 13]. We further define sub-categories under each capability, and manually craft text prompts accordingly. The design application scenario focuses on the real design application, which usually requires the seamless integration of multiple design technical capabilities. Example categories include infographics, animation, gaming, product, and visual art.

DEsignBench data and gallery. Based on the DEsignBench topology, we organize samples into an evaluation set of 215 prompts, with corresponding design category tags, leading to a new challenging generation benchmark focused on visual design. We collect images generated by the state-of-the-art T2I models (SDXL v1.0 [73], Midjourney v5.2 [3], Ideogram [2], Firefly 2 [1], and DALL-E 3 [8, 67]), and formulate them into the DEsignBench gallery for side-by-side qualitative comparisons.

DEsignBench evaluation. We conduct the human evaluation [75, 96, 80, 73] on images in the DEsignBench gallery, assessing them based on three primary criteria: visual aesthetics, image-text alignments, and design creativity. The design creativity aspect asks human annotators to evaluate if the generated image is a novel design, i.e., whether it showcases unique and innovative interpretations of the input prompt and brings a fresh perspective. Additionally, the evaluation also considers five other design-specific capabilities, i.e., text rendering, composition and layout, color harmony, 3D and cinematography, and medium and style, each paired with specific annotation guidelines.

Furthermore, we explore the automatic evaluation pipeline, which provides a more cost-effective approach with reproducible results. Automatic evaluation with large language models has shown promise in various natural language processing [18, 53, 27] and vision-language understanding tasks [97]. However, T2I evaluation is more complicated. It requires both a high-level semantic understanding (e.g., image-text alignment and a detailed visual comparison across two images (e.g., visual aesthetic ranking), not to mention several other design-specific criteria. Following prior studies that take large multimodal models (LMMs) [68, 69, 93, 62] for T2I image-text alignment evaluation $[8,5,95]$, we propose a pairwise model rating based on GPT-4V that comprehensively evaluates all aspects as a human annotator. The high consistency with human rating indicates the effectiveness of the proposed LMM-based T2I evaluation.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-006.jpg?height=1407&width=1634&top_left_y=245&top_left_x=191)

Figure 1: An overview of the DEsignBench's structure.

Our contributions are summarized as follows.

- We explore DALL-E 3 on imagining visual design. We then present DEsignBench, a new challenging text-to-image generation benchmark focusing on assisting visual design.
- We propose an automatic GPT-4V evaluation for DEsignBench evaluation, which provides reproducible results that align well with human ratings.
- We collect DEsignBench gallery, which side-by-side compares the images generated by various state-of-the-art T2I models (SDXL, Midjourney, Ideogram, Firefly2, DALL-E 3).

Remaining sections are organized as follows. Section 2 uses DALL-E 3 to provide an overview of the state of the art in T2I generation, and justify the experiment settings in DEsignBench. Section 3 and Section 4 introduce the design technical capability and the design application scenario, respectively, using insights from DALL-E 3. The human and GPT-4V quantitative evaluations are discussed in Section 5. Finally, the appendix shows the complete DEsignBench gallery, showcasing output comparisons among SDXL, Midjourney, Ideogram, Firefly2, and DALL-E 3.

## 2 DALL-E 3 Basics and DEsignBench Settings

In this section, we overview the state-of-the-art T2I generation capability, with explorations on DALL-E 3. We then introduce the experiment settings in DEsignBench.

### 2.1 DALL-E 3's Working Modes

ChatGPT prompt expansion. DALL-E $3[8,67,66]$ adopts ChatGPT [65] for prompt expansion, i.e., converting an input user query into a more detailed text description. As shown in Figure 2, we empirically observe that this prompt expansion (cf., user input $v s$. expanded prompt) also benefits other compared T2I models, such as SDXL [73] and Midjourney [3]. Therefore, we take the "expanded prompt" as the default setting in our DEsignBench.

In addition to DALL-E 3's default prompt expansion behavior in ChatGPT defined by the built-in system prompt, such as generating four prompts sequentially and producing four images, we find it helpful to add extra input prompts to ChatGPT for specialized prompt drafting.

- Generate a detailed description and then generate one image: Longer and more detailed prompts generally lead to better images, i.e., more object details, correct scene texts, and better image quality. We find it helpful to explicitly ask ChatGPT to provide a detailed description, and ease the task by asking for one prompt instead of four, both encourage a more detailed T2I prompt. We find this instruction particularly helpful in generating complicated scenes, such as posters, books, ads, etc., which are otherwise almost impossible to create.
- Exactly repeat the same prompt for one image: For other cases, we may want to shut down the ChatGPT prompt paraphrasing, e.g., changing a few attributes words in a controlled manner or producing the previously generated images. To achieve that, we can simply ask ChatGPT to "exactly repeat the same prompt."

Multi-round dialogue-based T2I. DALL-E 3 with ChatGPT also naturally supports the multiround dialogue-based generation. The chat interface allows users to refer to the generation history in generating the next image. For example, one may refer to a specific generated image and give an editing instruction, such as "Change the cloth in the second image into the blue color," and naturally continue with multi-round editing. Another example is to keep arbitrary visual aspects in the generated image, such as keeping the character appearance or image style when generating a multiple image comic stripe (e.g., in Figures 31-33).

### 2.2 T2I Generation Capability Overview

We next provide an overview of the DALL-E 3's generation capability, with popular testing prompts from existing benchmarks or community posts. Overall, we observe that DALL-E 3's unprecedented prompt following ability allows it to effectively solve many well-known challenge cases. This observation motivates us to go a step further, and construct DEsignBench that considers the more challenging yet valuable scenarios of visual designs.

Prompt following: detailed descriptions. Prompt following is one key challenge in T2I generation. Previous T2I models tend not to strictly follow the text prompt, leading to incorrect objects and attributes [26, 15, 10, 25]. We use the famous failure cases in PartiPrompts [96] to show DALL-E 3's prompt following capability. As shown in Figure 3, DALL-E 3 generates images with correct object counts, relative size, global and local attributes, minimal object hallucination, and scene text. As further discussed throughout the paper, unprecedented prompt following ability is critical for the imagined design scenarios, allowing designers to use arbitrary text words for image control more confidently.

Prompt following: uncommon scenes. In addition to following complicated long prompts, prompt following also requires models to faithfully generate the uncommon senses, such as the "A horse riding an astronaut." Following prompts for uncommon sense is essential for design scenarios, which usually involve imaginative creations with uncommon attributes and object combinations. In Figure 4, we examine representative challenging prompts from community posts [59]. DALL-E 3 shows the

| Word-level Acc. (\%) | Short Words | Challenging Words | Sentences | Total |
| :--- | :---: | :---: | :---: | :---: |
| Midjourney [3] | 0.0 | 0.0 | 4.3 | 1.1 |
| SDXL [73] | 37.9 | 5.0 | 19.0 | 25.0 |
| IF [41] | 62.5 | 15.8 | 39.4 | 45.0 |
| DALL-E 3 | $\mathbf{8 3 . 3}$ | $\mathbf{3 1 . 7}$ | $\mathbf{6 2 . 4}$ | $\mathbf{6 5 . 2}$ |

Table 1: Word-level text rendering accuracy when selecting the best from $N=4$ generated images.

capability to generate uncommon spatial relationships, object shapes, attributes, etc. Such prompt following capability may assist designers in creating their imaginative pieces more easily.

Image generation: text rendering. Text rendering [49,57,16,92, 84] is critical for design scenarios, yet remains to be challenging for previous T2I models $[3,73]$. We empirically observe that DALL-E 3 can more reliably render texts in images, though still not perfect on more complicated texts. Table 1 provides a quantitative comparison of the word-level scene text accuracy on 40 constructed samples. Specifically, we run the Microsoft Azure OCR system and compare the exact match words with the text in the input prompt. We generate $N=4$ images for each prompt and report the best results. We show additional qualitative results later in Figures 6-10.

Image generation: other challenges. We also examine other common failures shared among previous T2I models, such as hand and face generation, unique art styles, challenging objects, etc. We empirically observe that DALL-E 3 works more reliably in those challenging cases. Figure 5 shows several examples of such "common failures" discussed in previous papers and community posts [58, 77, 80, 26, 39], e.g., detailed and uncommon attribute designs, uncommon scenes, etc. We group the explorations based on their usage in design scenarios, presenting in the next section "design technical capability."

T2I Model A

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-009.jpg?height=407&width=685&top_left_y=306&top_left_x=37)

T2I Model B

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-009.jpg?height=404&width=701&top_left_y=305&top_left_x=712)

T2I Model C

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-009.jpg?height=406&width=705&top_left_y=304&top_left_x=1401)

User Input: Several little gray penguins selling ice cream cones on a sunny day, photorealistic, cinematic film-style lighting, beach, humor feeling
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-009.jpg?height=406&width=2070&top_left_y=760&top_left_x=36)

Expanded Prompt: Photo-realistic scene of a beachside ice cream stand run by little gray penguins. The humor is evident as one penguin tries to climb a ladder to reach a tall ice cream stack, another uses a tiny fan to keep cool, and a third penguin seems to be bargaining with a seagull. The sunny beach atmosphere, complete with laughing beachgoers, adds to the comedic setting. Cinematic film-style lighting enhances the vibrant colors and casts playful shadows.
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-009.jpg?height=396&width=1892&top_left_y=1314&top_left_x=38)

User Input: create a 16:9 wide poster for a computer vision conference in Seattle. The poster should include "Computer Vision and Pattern Recognition", "CVPR 2024", "June 2024 "
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-009.jpg?height=404&width=2054&top_left_y=1790&top_left_x=38)

Expanded Prompt: Photo of the Seattle skyline at dusk with iconic landmarks like the Space Needle visible. Overlaying the skyline, translucent digital graphics represent neural networks and pattern grids. The title 'Computer Vision and Pattern Recognition' is boldly displayed at the top in modern, sleek fonts. Below it, 'CVPR 2024' is highlighted. Further down, 'June 2024' and 'Seattle Convention Center' are elegantly positioned. The color palette consists of cool blues and tech-inspired neon accents, capturing the essence of the tech conference.

Figure 2: Qualitative comparisons between the generation results with original user inputs and ChatGPT expanded prompts. We hide the specific T2I model names as A, B, C. Please see appendix for comprehensive model comparisons.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-010.jpg?height=588&width=1022&top_left_y=259&top_left_x=42)

User Input: A robot painted as graffiti on a brick wall. The words "Fly an airplane" are written on the wall. A sidewalk is in front of the wall, and grass is growing out of cracks in the concrete. Expanded Prompt: Photo of a brick wall with a graffiti of a robot. On the wall, there are also words that read 'Fly an airplane'. A weathered sidewalk runs in front of this wall, and patches of grass sprout from the cracks in the concrete.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-010.jpg?height=596&width=1022&top_left_y=992&top_left_x=42)

User Input: A portrait of a statue of Anubis with a crown and wearing a yellow $t$-shirt that has a space shuttle drawn on it. A white brick wall is in the background.

Expanded Prompt: Realistic image of a majestic statue of Anubis, crowned and standing with a commanding presence. The god's attire is updated with a yellow $t$-shirt decorated with a finely drawn space shuttle. A clean, white brick wall frames the background, adding depth to the scene.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-010.jpg?height=599&width=1024&top_left_y=1739&top_left_x=41)

User Input: A plate that has no bananas on it. there is a glass without orange juice next to it. Expanded Prompt: Photo of a pristine white ceramic plate set on a wooden table, illuminated by soft ambient lighting. Adjacent to the plate, there's a clear glass, sparkling clean and devoid of any liquid, especially orange juice. The background consists of a soft beige wall with a hanging minimalist clock.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-010.jpg?height=580&width=1009&top_left_y=263&top_left_x=1073)

User Input: A rhino beetle this size of a tank grapples a real life passenger airplane on the tarmac. Expanded Prompt: Photo of a massive rhino beetle, comparable in size to a military tank, as it grapples onto a commercial passenger airplane. The scene takes place on a sunlit tarmac with aircraft staff observing from a safe distance.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-010.jpg?height=591&width=1008&top_left_y=992&top_left_x=1076)

User Input: A cream colored labradoodle next to a white cat with black-tipped ears. Expanded Prompt: Photo of a cream-colored labradoodle standing gracefully on a well-maintained lawn. To its right, a white cat with striking black-tipped ears is lounging lazily, their tails almost touching.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-010.jpg?height=588&width=1011&top_left_y=1750&top_left_x=1072)

User Input: Two baseballs to the left of three tennis balls.

Expanded Prompt: Oil painting of a vintage setting with two aged baseballs to the left, each with faded markings, and three classic white tennis balls to the right, all with slight paint brush strokes visible.

Figure 3: Challenging prompts [96] in following long and detailed descriptions.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-011.jpg?height=586&width=1044&top_left_y=260&top_left_x=20)

User Input: A child sleeping under a bed

Expanded Prompt: Photo of a spacious bedroom with white walls. Underneath a modern metal bed, a young child with short black hair and wearing green pajamas is nestled in a cozy position, surrounded by a fortress of cushions and blankets.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-011.jpg?height=585&width=1005&top_left_y=976&top_left_x=45)

User Input: A car with square wheels

Expanded Prompt: Photo of a sleek modern car parked on an open road, gleaming under the sunlight. Uniquely, it has four square wheels that seem out of place yet intriguing.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-011.jpg?height=594&width=1024&top_left_y=1709&top_left_x=38)

User Input: A car made of smaller cars

Expanded Prompt: Photo of a unique car parked on a city street, which, upon closer inspection, is composed entirely of smaller toy-sized cars meticulously arranged to form its structure. The sunlight creates a glint on the toy cars' windows, showcasing their variety of colors and models.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-011.jpg?height=588&width=1008&top_left_y=259&top_left_x=1076)

User Input: A horse riding an astronaut

Expanded Prompt: Render of a humorous setting where a white horse, looking a bit puzzled, is sitting on top of an astronaut's back. The astronaut tries to balance the horse while surrounded by asteroids and space debris.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-011.jpg?height=596&width=1008&top_left_y=973&top_left_x=1076)

User Input: The Statue of Liberty wearing sunglasses

Expanded Prompt: Illustration of the Statue of Liberty with a sunset backdrop, casting a warm orange hue over the scene. The statue, a symbol of freedom, now wears oversized round sunglasses, giving her a contemporary makeover.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-011.jpg?height=585&width=1006&top_left_y=1713&top_left_x=1080)

User Input: A plate placed on top of a pile of food

Expanded Prompt: Photo of a dimly lit dining table where a white ceramic plate is turned upside down, covering a mound of spaghetti with marinara sauce and meatballs. A fork is placed beside the plate, and stray noodles peek out from beneath the edges.

Figure 4: Challenging prompts in generating uncommon scenes.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-012.jpg?height=585&width=1019&top_left_y=84&top_left_x=43)

User Input: A woman stands wearing a black and white coat over a dark vest, orange shirt and pink tie Expanded Prompt: Photo of a confident woman with short brunette hair standing against a neutral background. She is wearing a black and white striped coat that reaches her knees. Underneath the coat, she has a dark vest and a bright orange shirt. Around her neck, she has tied a vibrant pink tie which stands out against the other colors.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-012.jpg?height=588&width=1013&top_left_y=804&top_left_x=38)

User Input: A realistic photo of a Pomeranian dressed up like a 1980 s professional wrestler with neon green and neon orange face paint and bright green wrestling tights with bright orange boots.

Expanded Prompt: Realistic image of a Pomeranian dog, looking determined and fierce, dressed as a 1980 s wrestler. The background is a wrestling ring with ropes and a cheering crowd. The Pomeranian's face boasts striking neon green and neon orange face paint patterns, a signature of the 1980s wrestling world. The dog's body is covered in shiny green wrestling tights, and its feet are encased in eye-catching bright orange wrestling boots, adding to its formidable appearance.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-012.jpg?height=580&width=1008&top_left_y=1605&top_left_x=41)

User Input: A donkey and an octopus are playing a game. The donkey is holding a rope on one end, the octopus is holding onto the other. The donkey holds the rope in its mouth. A cat is jumping over the rope.

Expanded Prompt: Photo of a sandy beach setting where a donkey and an octopus are engaged in a tug-of-war game. The donkey, with a brown and white coat, holds one end of the thick rope in its mouth, its hooves firmly planted in the sand. The octopus, with its deep blue tentacles, grips the other end of the rope, its body partially submerged in the shallow water. In the middle of this scene, a ginger cat with sleek fur gracefully leaps over the rope, casting a shadow beneath it.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-012.jpg?height=585&width=1005&top_left_y=84&top_left_x=1075)

User Input: On a tennis court, an old lady in a red t-shirt with blue hair holds up a black pan, standing next to a young man in black and white who holds on to a tennis racket.

Expanded Prompt: Wide shot of a tennis court bathed in afternoon sunlight. An old lady with striking blue hair and wearing a red t-shirt confidently holds a black pan. Next to her, a young man in a black and white outfit tightly clutches a tennis racket, both ready for an unconventional match.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-012.jpg?height=594&width=1011&top_left_y=798&top_left_x=1072)

User Input: A maglev train going vertically downward in high speed, New York Times photojournalism.

Expanded Prompt: Photo in the style of New York Times journalism capturing a maglev train's unprecedented vertical drop. Architectural marvels on either side seem dwarfed by the train's audacity. A mix of male and female professionals at the ground level pause from their routines, gazing skyward in disbelief.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-012.jpg?height=561&width=813&top_left_y=1609&top_left_x=1160)

User Input: Paying for a quarter-sized pizza with a pizza-sized quarter.

Expanded Prompt: Cartoon of a young woman with a playful expression, holding up a massive coin that's the size of a pizza. Opposite her, a pizzaiolo scratches his head in confusion, presenting her with a pizza that's only the size of a regular quarter.

Figure 5: Other challenging prompts from previous papers and community posts [58, 80, 26, 39].

## 3 Design Technical Capability

Design encompasses a broad spectrum, from product and advertisement to logo and fashion design. Essential to any design tool is the capacity to produce text, shapes, charts, and diagrams [56, 55, 11]. Beyond these basics, the tool should be adept at crafting layouts that are not only semantically accurate but also aesthetically appealing [81]. Mastery of elements such as 3D, lighting, color palettes, and varied rendering materials and styles is indispensable [60, 71, 13]. In the following section, we highlight DALL-E 3's competencies in addressing diverse design challenges.

### 3.1 Text Rendering and Typography

Figure 6 presents six diverse styled text renderings, spanning graffiti art, calligraphy, handwritten texts, mathematical symbols, multilingual scripts, and musical notations. While DALL-E 3 impressively renders English text across different styles, it exhibits some inaccuracies. The math equation, for instance, misinterprets certain operators and signs. While the layout for multilingual rendering appears organized, it struggles with certain languages, particularly Chinese and Japanese. The musical notation, while superficially resembling actual sheet music, includes several inaccuracies, underlining DALL-E 3's constraints in this domain.

Figure 7 illustrates renderings of infrequently occurring text. This includes misspelled words such as "Happpy Hallooween" and "Baaabas," and random character sequences like "CVD0p Sstpn6tsp".

Figure 8 showcases renderings of extended text passages. For instance, "Hierarchical Text-Conditional Image Generation with CLIP Latents." The compound text "gala apple NET NT 32oz (2 LB) 907g" poses a unique challenge with its amalgamation of words, numerals, and units. Yet, DALL-E 3 produces a layout reminiscent of a store price tag.

Effective typography is more than accurate spelling [11]. Font selection is vital, needing alignment with content and medium. The choice between serif and sans-serif hinges on communication context. Font size is key, with hierarchy distinguishing headings, subheadings, and body text for clarity and visual definition. Figure 32 and 33 depict the rendering Pusheenish font in the dialogue balloons. Figure 24 showcases the font hierarchy rendering in sophisticated posters.

For clear visuals, colors must contrast well with the background and convey intended emotions. Uniform alignment ensures a cohesive, organized text presentation. Figure 23 displays various font colors in GUI design, while Figure 22 showcases DALL-E 3's alignment capabilities in creating storybook design.

When these facets converge cohesively, typography elevates from a mere conveyance of information to a medium that enhances design aesthetics and user engagement. The "Born Pink" mug in Figure 39 exemplifies this, seamlessly blending handwritten and printed styles, harmonized by color and lighting choices.

### 3.2 Layout and Composition

Creating a compelling layout and composition in design demands a keen understanding and strategic implementation of several key elements [81], ensuring that the visual space effectively communicates and resonates with the viewer.

Figure 9 displays layouts including block diagrams, pie charts, flow charts, bar graphs, tables, and calendars. While DALL-E 3 generally crafts decent layouts, it sometimes struggles with intricate details.

Figure 10 illustrates multi-panel layouts such as storyboards, how-tos, memes, and comics. Consistency in elements, colors, and patterns is vital in multi-panel designs to unify the composition and guide viewers. Designers utilize flow and movement, directing the viewer's eye using lines and element arrangements, to ensure a seamless experience.

### 3.3 Color Harmony

Color harmony is a vital principle in design that ensures various colors in a composition create a cohesive, aesthetically pleasing experience for the viewer $[64,9]$. A harmonious color palette can evoke specific emotions, set the tone, and enhance the overall impact of a piece.

Figure 11 displays variations of color palettes in oil paintings inspired by "Impression Sunrise." These range from Spring, Summer, Autumn, and Winter Palettes to a Romantic Palette and a monochromatic green shade. This serves as a test to see if DALL-E can adeptly control and render color palettes. DALL-E 3 effectively captures the distinct tones associated with different seasons and themes.

Figure 12 presents six color palette variations in oil paintings, inspired by "Starry Night," testing complementary color harmonies. It's striking how DALL-E captures and renders these vibrant starry scenes with such vitality and beauty.

### 3.4 Medium and Style

The artistic medium and style are crucial in visual graphic design [56], defining the work's expressive potential and emotional resonance. The medium, encompassing the tools, materials, or digital platforms employed, sets the boundaries and opportunities for expression, shaping the tactile and sensory experiences of the audience.

Figure 13 shows examples of sketching a cat in different styles including continuous line drawing, charcoal sketch, stippling sketch, brush and ink sketch, etc. Figure 14 and 15 demonstrate the capability of specifying different art media, including block print, folk art, paint-by-numbers, watercolor wood carving, Lego style, glass blowing, calligraphy, etc. These examples are just a small set of the art styles and media that DALL-E 3 covers. They provide a glimpse of DALL-E 3's capability of rendering with a broad range of artistic media and styles.

### 3.5 3D and Cinematography

3D rendering [90] and cinematography [13] are transformative tools in the world of visual representation, allowing for the creation of intricate, lifelike scenes and stories. The depth, perspective, and dynamism brought about by these techniques offer a multi-dimensional view, enhancing the viewer's experience and immersion.

Figure 16 shows examples of 3D rendering, including basic shapes, spatial relationships, lighting effects, shadow, reflections, and various viewing angles. DALL-E 3 proficiently captures self-shadows and cast shadows and effectively manages reflections on both flat and curved surfaces. The transition between two light sources is smooth. We find that DALL-E 3 sometimes does not follow view angles precisely. For example, the front view rendering is noticeably off.

In Figure 17, we show DALL-E 3's capabilities of generating special lighting effects including chemiluminescent glow, bioluminescent glow, light-painting, and Aurora glimmering.

Figure 18 shows different camera angles and positions, including closeups, bird-eye level, low and side angles. For close-up shots, DALL-E 3 blurs the background appropriately to enhance the scene depth and puts the focus on the foreground.

Figure 19 shows examples of simulating fisheye and wide angle lenses, slow and faster shutter speeds, instant camera, and tilt shift photography. At the bottom left, DALL-E 3 simulates an instant camera whose photos are usually grainy. At the bottom right, DALL-3 simulates tilt-shift photography with the focus on the lady while gradually blurring her surroundings.

Figure 20 and 21 demonstrate DALL-E 3 capabilities of rendering crowded scenes. Figure 20 shows rendering different numbers of bears. DALL-E 3 correctly generates the desired number of bears when the number is small. When the number gets larger, however, DALL-E 3 makes mistakes (as shown in the last row). Figure 21 generates images of large human crowds on a variety of occasions. We find that DALL-E 3 does a nice job of positioning the texts and rendering them with the correct perspectives. At the bottom left, DALL-E 3 generates an exaggerated scene of a popular burger eatery with a super long serving counter and a large waiting crowd. The exaggeration looks plausible and shows the popularity of the burger.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=586&width=1019&top_left_y=65&top_left_x=43)

User Input: a graffiti art of the text "free the pink" on a wall

Expanded Prompt: Photo of a smooth stone wall with the graffiti art 'free the pink' painted in a gradient from pink to blue. Surrounding the text are intricate patterns and a silhouette of a city skyline at the base.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=594&width=1011&top_left_y=771&top_left_x=53)

User Input: handwritten text "Olympic national park" written on a piece of paper Expanded Prompt: Photo of a crumpled piece of paper held by a person's hand, with the words 'Olympic national park' scribbled in cursive handwriting using a blue pen.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=127&width=699&top_left_y=1449&top_left_x=195)

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=122&width=678&top_left_y=1534&top_left_x=211)

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=76&width=672&top_left_y=1626&top_left_x=214)

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=54&width=675&top_left_y=1678&top_left_x=211)

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=120&width=680&top_left_y=1699&top_left_x=213)
CHIND   HELO HTIAO F-F j  $\overline{\bar{v}}$ JAPANIC

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=71&width=673&top_left_y=1821&top_left_x=211)

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=89&width=673&top_left_y=1854&top_left_x=211)
  $\pm \neq \dot{\dot{m}}$ 

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=62&width=673&top_left_y=1975&top_left_x=211)

User Input: show me on a white paper how many different languages you can write hello

Expanded Prompt: A wide image of a white paper filled with the word 'Hello' written in different languages. The paper showcases: English: 'Hello', Spanish: 'Hola', French: 'Bonjour', German: 'Hallo', Italian: 'Ciao', Portuguese: 'Ol', Russian: '', Chinese: '', Japanese: ' ', Korean: '  ', Arabic: '', Hindi: '', Bengali: '', Urdu: '', and Swahili: 'Habari'. The various greetings are written in distinct fonts, representing the essence of each language.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=591&width=1008&top_left_y=62&top_left_x=1076)

User Input: create calligraphy with red stamp, the text is "happy new year"

Expanded Prompt: Photo of elegant calligraphy on white paper, beautifully written with the phrase 'happy new year'. In the bottom right corner, there's a red stamp imprint.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=599&width=1014&top_left_y=768&top_left_x=1073)

User Input: a math equation "-8x+6 $=-5 x+13$ " with a puzzling face next to it

Expanded Prompt: Drawing of a notebook page with the handwritten equation "-8x+6= $-5 x+13$ ", and a sketch of a questioning face with big eyes next to it.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-015.jpg?height=599&width=1044&top_left_y=1454&top_left_x=1061)

User Input: create a page with three row music notes with title in the top center

Expanded Prompt: A parchment sheet music page featuring three rows of musical notes in dark ink. Above the musical notes, centered at the top, is a title written in a decorative script that says 'Harmony of the Heart'. The staves are clearly defined and the notes are neatly placed.

Figure 6: Rendering stylized texts, e.g., graffiti art "free the pink," calligraphy "happy new year," handwritten texts "Olympic national park," math equation " $-8 x+6=-5 x+13$," "hello" in multiple languages, and music notes.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-016.jpg?height=583&width=1019&top_left_y=66&top_left_x=43)

User Input: A TV show poster named "Ira Finkelstein s Christmas"

Expanded Prompt: Photo-style TV show poster with the title 'Ira Finkelstein's Christmas' at the top in festive red and green colors. The background features a snowy town scene with decorated Christmas trees and fairy lights. In the foreground, a diverse group of characters, including a young boy with glasses, an elderly woman with white hair, and a teenager with a beanie hat, are gathered around a large gift box, looking excited and curious.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-016.jpg?height=591&width=686&top_left_y=832&top_left_x=210)

User Input: cartoon pumpkin with text "Happpy Hallooween", typography, illustration Expanded Prompt: Cartoon depiction of a pumpkin with big, sparkling eyes and a cute smile, sitting on a patch of grass under a crescent moon. The phrase 'Happpy Hallooween' is written above in a bold, whimsical typography that seems to glow in the moonlight.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-016.jpg?height=585&width=1022&top_left_y=1540&top_left_x=42)

User Input: A bear holds a board saying "T76Ljoi N4ei8".

Expanded Prompt: Watercolor painting of a snowy landscape where a white polar bear stands prominently in the foreground. It holds a board with the text 'T76Ljoi N4ei8'. The background has muted colors of icy mountains and the aurora.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-016.jpg?height=586&width=1008&top_left_y=65&top_left_x=1076)

User Input: A photo of a frog reading the newspaper dated "Oct 1st 2023" written on it. There is a frog printed on the newspaper.

Expanded Prompt: Photo of a tree frog with its legs stretched out, engrossed in reading a newspaper. Emblazoned on the newspaper is the date 'Oct 1st 2023'. A cartoon frog character decorates the newspaper's main story.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-016.jpg?height=569&width=1003&top_left_y=843&top_left_x=1079)

User Input: Bananas arranged on a picnic table to form the message "This is baaabas" Expanded Prompt: Illustration of bananas of various sizes artistically placed on a checkered picnic cloth on a table. The bananas come together to form the words 'This is baaabas'. Birds are seen chirping nearby.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-016.jpg?height=588&width=1008&top_left_y=1538&top_left_x=1076)

User Input: top-down view of a whole cake on the table with words "CVDOp Sstpn6tsp" written on the cake

Expanded Prompt: Photo of a top-down angle of a vanilla cake with colorful sprinkles on a glass table. The cake features the words 'CvDOp Sstpn6tsp' written with green icing.

Figure 7: Rendering text words that are less frequently appeared, e.g., names and dates "Ira Finkelstein's Christmas" and "Oct 1st 2023," misspellings "Happpy Hallooween" and "This is baaabas," and random strings "T76Ljoi N4ei8" and "CVD0p Sstpn6tsp."

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-017.jpg?height=599&width=1022&top_left_y=91&top_left_x=42)

User Input: A dining table setting with a bottle of "Chateau Belinga Bergera rose"

Expanded Prompt: Photo of an elegant dining table setting with white porcelain plates, silver cutlery, and crystal glasses. In the center, there's a bottle of 'Chateau Belinga Bergera rose' with its label clearly visible, surrounded by softly glowing candles.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-017.jpg?height=591&width=1022&top_left_y=843&top_left_x=42)

User Input: A movie poster with logo "Playing for Change Songs Around The World Part 2" on it Expanded Prompt: Rendered movie poster with a 3D stage setup. On the stage, diverse musicians from different parts of the world are performing together. Above the stage, spotlights converge to form the logo with the text 'Playing for Change Songs Around The World Part 2'. The background has a crowd cheering and waving flags from various nations.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-017.jpg?height=586&width=767&top_left_y=1596&top_left_x=300)

User Input: a box for "Microsoft Surface Headphones"

Expanded Prompt: Photo of a sealed packaging box for Microsoft Surface Headphones, with a clear image of the headphones displayed on the front and product details on the back.
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-017.jpg?height=588&width=984&top_left_y=96&top_left_x=1079)

User Input: Create a vector graphic design with letters in different shapes with exactly spelled text : "Follow your heart, not the crowd" cartoon style, t-shirt design, poster, illustration, typography

Expanded Prompt: Typography vector design for a t-shirt, illustrating the words 'Follow your heart, not the crowd'. The letters are shaped in a fun cartoon style, bursting with creativity.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-017.jpg?height=591&width=1008&top_left_y=843&top_left_x=1079)

User Input: whiteboard with texts "Hierarchical Text-Conditional Image Generation with CLIP Latents", typography, poster, illustration

Expanded Prompt: Illustration of a lecture hall setting with a professor pointing to a whiteboard that has the text 'Hierarchical Text-Conditional Image Generation with CLIP Latents' written on it. The students are attentively listening.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-017.jpg?height=545&width=615&top_left_y=1619&top_left_x=1270)

User Input: a bag of "gala apple NET NT 32oz (2 LB) 907g"

Expanded Prompt: Vector image of a sealed bag with the inscription 'gala apple NET NT 32oz (2 LB) $907 \mathrm{~g}$ ' and several gala apples visible inside.

Figure 8: Rendering longer pieces of texts, e.g., on bottles and t-shirts "Chateau Belinga Bergera rose" and "Follow your heart, not the crowd," on posters and boards "Playing for Change Songs Around The World Part 2" and "Hierarchical Text-Conditional Image Generation with CLIP Latents," and on product packages "Microsoft Surface Headphones" and "gala apple NET NT 32oz (2 LB) 907g."

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-018.jpg?height=504&width=632&top_left_y=103&top_left_x=234)

User Input: create a diagram with 4 blocks, representing 4 steps with text shown in each box, processing, training, finetuning, and inference

Expanded Prompt: A wide diagram with four rectangular blocks arranged horizontally. Each block is connected with an arrow pointing to the next. The first block is labeled 'Processing', the second 'Training', the third 'Finetuning', and the fourth 'Inference'. Each block has a distinct color, and the text is bold and clear.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-018.jpg?height=558&width=594&top_left_y=900&top_left_x=256)

Expanded Prompt: Flowchart on a wide layout showcasing the algorithm to determine if a given integer is even or odd. The flow starts with the 'Start' box, leading to a decision box labeled 'Is the number divisible by 2?'. If the answer is 'Yes', the flow proceeds to an action box labeled 'Print even' and then to the 'End' box. If the answer is 'No', it moves to another action box labeled 'Print odd', and then also proceeds to the 'End' box. The flowchart uses clear arrows and boxes to visually represent the algorithm's logic.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-018.jpg?height=582&width=1002&top_left_y=1666&top_left_x=60)

Expanded Prompt: A photo of a white paper on a wooden table detailing sales metrics. The table has columns labeled 'Country', 'Sales Volume', 'Revenue', 'Profit', and 'Profit Margin'. Rows list metrics for USA, China, Australia, India, and South Korea. USA has the highest sales volume at 40,080 and revenue of $\$ 15,971,880$. Profit margins range from $17.5 \%$ for India to $19.4 \%$ for Australia. The bottom row aggregates the totals, showing a combined sales volume of 141,282 and total revenue of $\$ 67,754,238$. The average profit margin is $18.8 \%$.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-018.jpg?height=534&width=686&top_left_y=112&top_left_x=1316)

User Input: A pie chart visually representing three segments. The first segment occupies $10 \%$ of the chart with a distinct color. The second segment also takes up $20 \%$ of the pie with a different shade. The third segment dominates the chart with a significant $70 \%$ slice. Each segment is labeled with its respective percentage, providing a clear visual breakdown. ar=16:9 Expanded Prompt: 16:9 digital illustration of a pie chart that visually divides into three segments. The first $10 \%$ is displayed in muted orange, the next $20 \%$ in pastel purple, and the dominating $70 \%$ is presented in a rich turquoise. Every segment comes with its percentage label for clarity.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-018.jpg?height=553&width=607&top_left_y=911&top_left_x=1279)

Expanded Prompt: Wide bar chart on a white paper background comparing the total fueling cost of three vehicles: Ford F150 (Regular Cab), Toyota RAV4, and Honda Civic. The Ford F150's cost is represented by the tallest purple bar at $\$ 130.96$. The Toyota RAV4 is represented by a medium-sized bar at $\$ 82.56$, and the Honda Civic by the shortest bar at $\$ 70.55$. All bars are colored in purple, and the chart has a clear title at the top reading 'Total fueling cost'. The costs are labeled above each bar for clarity.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-018.jpg?height=555&width=724&top_left_y=1682&top_left_x=1226)

Expanded Prompt: Illustration of a festive December 2024 calendar. The calendar is surrounded by decorative winter elements such as snowflakes, pine cones, and holly leaves around the borders. Each day of the month is placed within its own distinct box. The name of the month, 'December 2024', is prominently displayed at the top in a stylish and elegant font. The colors used are a combination of wintry blues, whites, and hints of gold and red for a festive touch. The overall design evokes the warmth and joy of the holiday season.

Figure 9: Layout and composition capability: diagram, flowchart, bar chart, pie chart, table, and calendar.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-019.jpg?height=599&width=1024&top_left_y=134&top_left_x=38)

Prompt: A wide cartoon storyboard capturing a day in the life of the 'Super Duper Sunflower Hedgehog'. Panel 1: Morning - The sunflower hedgehog wakes up, stretching its petals against a backdrop of a rising sun. Its spines are tiny sunflower stems, and in
looks refreshed. Panel 2: Midday - The sunflower hedgehog is seen helping other plants by providing them shade with its large sunflower head, showcasing its super nature. Birds and butterflies hover around, grateful for the shelter. Panel 3: Afternoo Our hero is on a playful chase with a butterfly, rolling around a meadow, leaving a trail of sunflower seeds behind. Panel 4: Evening - As the sun sets, the sunflower hedgehog sits atop a hill, looking at the horizon, reflecting the golden hues of the setting sun. Stars begin to twinkle as night approaches. Panel 5: Night - The sunflower hedgehog, under a starry sky, curls up to sleep with tiny fireflies illuminating its petals, ending its super duper day.
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-019.jpg?height=550&width=1026&top_left_y=904&top_left_x=36)

Prompt: A wide image divided into 3 panels demonstrating step-by-step instructions for replacing a car tire. Panel 1: A car with a flat tire, tools like a jack and a wrench laid out, captioned 'Prepare Tools \& Lift Car'. Pane 2: An Caucasian man with a tan complexion using a wrench to remove the lug nuts and taking off the flat tire, captioned 'Remove Flat Tire'. Panel 3: The same man placing the spare tire on the car and tightening the lug nuts, with the caption 'Install Spare Tire'. The design is clear, concise, and visually instructive.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-019.jpg?height=561&width=982&top_left_y=1611&top_left_x=62)

Prompt: Illustration depicting 5 tips to enhance mental health. The scene is divided into five sections. 1) A peaceful bedroom with a person sleeping soundly under a starry night sky to represent 'Sleep well'. 2) A dining table filled with nutritious foods like fruits, vegetables, and whole grains to symbolize 'Eat well'. 3) A person jogging in a park during sunrise, embodying 'Exercise'. 4) A group of diverse individuals laughing and chatting in a cozy setting, illustrating 'Make friends'. 5) A happy individual cuddling with a variety of pets like a dog, cat, and bird, showcasing 'Get a pet'
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-019.jpg?height=538&width=1010&top_left_y=162&top_left_x=1079)

Prompt: A wide 3-panel sequence depicting the changing emotions of a dog. Panel 1: The dog has a joyful expression, its tail wagging and tongue hanging out, eyes sparkling with happiness. Panel 2: The dog's mood shifts; its ears droop, eyes become sadder, and the overall demeanor is one of sorrow. Panel 3: The dog's expression becomes more nuanced, showing a mix of sadness and resignation, representing disappointment. Its eyes look to the side, avoiding direct gaze, and the set of its mouth indicates mild frustration. The sequence effectively conveys the progression of the dog's emotions.
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-019.jpg?height=552&width=724&top_left_y=904&top_left_x=1226)

Prompt: Four-panel cartoon sequence for GPT-4 instructions. Panel 1: A computer screen displaying the GPT-4 logo. Caption: '1. Open GPT-4'. Panel 2: A close-up of a user's hand typing a question into a text box. Caption: '2. Type your query'. Panel 3: A loading symbol next to the GPT-4 logo on the screen. Caption: '3. Wait a moment'. Panel 4: A happy user reading the Al-generated response on the screen. Caption: '4. Read the answer'

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-019.jpg?height=575&width=1049&top_left_y=1610&top_left_x=1075)

Prompt: Four-panel cartoon sequence emphasizing DALL-E 3's image creation process. Panel 1: A sleek computer interface with the vibrant DALL-E 3 logo. Caption: '1. Access DALL-E 3'. Panel 2: A user's hand hovering over a keyboard, ready to type a description. Caption: '2. Describe your imagination'. Panel 3: An animated computer cursor turning into a magical wand, sprinkling stars over the DALL-E 3 logo. Caption: '3. DALL-E 3 crafts your vision'. Panel 4: A user, eyes wide in amazement, looks at a stunning Al-generated image on the screen. Caption: '4. Revel in your Al masterpiece!'.

Figure 10: Multi-panel motion graphics.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-020.jpg?height=583&width=1019&top_left_y=110&top_left_x=43)

Prompt: Oil painting influenced by Monet's impressionist style, presenting a sunrise over a harbor. The calm waters are bathed in a golden light from the sun, with distant silhouettes of anchored ships and boats. The sky transitions through soft hues of light pinks, greens, and yellows. The sun's shimmering reflection on the water enhances the depth of the scene. The artwork is characterized by its loose, expressive brush strokes, embodying the serenity of a peaceful morning.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-020.jpg?height=585&width=1019&top_left_y=865&top_left_x=43)

Prompt: Oil painting influenced by Monet's impressionist style, showcasing a sunrise over a harbor The calm waters radiate with the sun's warm, golden light, creating shimmering reflections. Anchored ships and boats form distant silhouettes against the luminous backdrop. The sky transitions gracefully through warm hues of oranges, browns, and reds. The artwork is characterized by its loose, expressive brush strokes, embodying the serenity of a peaceful morning.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-020.jpg?height=588&width=1009&top_left_y=1587&top_left_x=43)

Prompt: Oil painting influenced by Monet's impressionist style, showcasing a sunrise over a harbor. The calm waters radiate with the sun's warm, golden light, and distant ships and boats form silhouettes against this luminous backdrop. The sky transitions gracefully through soft hues of pinks, purples, and peaches. The sun's shimmering reflection on the water enhances the depth of the scene. The artwork is characterized by its loose, expressive brush strokes, embodying the serenity of a peaceful morning.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-020.jpg?height=583&width=1005&top_left_y=110&top_left_x=1075)

Prompt: Oil painting channeling Monet's impressionist style, portraying a sunrise over a harbor. The scene is bathed in a warm, golden light from the sun, with the calm waters reflecting this hue. Distant ships and boats are anchored, forming silhouettes. The sky artistically transitions through bright blue, yellows, and reds. The sun's shimmering reflection on the water adds depth to the scene. The artwork is marked by its loose, expressive brush strokes, conveying the serenity of a peaceful morning.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-020.jpg?height=591&width=1006&top_left_y=862&top_left_x=1080)

Prompt: Wide oil painting inspired by Monet's impressionist style, depicting a sunrise over a harbor. The serene waters glow with a golden hue from the sun, creating shimmering reflections. Anchored ships and boats are silhouetted in the distance. The expansive sky transitions gracefully through cool blues, white, and greys. The artwork is characterized by its loose, expressive brush strokes, conveying the essence and tranquil mood of a peaceful morning.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-020.jpg?height=586&width=1006&top_left_y=1596&top_left_x=1080)

Prompt: Oil painting channeling Monet's impressionist technique, presenting a sunrise over a harbor. The serene waters radiate with the sun's golden light, and distant silhouettes of ships and boats are evident. The expansive sky is artfully painted with variations of a single green shade. The sun's shimmering reflection on the water adds depth and vibrancy to the scene. The artwork is marked by its loose, expressive brush strokes, conveying the tranquility of a peaceful morning.

Figure 11: Different color palettes used for "Impression Sunrise": Spring, Summer, Autumn, Winter, Romantic, and Monochronmatic with a single green shade.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-021.jpg?height=583&width=1022&top_left_y=66&top_left_x=42)

Expanded Prompt: Wide oil painting inspired by "Starry Night", showcasing the tranquility of a peaceful village against the backdrop of a dynamic and undulating vibrant night sky. Celestial patterns swirl in vivid hues of yellows, whites, blues, and greens, forming a stark yet harmonious contrast with the subdued, warmly lit structures below. A towering, dark cypress tree stands prominently, symbolizing a bridge between the earthly and the ethereal, seemingly conversing silently with the passionate skies. This artistic creation beckons viewers into a realm where serenity and chaos dance in harmony, painting a story that speaks of calm, chaos, and the vast wonders of the cosmos.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-021.jpg?height=586&width=1019&top_left_y=859&top_left_x=43)

Expanded Prompt: Wide oil painting inspired by 'Starry Night'. The scene depicts a peaceful village beneath a vibrant night sky. Celestial patterns swirl in hues of burnt orange, peach, navy blue, and sky blue, creating a mesmerizing contrast to the calm village below. A significant, dark cypress tree stands prominently, symbolizing a bridge between the earth and the cosmos. The painting captures the balance between calm and chaos, hinting at the vast wonders of the universe.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-021.jpg?height=582&width=1019&top_left_y=1601&top_left_x=43)

Expanded Prompt: Wide oil painting inspired by "Starry Night", blending the tranquility of a peaceful village with the dynamic and undulating movements of a vibrant night sky. The sky showcases swirling vortexes of vivid Dark Violet, Lavender, Amber, and Light Yellow. These celestial hues provide a stark yet harmoniously aesthetic contrast to the subdued, warmly illuminated structures below. A large, dark cypress tree stands prominently in the forefront, perhaps symbolizing a bridge between the earthly and heavenly realms. This tree appears to be in a silent conversation with the fervent heavens above. The entire composition invites viewers into a realm where serenity and chaos meld, offering a visual narrative that hints at the tranquility, tumult, and boundless wonder of the cosmos.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-021.jpg?height=586&width=1009&top_left_y=62&top_left_x=1073)

Expanded Prompt: Wide oil painting inspired by "Starry Night". The scene showcases the tranquility of a peaceful village below, contrasted by the dynamic and undulating movements of a vibrant night sky above. Celestial elements swirl in vortexes of vivid Hot Pink, Soft Pink, Deep Turquoise, and Aqua, creating a breathtaking and harmonious aesthetic contrast to the warmly illuminated structures of the village below. A dark and prominent cypress tree stands tall in the foreground, perhaps symbolic of the connection between the earthly and the divine. This tree seems to engage in a silent dialogue with the fervent heavens above. The overall composition is a masterful blend of serene village life and the chaotic beauty of the universe, inviting observers to appreciate both the tranquility and tumult of existence.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-021.jpg?height=596&width=1012&top_left_y=854&top_left_x=1074)

Expanded Prompt: Wide oil painting inspired by "Starry Night", showcasing the tranquility of a peaceful village beneath a vibrant night sky filled with swirling vortexes of Dark Teal, Light Teal, Dark Coral, and Light Coral. These vivid celestial patterns offer a harmonious aesthetic contrast to the warmly lit structures below. In the midst stands a prominent, dark cypress tree, representing a symbolic bridge between the earthly and heavenly domains. This captivating oil painting embodies both serenity and chaos, offering viewers a glimpse into the universe's infinite wonder and beauty.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-021.jpg?height=594&width=1010&top_left_y=1592&top_left_x=1075)

Expanded Prompt: Wide oil painting inspired by "Starry Night", blending the serenity of a calm village with the lively undulations of a vibrant night sky. Celestial aspects are portrayed as swirling whirlpools of vivid Crimson, Pink, Sage Green, and Light Sage. These colors create a harmonious yet striking contrast against the muted, warmly lit structures underneath. A pronounced, shadowy cypress tree, symbolizing perhaps a bridge between terrestrial and celestial worlds, majestically rises, as if in a muted conversation with the impassioned skies. This expertly crafted oil painting beckons viewers to immerse themselves in a universe where calmness and chaos beautifully merge, narrating a tale of peace, upheaval, and the boundless marvels of the cosmos.

Figure 12: Different complementary color palettes used for oil painting inspired by "Starry Night:" Blue and Yellow Palette, Pink and Turquoise Palette, Blue and Orange Palette, Teal and Coral Palette, Violet and Amber Palette, Crimson and Sage Palette.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-022.jpg?height=604&width=629&top_left_y=83&top_left_x=108)

User Input: Life drawing sketch of a cat

Expanded Prompt: Sketch in a square composition of a cat captured in life drawing style. The artwork emphasizes the feline's anatomy and form using swift, confident strokes, capturing the essence of a live model study session.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-022.jpg?height=631&width=635&top_left_y=861&top_left_x=108)

User Input: Value study sketch of a cat

Expanded Prompt: Sketch in a square composition of a cat

depicted in a value study style. The artwork focuses on capturing the different shades of light and dark on the feline's form, emphasizing the depth, volume, and three-dimensionality of the subject.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-022.jpg?height=642&width=632&top_left_y=1647&top_left_x=107)

User Input: Stippling sketch of a cat

Expanded Prompt: Stippling sketch of a cat with its fur, eyes, and features meticulously crafted using thousands of tiny dots. The density of the dots varies, creating depth and shading throughout the artwork.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-022.jpg?height=545&width=545&top_left_y=123&top_left_x=801)

User Input: Continuous line sketch of a cat

Expanded Prompt: Sketch in a square composition of a cat depicted using a continuous line technique. The artwork captures the feline's form and features with a single, unbroken line, weaving in and out to create a cohesive and fluid representation.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-022.jpg?height=553&width=567&top_left_y=897&top_left_x=779)

User Input: Blind contour sketch of a cat

Expanded Prompt: Sketch in a square composition of a cat depicted using a blind contour technique. The artwork is drawn without looking at the paper, resulting in a unique and abstract representation of the feline, capturing its essence in a spontaneous manner.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-022.jpg?height=607&width=571&top_left_y=1664&top_left_x=777)

User Input: Geometric art sketch of a cat

Expanded Prompt: Geometric art sketch of a cat in a 1:1 ratio, where the feline is crafted from a series of interconnected triangles.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-022.jpg?height=637&width=635&top_left_y=61&top_left_x=1382)

User Input: Loose gestural sketch of a cat

Expanded Prompt: Sketch in a square composition of a cat captured in a loose gestural style. The artwork emphasizes quick, fluid strokes that capture the feline's energy and movement, creating an impressionistic and spontaneous representation

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-022.jpg?height=621&width=632&top_left_y=863&top_left_x=1386)

User Input: Charcoal sketch of a cat

Expanded Prompt: Sketch in a square composition of a cat depicted using a charcoal medium. The artwork captures the feline's features with rich, velvety strokes, blending and smudging to create depth and shadow, evoking the texture and tonality of charcoal art.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-022.jpg?height=607&width=634&top_left_y=1667&top_left_x=1385)

User Input: Brush and ink sketch of a cat

Expanded Prompt: Illustration in a 1:1 ratio of a cat crafted in the style of a brush and ink sketch. The artwork captures the feline using bold brush strokes, ink washes, and a design that emphasizes the dynamic interplay of light and shadow.

Figure 13: Sketch with a different style. Prompts inspired by https://docs.midjourney. com/docs/explore-prompting-1.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-023.jpg?height=588&width=572&top_left_y=80&top_left_x=142)

User Input: Block print style cat

Expanded Prompt: Photo of a feline figure depicted in the traditional block print style, set against a neutral background. The cat's posture exudes elegance, with intricate patterns and strong outlines defining its form. The square composition enhances the focus on the feline's features.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-023.jpg?height=628&width=623&top_left_y=857&top_left_x=117)

User Input: Graffiti style cat

Expanded Prompt: Photo in a square composition of a graffiti-styled cat on a brick wall. Vibrant colors, bold strokes, and urban aesthetics come together to depict the feline in a dynamic and street art manner

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-023.jpg?height=623&width=629&top_left_y=1656&top_left_x=108)

User Input: Paint-by-numbers style cat

Expanded Prompt: Photo in a square composition of a cat depicted in a paint-by-numbers style. The feline is segmented into numbered sections, each representing a different color, awaiting to be filled in by the artist.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-023.jpg?height=629&width=632&top_left_y=65&top_left_x=752)

User Input: Folk art style cat

Expanded Prompt: Photo of a feline figure captured in a folk art style, set against a rustic background. The cat displays vibrant colors and whimsical patterns, embodying the charm and simplicity of traditional folk art in a square format.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-023.jpg?height=588&width=588&top_left_y=882&top_left_x=758)

User Input: Risograph style cat

Expanded Prompt: Photo in a square composition of a cat captured in risograph style. The image showcases overlaying colors, grainy textures, and a slightly off-register print effect, evoking the unique aesthetics of risograph printing.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-023.jpg?height=607&width=612&top_left_y=1664&top_left_x=754)

User Input: Pencil sketch style cat

Expanded Prompt: Photo in a square composition of a cat

captured in pencil sketch style. The image highlights the intricate details of the feline's fur, whiskers, and eyes, with the soft shading and detailed linework reminiscent of a hand-drawn sketch.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-023.jpg?height=632&width=639&top_left_y=61&top_left_x=1380)

User Input: Cyanotype style cat

Expanded Prompt: Photo in a square composition of a cat captured in cyanotype style. The deep blue tones and high contrast showcase the feline's silhouette and intricate details, reminiscent of the early photographic cyanotype process.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-023.jpg?height=558&width=637&top_left_y=889&top_left_x=1381)

User Input: Ukiyo-e style cat

Expanded Prompt: Illustration in a 1:1 ratio of a cat rendered in the Ukiyo-e art style. The design showcases the cat lounging beneath a cherry blossom tree, with detailed patterns, soft gradients, and the characteristic elegance of Ukiyo-e artworks.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-023.jpg?height=618&width=632&top_left_y=1667&top_left_x=1386)

User Input: Watercolor style cat

Expanded Prompt: Watercolor painting in a square format of a cat, exuding a dreamy and soft ambiance. The feline's fur is painted with gentle brush strokes, blending various hues to capture its texture, and the background is a wash of muted colors that complements the cat's portrait.

Figure 14: Specifying an artistic medium. Prompts inspired by https://docs.midjourney.

com/docs/explore-prompting-1

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-024.jpg?height=623&width=615&top_left_y=68&top_left_x=121)

User Input: Wood carving style cat

Expanded Prompt: 3D render in a square composition of a cat inspired by wood carving artistry. The feline showcases intricate carved details, grain textures, and a natural wooden color palette, capturing the essence of traditional woodwork.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-024.jpg?height=643&width=635&top_left_y=852&top_left_x=108)

User Input: Acrylic style cat

Expanded Prompt: Acrylic painting in a square composition of a cat, showcasing bold brush strokes and vibrant colors. The feline's fur texture and expression are captured with the characteristic sheen and depth of acrylic paints.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-024.jpg?height=626&width=635&top_left_y=1649&top_left_x=108)

User Input: Glassblowing style cat

Expanded Prompt: 3D render in a square composition of a cat inspired by glassblowing artistry. The feline is depicted with translucent surfaces, intricate curves, and vibrant colors, showcasing the beauty and craftsmanship of glass art.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-024.jpg?height=585&width=577&top_left_y=84&top_left_x=774)

User Input: Pixel art style cat

Expanded Prompt: Pixel art illustration in a square composition of a cat. The feline is designed using a grid of colored squares, each representing a pixel. The artwork captures the cat's distinct features, with a retro and digital aesthetic reminiscent of classic video games.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-024.jpg?height=558&width=585&top_left_y=889&top_left_x=759)

User Input: Lego style cat

Expanded Prompt: Photo in a square composition of a cat constructed using LEGO bricks. The feline's form is recreated using a variety of colored blocks, showcasing the distinct features and posture of the cat in a blocky, LEGO-inspired design.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-024.jpg?height=596&width=609&top_left_y=1662&top_left_x=758)

User Input: Calligraphy style cat

Expanded Prompt: Illustration in a square composition of a cat inspired by calligraphy artistry. The feline is portrayed using flowing calligraphic strokes, intricate swirls, and a harmonious balance of thick and thin lines, capturing the elegance of calligraphy.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-024.jpg?height=623&width=629&top_left_y=68&top_left_x=1385)

User Input: Blacklight painting style cat

Expanded Prompt: Photo in a square composition of a cat depicted in blacklight painting style. Under ultraviolet light, the cat's features are illuminated with vibrant neon colors, making it stand out against a dark background, capturing the ethereal glow of blacklight art.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-024.jpg?height=637&width=632&top_left_y=858&top_left_x=1386)

User Input: Mix media style cat

Expanded Prompt: Mixed media artwork in a square composition of a cat, blending various materials and techniques. The feline's portrait incorporates elements like collage, paint, and textured paper, creating a layered and multidimensional look.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-024.jpg?height=521&width=542&top_left_y=1691&top_left_x=1431)

User Input: Vector style cat

Expanded Prompt: Vector illustration in a square composition of a cat with crisp lines, flat colors, and a minimalist design. The feline showcases geometric shapes, smooth curves, and a style that emphasizes the clarity and precision of vector graphics.

Figure 15: Specifying an artistic medium. Prompts inspired by https://docs.midjourney.

com/docs/explore-prompting-1.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-025.jpg?height=637&width=639&top_left_y=64&top_left_x=106)

User Input: a 3D cylinder and a 3D cone sitting on a grey table, square Expanded Prompt: 3D render of a scene set on a square grey table. Position on the table are two geometric forms: a cylinder and a cone. The cylinder stands vertically, its circular base resting on the table surface. Beside it, the cone is placed with its wider base on the table and its tapered end pointing upwards. Both figures are cast in a soft light, producing faint shadows on the table. The background is kept neutral to keep the focus on the geometric shapes.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-025.jpg?height=553&width=545&top_left_y=954&top_left_x=194)

User Input: A cuboid whose surface material is shiny metal. A flower and a mug are nearby with their reflections on the cuboid.

Expanded Prompt: 3D render set in a minimalist environment. Dominating scene is a shiny metallic cuboid, reflecting the surroundings with high clarity Positioned adjacent to the cuboid is a delicate flower with its petals and stem clearly mirrored on the cuboid's surface. Beside the flower, there's a mug, its curvature and handle also producing a vivid reflection on the metal. The interplay of the objects and their reflections creates a harmonious and

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-025.jpg?height=557&width=627&top_left_y=1749&top_left_x=115)

User Input: a front view of a toy elephant with clay material, rendered in 3D style.

Expanded Prompt: 3D render depicting a toy elephant, viewed from the front. The elephant is crafted with a clay-like material, giving it a textured and organic appearance. Its features, including large floppy ears, a curved trunk, and expressive eyes, are meticulously detailed. The toy elephant stands on a neutral surface, and the soft lighting in the scene accentuate the nuances of its clay texture, creating a sense of depth and realism.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-025.jpg?height=645&width=626&top_left_y=65&top_left_x=755)
cube are close to each other. The light bulb is shining bright. Expanded Prompt: 3D render of a scene set on a table. Positioned on the table are three distinct objects: a smooth sphere, a perfectly shaped cube, and a glowing light bulb. The sphere and cube are placed close to each other, with the cube's sharp edges contrasting the sphere's curved surface. A little distance away, the light bulb stands upright, emitting a bright luminance that illuminates the scene and casts defined shadows of the

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-025.jpg?height=640&width=640&top_left_y=870&top_left_x=751)
User Input: a room with two light bulbs on the ceiling. The first light bulb is shining red lights. The second light bulb is shining green lights.

Expanded Prompt: 3D render of a spacious room with a high ceiling. Two distinct light bulbs hang from the ceiling, each casting its unique glow. The first light bulb emits a deep red hue, bathing a section of the room in its warm luminescence. Adjacently, the second light bulb shines with a vibrant green light, creating a stark contrast with the red. The interplay of these two colored lights produces a mesmerizing ambiance, with the room's walls,

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-025.jpg?height=572&width=617&top_left_y=1671&top_left_x=754)

Expanded Prompt: 3D render showcasing a toy elephant, viewed from the side. The elephant is meticulously crafted with a clay-like material, lending it a rich, textured, and organic feel. Its defining features, like the large floppy ears, the gracefully curved trunk, and its soulful eyes, are exquisitel detailed. Positioned on a neutral surface, the toy elephant is bathed in soft, ambient lighting that highlights the intricacies of its clay texture, creating a visually compelling depth and realism.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-025.jpg?height=637&width=632&top_left_y=61&top_left_x=1386)

User Input: A cone sits on top of a cube next to a window with daylight shining in

Expanded Prompt: 3D render showcasing a tranquil indoor setting. Against a wall stands a large window, allowing a cascade of soft daylight to enter the room. This gentle illumination highlights a scene on a table beside the window: a perfect cube with a cone precisely positioned on its top. The cone's pointed tip contrasts with the cube's sharp edges, and the sunlight casts subtle shadows, emphasizing the geometric interplay. The background is

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-025.jpg?height=572&width=591&top_left_y=923&top_left_x=1426)

User Input: a cylinder whose surface material is shiny. A cute toy dog in wireframe is looking at the cylinder and sees its reflection on the cylinder Expanded Prompt: 3D render set against a neutral backdrop. Central to the scene is a cylinder, its surface polished to a high sheen, capturing every detail of its surroundings. A few steps away from the cylinder is a toy dog, crafted in a wireframe design. Despite its geometric structure, the toy dog exudes a sense of cuteness with its pose and proportions. The dog appears curiously fixated on the cylinder, and its wireframe silhouette is clearly

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-025.jpg?height=634&width=632&top_left_y=1667&top_left_x=1386)

Expanded Prompt: 3D render presenting a toy elephant, viewed from the back. The elephant, sculpted from a clay-like material, exudes a rich texture and organic look. While its back and tail are the main focus from this angle, glimpses of its other features, such as its large floppy ears and the curve of its trunk, can still be seen. Its soulful eyes, although not directly visible from this angle, add to the overall expressiveness of the piece. Positioned on a neutral ground, the toy elephant benefits from soft ambient lighting, which highlights the subtleties of its clay composition, offering a sense of depth and genuine craftsmanship.

Figure 16: 3D core capabilities

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-026.jpg?height=586&width=1022&top_left_y=65&top_left_x=42)

User Input: wide photos of a carbon fiber bicycle with chemiluminescent glow

Expanded Prompt: Wide photo showcasing a sleek carbon fiber bicycle standing on a dimly lit urban street. The unique feature of this bicycle is its chemiluminescent glow, which emanates from the frame, wheels, and even the handlebars. The luminescence, appearing in striking shades of blue and green, contrasts beautifully with the dark surroundings. This innovative design not only adds an aesthetic appeal but also ensures safety for nighttime riders. The ambient light from nearby street lamps casts a subtle shimmer on the ground, accentuating the glow of the bicycle and making it the focal point of the scene.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-026.jpg?height=591&width=1018&top_left_y=870&top_left_x=44)

User Input: Photos of a shimmering dress made of fiber-optic weave, glowing in various shades of blue, draped elegantly on a young woman.

Expanded Prompt: Wide photo of a young woman with dark skin elegantly wearing a shimmering dress made of fiber-optic weave. The dress glows beautifully in various shades of blue, illuminating the surroundings slightly. The ambient lighting emphasizes the radiance of the dress, casting a subtle glow on her face, revealing her serene expression. She stands confidently, allowing the dress to naturally drape and showcase its luminosity. The backdrop is muted, ensuring the focus remains on the stunning dress and the model.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-026.jpg?height=510&width=978&top_left_y=1691&top_left_x=42)

User Input: wide photos of a concept superyacht with chemiluminescent glow

Expanded Prompt: Wide photo showcasing a cutting-edge concept superyacht, poised majestically on calm ocean waters during the twilight hours. Its sleek, aerodynamic design is accentuated by a breathtaking chemiluminescent glow emanating from the yacht's contours, casting a serene blue luminescence over the surrounding waters. The water reflects the yacht's glow, creating a dreamlike atmosphere. The sky above is painted with hues of deep purples and blues, with the first stars of the evening starting to appear. The superyacht represents the pinnacle of luxury and innovation, blending seamlessly with the beauty of the natural world

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-026.jpg?height=583&width=1003&top_left_y=66&top_left_x=1076)

User Input: Wide photos of a tranquil pond where the lily pads, lotus flowers, and even the ripples on the water surface shine with a bioluminescent glow.

Expanded Prompt: Wide photo of a serene pond at nighttime. The lily pads and blooming lotus flowers emit a natural bioluminescent glow, casting soft light over the water. The gentle ripples on the water's surface catch this glow, creating a mesmerizing dance of light. Surrounding the pond, the darkness of the night contrasts sharply with the radiant beauty of the bioluminescence, making the scene look ethereal and otherworldly.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-026.jpg?height=591&width=1003&top_left_y=875&top_left_x=1076)

User Input: Wide light painting photography showcasing a little girl making light painted swirls in a park forming a heart shape, her joyful face illuminated by the glowing trails.

Expanded Prompt: Wide light painting photography capturing a moment in a dimly lit park. A little girl with an expression of delight is in the act of swirling a light-emitting device, creating luminous trails that form a heart shape around her. The soft ambient light from the surroundings contrasts beautifully with the vibrant light painted swirls, highlighting the child's joyful face which is illuminated by the mesmerizing glow of her creation.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-026.jpg?height=594&width=1011&top_left_y=1687&top_left_x=1072)

User Input: Wide photo of a pair of ballerinas gracefully dancing on a frozen pond, their pirouettes echoing the movements of the Aurora glimmer.

Expanded Prompt: Wide photo of two ballerinas, one with a fair complexion and the other with a medium complexion, gracefully dancing on a frozen pond. The shimmering reflection of the Aurora lights in the sky mirrors their movements, casting a mesmerizing glow on the ice. The ethereal beauty of the Aurora contrasts with the serene stillness of the pond, creating a harmonious blend of motion and tranquility.

Figure 17: Special lighting effects. Prompts inspired by https://juejin.cn/post

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-027.jpg?height=586&width=1019&top_left_y=65&top_left_x=43)

User Input: Street style, a woman, extreme close-up shot of the eyelash, photography, natural lighting Expanded Prompt: Drawing inspiration from urban aesthetics, this street style photograph offers an extreme close-up view of a woman's eyelash. The detailed shot showcases the intricate lengths and textures of the lashes, each individual strand highlighted by the soft glow of natural lighting. Minute reflections on the eyelash might hint at the city's vibrant life or the sky overhead. The background, though blurred due to the close focus, adds depth to the composition, ensuring the eyelash remains the focal point. This image is a celebration of the beauty in minute details, contrasting the expansive nature of city life with the delicate intricacy of a single human feature.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-027.jpg?height=591&width=1019&top_left_y=859&top_left_x=43)

User Input: Street style bird-eye-view photo of a woman, film shot, natural lighting

Expanded Prompt: Bird-eye-view street style photo capturing a woman from directly above as she walks along a city street. The film-inspired aesthetic adds a layer of grain and richness to the image. Sunlight, filtered through the buildings, casts intricate shadows on the pavement, contrasting with patches of light. The woman at the center of the frame, is adorned in modern urban attire, her shadow elongating beneath her. Around her, the city's textures - cobblestones, crosswalks, and street markings - form a mesmerizing pattern. The vintage film ambiance of the photo, combined with the contemporary fashion of the woman, creates a fusion of the old and the new.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-027.jpg?height=582&width=1019&top_left_y=1666&top_left_x=43)

User Input: Street style low-angle photo from below of a woman, film shot, natural lighting Expanded Prompt: Low-angle street style photo capturing a woman as viewed from below, emphasizing her stature and the surrounding urban environment. The film-inspired graininess adds depth and a touch of nostalgia to the scene. Natural sunlight filters between buildings, casting a warm glow on her, highlighting the textures of her outfit, and creating elongated shadows on the ground. Her attire speaks volumes of modern urban fashion, with every detail meticulously chosen. As she strides forward, her gaze might be directed towards something in the distance or lost in thought. The surrounding city elements, such as storefronts and passersby, blend into the background, putting her at the forefront. This composition marries the charm of old cinema with the vibrancy of contemporary street style.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-027.jpg?height=583&width=1006&top_left_y=66&top_left_x=1080)

User Input: Street style closeup photo of a woman, film shot, natural lighting

Expanded Prompt: Closeup street style photo of a woman, capturing intricate details of her face and expressions. The film-inspired aesthetic lends a grainy texture to the image, enhancing its nostalgic charm Natural sunlight softly illuminates her features, accentuating the contours of her face and casting gentle shadows. Her eyes carry a hint of mystery, and her lips are slightly parted, perhaps mid-conversation. Subtle makeup enhances her natural beauty without overshadowing it. The backdrop, though blurred, gives a hint of the urban environment around her. The combination of the vintage film look with the woman's contemporary style creates a captivating portrait of modern elegance.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-027.jpg?height=593&width=1008&top_left_y=858&top_left_x=1076)

User Input: Street style eye-level photo of a woman, film shot, natural lighting

Expanded Prompt: Eye-level street style photo capturing a woman as she confidently navigates the urban environment. The film-inspired aesthetic is evident through the grainy texture and warm color tones, giving the image an authentic vintage feel. Natural sunlight bathes the scene, highlighting the woman's features and casting soft, diffused shadows around her. She's dressed in modern, chic attire, perfectly complementing the bustling cityscape backdrop. Storefronts, pedestrians, and urban elements form the backdrop, adding depth and context to the image. The harmonious blend of the filmic vibe with the woman's contemporary fashion creates a timeless snapshot of city life.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-027.jpg?height=594&width=1032&top_left_y=1657&top_left_x=1078)

User Input: Street style side-angle photo from below of a woman, film shot, natural lighting

Expanded Prompt: Side-angle street style photo capturing a woman, emphasizing her stature and the

surrounding urban environment. The film-inspired graininess adds depth and a touch of nostalgia to the scene. Natural sunlight filters between buildings, casting a warm glow on her, highlighting the textures of her outfit, and creating elongated shadows on the ground. Her attire speaks volumes of modern urban fashion, with every detail meticulously chosen. As she strides forward, her gaze might be directed towards something in the distance or lost in thought. The surrounding city elements, such as storefronts and passersby, blend into the background, putting her at the forefront. This composition marries the charm of old cinema with the vibrancy of contemporary street style.

Figure 18: Camera angles, proximity, and positions.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-028.jpg?height=586&width=1002&top_left_y=43&top_left_x=60)

User Input: Street style, a woman, fish-eye lens, photography, natural lighting

Expanded Prompt: Street style photography captures a woman through the distinctive perspective of a fish-eye lens. This unique lens effect exaggerates the field of view, bending the surroundings in a circular fashion and placing the woman at the center. The urban elements around her, such as buildings and streets, curve dramatically, drawing the viewer's eye inward. Despite the distortion, the woman's fashion and presence are unmistakably modern and urban. Natural lighting enhances the scene, casting a luminous glow on her and emphasizing the warped cityscape's contours. Her attire stands out, reflecting contemporary trends while adapting to the lens's curvature. This image is a fusion of modern street style and the artistic flair of fish-eye

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-028.jpg?height=591&width=1002&top_left_y=856&top_left_x=60)

User Input: Street style, a woman, slow shutter speed, photography, night

Expanded Prompt: In the heart of the city at night, a street style photograph captures a woman using a slow shutter speed. The prolonged exposure creates a mesmerizing play of light trails, perhaps from passing cars or neon signs, wrapping around her form in a dance of luminescence. The woman stands as a semi-still figure amidst this dynamic light display, her attire reflecting the modern urban fashion. The slow shutter speed adds a ghostly, ethereal quality to her movement, making her appear as if she's drifting through the city's nocturnal pulse. Ambient city lights, street lamps, and storefronts cast a moody glow, contrasting with the deeper shadows and creating a rich tapestry of light and dark. This image is a poetic blend of contemporary style and the enchanting allure of night-time photography.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-028.jpg?height=594&width=1008&top_left_y=1671&top_left_x=57)

## User Input: Street style, a woman, instant camera, photography, night

Expanded Prompt: Set against the backdrop of the city's nocturnal ambiance, a street style photograph captures a woman as if taken through an instant camera. The image exudes the characteristic vintage charm and graininess of instant film, giving it a nostalgic touch. The woman, illuminated by the soft glow of nearby streetlights or neon signs, stands out, her attire echoing modern urban fashion yet juxtaposed with the retro feel of the photograph. The surrounding city elements, such as dimly lit alleyways, glowing storefronts, or passersby, add to the atmosphere, but she remains the central focus. Shadows play a significant role, adding depth and mystery to the composition. This image beautifully blends the spontaneity of instant camera photography with the allure of nighttime street style.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-028.jpg?height=591&width=1006&top_left_y=43&top_left_x=1080)

User Input: Street style, a woman, wide angle lens, photography, natural lighting

Expanded Prompt: Street style photography captures a woman using a wide-angle lens, creating a dynamic and expansive perspective set against the urban landscape. The wide-angle lens offers a broader field of view, encapsulating not only the woman but also a vast portion of the city around her. The curvature and distortion typical of this lens add an artistic flair, exaggerating the depth and scale of the scene. Natural lighting bathes the entire setting, emphasizing the contours of buildings, streets, and her figure. The woman, dressed in contemporary urban fashion, stands confidently, her presence magnified amidst the stretched city backdrop. This image is a vibrant blend of modern street style and the dramatic effect of wide-angle photography, offering a fresh perspective on city life.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-028.jpg?height=593&width=1008&top_left_y=858&top_left_x=1079)

User Input: Street style, a woman, fast shutter speed, photography, night

Expanded Prompt: Using a fast shutter speed, this street style photography captures a fleeting moment of a woman amidst the hustle and bustle of urban life. The quick shutter freezes her motion, rendering every detail with crisp precision, be it the sway of her hair, the ripple of her clothing, or the dynamic posture of her stride. The natural lighting enhances the clarity and depth of the image, casting sharp shadows and highlighting the textures of her outfit. Surrounding elements, perhaps moving vehicles or pedestrians, are also caught in this still frame, but she remains the undeniable focal point. Her attire, reflecting contemporary urban fashion, stands out vividly against the city backdrop. This image offers a snapshot of city life, where the transient becomes permanent through the artistry of fast shutter speed photography.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-028.jpg?height=604&width=1006&top_left_y=1671&top_left_x=1080)

User Input: Street style, a woman, tilt shift photography, photography, natural lighting

Expanded Prompt: Using the distinct technique of tilt-shift photography, this street style image captures woman set against the urban sprawl. The tilt-shift effect renders a specific portion of the image in sharp focus, most likely the woman, while gradually blurring the surroundings. This creates a miniature or 'toy-like appearance to the scene. Bathed in natural lighting, she stands out vividly, her attire reflecting contemporary urban fashion. The city around her, with its buildings, roads, and perhaps moving vehicles, takes on a surreal, almost diorama-like quality due to the selective focus. This image is a blend of modern street style and the whimsical artistry of tilt-shift photography, offering a fresh and enchanting perspective on city life.

Figure 19: Camera settings and lens.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-029.jpg?height=588&width=1022&top_left_y=42&top_left_x=42)

Expanded Prompt: Wide photo set within the University of Washington's library. Two teddy bears, distinct in color and appearance, appear as if they are deeply engrossed in their academic pursuits. They are surrounded by neat stacks of books with meticulous details on wooden desks that reflect the library's rich history. The room's ambiance is brought to life by the stained glass windows, allowing a soft light to seep in and highlight the texture and content of the books. A vintage globe stands prominently on one of the desks, adding to the academic environment. The medium shot focuses on the teddy bears' upper bodies, skillfully juxtaposing the charm of the teddies with the solemnity of the library setting.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-029.jpg?height=585&width=1011&top_left_y=862&top_left_x=53)

Expanded Prompt: Wide photo set within the University of Washington's library. Four teddy bears, distinct in color and appearance, appear as if they are deeply engrossed in their academic pursuits. They are surrounded by neat stacks of books with meticulous details on wooden desks that reflect the library's rich history. The room's ambiance is brought to life by the stained glass windows, allowing a soft light to seep in and highlight the texture and content of the books. A vintage globe stands prominently on one of the desks, adding to the academic environment. The medium shot focuses on the teddy bears' upper bodies, skillfully juxtaposing the charm of the teddies with the solemnity of the library setting.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-029.jpg?height=602&width=1028&top_left_y=1667&top_left_x=39)

Expanded Prompt: Wide photo set within the University of Washington's library. 25 teddy bears distinct in color and appearance, appear as if they are deeply engrossed in their academic pursuits. They are surrounded by neat stacks of books with meticulous details on wooden desks that reflect the library's rich history. The room's ambiance is brought to life by the stained glass windows, allowing a soft light to seep in and highlight the texture and content of the books. A vintage globe stands prominently on one of the desks, adding to the academic environment. The medium shot focuses on the teddy bears' upper bodies, skillfully juxtaposing the charm of the teddies with the solemnity of the library setting

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-029.jpg?height=589&width=1010&top_left_y=39&top_left_x=1075)

Expanded Prompt: Wide photo set within the University of Washington's library. Three teddy bears, distinct in color and appearance, appear as if they are deeply engrossed in their academic pursuits. They are surrounded by neat stacks of books with meticulous details on wooden desks that reflect the library's rich history. The room's ambiance is brought to life by the stained glass windows, allowing a soft light to seep in and highlight the texture and content of the books. A vintage globe stands prominently on one of the desks, adding to the academic environment. The medium shot focuses on the teddy bears' upper bodies, skillfully juxtaposing the charm of the teddies with the solemnity of the library setting.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-029.jpg?height=594&width=1011&top_left_y=855&top_left_x=1077)

Expanded Prompt: Wide photo set within the University of Washington's library. Five teddy bears, distinct in color and appearance, appear as if they are deeply engrossed in their academic pursuits. They are surrounded by neat stacks of books with meticulous details on wooden desks that reflect the library's rich history. The room's ambiance is brought to life by the stained glass windows, allowing a soft light to seep in and highlight the texture and content of the books. A vintage globe stands prominently on one of the desks, adding to the academic environment. The medium shot focuses on the teddy bears' upper bodies, skillfully juxtaposing the charm of the teddies with the solemnity of the library setting.
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-029.jpg?height=588&width=1010&top_left_y=1678&top_left_x=1076)

Expanded Prompt: Wide photo set within the University of Washington's library. 100 teddy bears, distinct in color and appearance, appear as if they are deeply engrossed in their academic pursuits. They are surrounded by neat stacks of books with meticulous details on wooden desks that reflect the library's rich history. The room's ambiance is brought to life by the stained glass windows, allowing a soft light to seep in and highlight the texture and content of the books. A vintage globe stands prominently on one of the desks, adding to the academic environment. The medium shot focuses on the teddy bears' upper bodies, skillfully juxtaposing the charm of the teddies with the solemnity of the library setting.

Figure 20: Crowded scene.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-030.jpg?height=588&width=1024&top_left_y=64&top_left_x=41)

Expanded Prompt: Photorealistic scene capturing the heart of Times Square during the exhilarating New Year's Eve countdown ushering in 2024. The area is densely packed with jubilant individuals, their faces reflecting the joy and optimism of welcoming a new year. Skyscrapers adorned with brilliant neon signs and screens add to the ambiance, painting the night with a myriad of colors. Central to the festivities is the New Year's Eve ball, steadily descending to mark the transition. Dominating the visual landscape, a grand digital screen prominently displays the messages 'Happy New Year' and '2024', symbolizing the collective celebration and the dawn of new possibilities.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-030.jpg?height=588&width=1019&top_left_y=861&top_left_x=43)

Expanded Prompt: Photo taken in 2010 on Mumbai's Juhu Beach during the Holi festival. Seven friends of diverse genders and descents are immersed in joy and colors, with vibrant color powder thrown all around them. Their expressions are filled with happiness and enthusiasm. The backdrop features other revelers celebrating, the vast Arabian Sea, and Mumbai's skyline in the distance. Captured in a wide shot, the full bodies of the friends are visible, with the color powder frozen mid-air, creating a dynamic and lively scene. The bright morning sunlight amplifies the colors, resulting in a high saturation that encapsulates the essence of the festival.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-030.jpg?height=593&width=1019&top_left_y=1666&top_left_x=43)

Expanded Prompt: Photorealistic visualization of a bustling urban burger eatery. The scene is dominated by a vast crowd, all eagerly waiting in an extended line, their faces a mix of anticipation and hunger. At the serving counter, the chef stands out, his face a portrait of surprise and mild distress, clearly unprepared for the surge of customers. Enhancing the ambiance of the place is its witty branding: a prominently displayed logo reading 'TLDR', and right beside it, a bold statement on the wall declares 'Too Long, Didn't Reserve'. The intricate details, from the expressions of the patrons to the playful branding, provide a snapshot of a restaurant that has unexpectedly become the hottest spot in town.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-030.jpg?height=586&width=1000&top_left_y=65&top_left_x=1083)

Expanded Prompt: Nighttime photo inspired by a 'Great Gatsby' themed party. The scene is alive with a crowd of elegantly dressed individuals dancing with fervor on an expansive outdoor lawn. Women in flapper dresses and feathered headbands move gracefully, while men in tuxedos and slicked-back hair exude 1920s sophistication. The energy of the Roaring Twenties is palpable. Above, the night sky is illuminated by a spectacular display of colorful fireworks, their bursts and patterns reflecting the exuberance of the era. Every detail, from the twinkling lights decorating the trees to the vintage cars parked in the distance, adds depth and authenticity to this lavish celebration.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-030.jpg?height=585&width=1006&top_left_y=865&top_left_x=1080)

Expanded Prompt: Photorealistic low angle perspective from within the throngs attending the CVPR 2048 international conference in Seattle. As the viewer's gaze rises, the iconic Space Needle stands tall against the sky, surrounded by the city's distinctive skyline. The crowd is a diverse mix of enthusiastic students donning backpacks and seasoned researchers deep in conversation, all gathered for this monumental event in the field of computer vision and pattern recognition. The ambient noise of discussions, networking, and the occasional laughter fills the air. A large, unmissable logo reading 'CVPR 2048' is visible, emphasizing the grandeur and importance of the event. This view, taken from amidst the attendees, offers a firsthand experience of the conference's energy, scale, and significance.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-030.jpg?height=588&width=1027&top_left_y=1663&top_left_x=1080)

Expanded Prompt: Animate-style visual capturing the intensity of the 'NBA Final'. An expansive stadium is filled to the brim with fans who are rendered in the unique anime art style. Their eyes, large and expressive, of anime-inspired reactions - from clenched fists to open-mouthed shouts. On the basketball court, the players, also in anime style, are depicted with exaggerated, dynamic poses, and their swift moves are accentuated by motion streaks. Vibrant colors dominate the scene, and sharp contrasts bring out the dynamism of the moment. Floating prominently above the scene are the stylized logos 'NBA' and 'Final', their designs harmonizing perfectly with the anime aesthetic. This image encapsulates the passion of the finals designs harmonizing perfectly with
through the distinct lens of anime.

Figure 21: Crowded scene.

## 4 Design Scenario

In the evolving landscape of design [71, 56, 55], the prowess of AI models in various design domains has become an area of keen interest. This comprehensive analysis dives into DALL-E 3's capabilities across diverse design spectrums, from the intricacies of infographics and the dynamism of animation and gaming to the finesse required in product design and the artistic nuances in visual art. Each subsection sheds light on specific challenges and achievements of DALL-E 3, presenting a holistic view of its strengths and areas of improvement. Through a series of illustrative figures and descriptions, we unravel the depth and breadth of DALL-E 3's design proficiency, offering insights into its potential and limitations in reshaping the future of design.

### 4.1 Infographics Design

This section delves into DALL-E 3's proficiency across a spectrum of infographic designs, from storybook pages and advertisements to menus, GUIs, movie posters, logos, etc.

In Figure 22, storybook pages, research posters, and menus are presented. DALL-E 3 crafts compelling layouts for each. The storybook pages feature text paragraphs, which is a significant challenge for image generation models. While DALL-E 3 struggles with paragraph perfection, individual letters are discernible and many words remain clear.

Figure 23 showcases industrial design drafts, floor plans, and GUI designs, with DALL-E 3 producing commendable text and layout renderings.

Figure 24 depicts assorted advertisement posters and book covers, each with varying text, fonts, and sizes. For example, in the two conference posters in the middle row, there are very small texts at the bottom: "the international conference on learning representation" and "Computer Vision and Pattern Recognition." It is impressive that DALL-E 3 DALL-E 3 adeptly renders the minute texts, underscoring its meticulous detailing.

Figure 25 shows movie posters, photorealistic advertisement posters, and cartoon book pages. In the movie poster at the top left, DALL-E 3 does a nice job of rendering the main character in a way that smoothly transitions between the two very different color themes. In the advertisement image at the middle left, both the brand name "crispy" and the slogan "unleash the fizz" are spelled correctly, and their rendering follows the curvature of the soda can surface. In addition, the can that the person is holding has the same look as the "Crispy" soda.

Figure 26 and 27 offer glimpses into logo designs, postcards, and themed greeting cards. Logos are sleek, while greeting cards aptly capture seasonal and cultural nuances.

Lastly, Figure 28 displays coloring book pages, where DALL-E 3 retains the signature black and white line drawing style. Figure 29 presents sticker designs set against a pristine background.

### 4.2 Animation/Gaming Design

This section explores DALL-E 3's capabilities in animation and game designs, including cinematic scenes, comic strips, storyboards, and in-game scenes.

Figure 30 shows examples of cinematic scenes. DALL-E 3 does a decent job of using closeup shots, scene depth, and lighting to enhance the drama and intensity.

Figure 31, 32, 33 present comic strips across multiple panels. Despite generating each panel independently, DALL-E 3 consistently retains character identities and adeptly positions dialogue bubbles with legible texts.

Figure 34 shows a storyboard of two warriors going from fighting to reconciliation. There are 6 images, and each image is generated independently. DALL-E 3 successfully creates the gradual emotion changes of the two warriors. In addition, DALL-E 3 is able to maintain the identities of the two warriors across the panels.

Figure 35 highlights emojis and varied cartoon styles, spanning Comics, Anime, and Ghibli.

Lastly, Figure 36 and 37 shows examples of various game-related scenarios. DALL-E 3 understands the difference between a game scene (e.g., middle left) and a game-playing environment (bottom left). In addition, it is able to generate a first-person shooter perspective with a GUI panel.

### 4.3 Product Design

This section explores DALL-E 3's capabilities in product and fashion designs as well as clothing alterations.

Figure 38 and 39 show a variety of product designs. All the product images generated by DALL-E 3 look elegant with appealing color and texture. The text font matches very well with the corresponding product type. It is interesting to note that in the "Born Pink" mug image at the middle left of Figure39, the letters "B" and "P" share a half letter. The sharing looks so natural that it is hardly noticeable.

Figure 40 presents fashion design examples. The line sketch style gives a professional look. The dresses look appropriate for the corresponding seasons.

Lastly, Figure 41 exhibits clothing alterations. DALL-E 3 adeptly interprets text prompts, adjusting garment colors and styles with precision.

### 4.4 Visual Art Design

This section explores DALL-E 3's capabilities in 3D sculpture design, historical art recreation, and time-space travel.

Figure 42 shows examples of 3D sculpture designs. At the middle left, the prompt indicates to add Sun Wukong, who is the beloved Monkey King from the Chinese novel "Journey to the West," as the fifth statue in Mount Rushmore, but DALL-E 3 mistakenly added three statues of Sun Wukong. Nonetheless, the generated image gives an illusion of being sculpted on the rock.

Figure 43 shows examples of recreating historical arts, including the city life of the capital city in the Tang dynasty and London in 1816. The image at the bottom right is an imagination of Times Square in 2075, which looks futuristic with green buildings and flying vehicles.

Figure 44 shows a variety of knolling examples. We find that DALL-E 3's knolling design usually contains a lot of detailed elements. Even though the number of elements is sometimes very large, their geometric arrangement is always aesthetically pleasing.

## The Three Litttle Pigs

Three little pid pigs each builld of the house:

the hoss, onde of straw, stick, one of blows of two hous and othe, the big bad, pif pric, them the brig bad house.

The bi bad and buff, he blows dovhe house. and hre bind the thous, plome loith bon oiwe tous, hose the the the to pad pigt be frele with the brick $\mathrm{k}$ hous.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-033.jpg?height=293&width=653&top_left_y=336&top_left_x=304)
Together he heg can't blow begfuge with thawn the wolf.

User Input: A storybook page with title "The Three Little Pigs" and text "Three little pigs each build a house: one of straw, one of sticks, and one of bricks. The big bad wolf comes along and, with a huff and a puff, blows down the first two houses. However, he can't blow down the brick house. The first two pigs take refuge with the third, and together they outsmart the wolf."

Expanded Prompt: Watercolor painting of a beautifully designed storybook page. At the top, 'The Three Little Pigs' title is written with embellishments. The story is described below: 'Three little pigs each build a house: one of straw, one of sticks, and one of bricks. The big bad wolf comes along and, with a huff and a puff, blows down the first two houses. However, he can't blow down the brick house. The first two pigs take refuge with the third, and together they outsmart the wolf.' The page's edges have subtle watercolor stains.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-033.jpg?height=550&width=534&top_left_y=939&top_left_x=275)

User Input: create a wide 16:9 poster for paper "Hierarchical Text-Conditional Image Generation with CLIP Latents", with keywords "abstract", "model", "prior", "decoder", and "results", with a method diagram in the center Expanded Prompt: Photo of a wide 16:9 academic poster for the paper titled 'Hierarchical Text-Conditional Image Generation with CLIP Latents'. The background is a subtle gradient of blue, transitioning from a deep navy at the bottom to a lighter shade at the top. Central to the poster is a method diagram, intricately detailing the process involving the abstract, model, prior, decoder, and results. The diagram has labeled arrows and boxes, each highlighting a keyword: 'abstract', 'model', 'prior', 'decoder', and 'results'. The title of the paper is prominently displayed at the top in bold white letters, and at the bottom, there's a brief summary of the findings and implication of the research. The overall design is professional, sleek, and informative, catering to an academic audience.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-033.jpg?height=593&width=1020&top_left_y=1739&top_left_x=43)

User Input: generate a menu. including "Boba Milk Tea \$4.69", "Honey Green Tea \$2.89", "Season Specia $\$ 4.39$

Expanded Prompt: 3D render of a stylish menu stand situated on a wooden table surface. It lists three items: 'Boba Milk Tea' with a price of $\$ 4.69$, 'Honey Green Tea' for $\$ 2.89$, and the 'Season Special' available for $\$ 4.39$ The aesthetic of the menu is chic and alluring.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-033.jpg?height=583&width=1005&top_left_y=66&top_left_x=1075)

User Input: A storybook page with title "Goldilocks and the Three Bears" and text "A little girl named Goldilocks stumbles upon a house in the woods. Inside, she finds three bowls of porridge: one too hot, one too cold, and one just right. She then finds three chairs and three beds, testing each one. Eventually, she falls asleep in the smallest bed. The three bears, who live in the house, come home and notice someone has been there. They find Goldilocks asleep, and she wakes up in fright and runs away.

Expanded Prompt: Photo of a storybook page opened to the title 'Goldilocks and the Three Bears'. The page has a detailed illustration of Goldilocks looking curiously at three bowls of porridge. The text reads: 'A little gir! named Goldilocks stumbles upon a house in the woods. Inside, she finds three bowls of porridge: one too hot, one too cold, and one just right. She then finds three chairs and three beds, testing each one. Eventually, she falls asleep in the smallest bed. The three bears, who live in the house, come home and notice someone has been there. They find Goldilocks asleep, and she wakes up in fright and runs away.'

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-033.jpg?height=561&width=509&top_left_y=945&top_left_x=1334)

User Input: create a wide 16:9 poster for paper "Language models are few-shot learners", with a small simple line plot in the center, with section keywords "abstract", "model", "in-context", "few-shot", and "NeurIPS 2020 " in different locations

Expanded Prompt: Wide 16:9 poster showcasing the paper titled 'Language models are few-shot learners' The background is a soft gradient of academic blue. In the center, there's a small, simple line plot, perhaps representing some data from the paper. The plot has a white background with a black grid. Around the plot in various locations and orientations are section keywords. 'Abstract' is at the top-left corner in bold, 'Model' is at the bottom-right in italic, 'In-context' is placed diagonally near the top-right, 'Few-shot' is at the bottom-left, and 'NeurIPS 2020' stands prominently at the top-center. The overall design is clean and

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-033.jpg?height=599&width=1034&top_left_y=1739&top_left_x=1074)

User Input: generate a menu for a fastfood restaurant. including "Double Cheeseburger \$1.69", "Baked Apple Pie \$0.89", "Chicken Sandwich \$4.39", "Sundae \$1.49"

Expanded Prompt: A photo of a colorful fast-food restaurant menu board. It prominently features 'Double Cheeseburger' with a price tag of '\$1.69'. Further down, 'Chicken Sandwich' is displayed for '\$4.39'. The dessert section highlights the 'Baked Apple Pie' for '\$0.89' and a delicious 'Sundae' priced at '\$1.49'. The background of the menu has a mix of vibrant colors and appetizing food imagery.

Figure 22: Rendering documents such as storybooks, academic posters, and menus.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-034.jpg?height=586&width=1019&top_left_y=65&top_left_x=43)

User Input: create an industrial drafting on a wide white paper, with a title in the top center Expanded Prompt: A wide white paper displays a detailed industrial drafting. Precision lines, measurements, and technical symbols dominate the paper, showcasing the intricate design of a machinery component. In the top center of the paper, a bold title reads 'INDUSTRIAL DESIGN'. The entire setup is placed on a wooden table, with pencils and a ruler beside the drafting.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-034.jpg?height=574&width=873&top_left_y=865&top_left_x=43)

User Input: create a floor plan with text "living room" and "bedroom"

Expanded Prompt: A floor plan sketch illustrating the layout of a house with labeled rooms. The plan includes a spacious living room adjacent to a cozy bedroom. Both rooms are clearly marked with the text 'living room' and 'bedroom'. The living room features an open concept, while the bedroom has an attached bathroom. The overall design is minimalist and functional.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-034.jpg?height=556&width=675&top_left_y=1625&top_left_x=215)

User Input: create a cellphone GUI screen for online flight booking with a "Search Flights" button Expanded Prompt: An online flight booking mobile interface. The app's logo is displayed at the top alongside a search bar. Beneath, there are input areas for the departure city, destination city date of travel, and passenger count. At the bottom, there's a noticeable 'Search Flights' button. Icons of airplanes and calendars are present adjacent to their relevant fields. The design has a calming palette of blues and whites for easy user interaction.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-034.jpg?height=586&width=1009&top_left_y=65&top_left_x=1073)

User Input: create a wide blueprint book page for an industrial drawing draft, with title "draft v1". Expanded Prompt: A wide blueprint book page showing an intricate industrial drawing draft. The layout consists of detailed diagrams, measurements, and notes relating to the design. The paper has a slightly weathered look, giving it an authentic feel. Prominently at the top, the title 'draft v1 is written in bold, vintage-styled lettering, and the overall look of the page conveys a sense of precision and professionalism.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-034.jpg?height=575&width=857&top_left_y=862&top_left_x=1076)

User Input: create a floor plan with master bedroom sized "14'7 x 13'10"

Expanded Prompt: Drawing of a house floor plan where the focal point is a master bedroom with dimensions of $14^{\prime} 7 \times 13^{\prime} 10$. The plan should be labeled and include doors and windows.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-034.jpg?height=547&width=932&top_left_y=1605&top_left_x=1071)

User Input: Photo of a website GUI interface designed for online shopping, with "deal", "cart", "Menu" sections

Expanded Prompt: Photo of a crisp and modern website GUI interface tailored for online shopping. The web page is displayed on a desktop computer monitor and a laptop screen, both of which are positioned in the center of the composition. The website's layout showcases three prominent sections labeled 'deal', 'cart', and 'Menu'. The 'deal' section flaunts some discounted items with price tags, while the 'cart' section provides a visual representation of items ready for checkout. The 'Menu' is presented as a dropdown or sidebar, allowing users to navigate through different product categories. The background of the image is a light neutral tone, emphasizing the screens and the vibrant interface.

Figure 23: Rendering documents such as industrial drafts, floorplans, and GUI.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-035.jpg?height=583&width=1020&top_left_y=66&top_left_x=43)

User Input: create a 16:9 wide ads poster for espresso. The poster should have a cup of espresso in the center with title "espresso", and "made in italy", "1995", "about", "contents", "source", Italian map and flag Expanded Prompt: Wide advertisement poster in a 16:9 ratio, showcasing espresso. In the center, there's a steaming cup of espresso on a saucer with a small spoon beside it. Above the cup, the title 'espresso' is written in bold, elegant letters. Below the cup, the phrases 'made in Italy' and '1995' are written, capturing the essence of the drink's origin and history. To the left of the cup, there's a section labeled 'about' with a brief description of espresso, and to the right, there's a section labeled 'contents' detailing the ingredients. Beneath these sections, there's another labeled 'source' explaining where the beans come from. In the background, faintly overlaying the entire poster, is a map of Italy, and in one corner, the Italian flag waves proudly.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-035.jpg?height=594&width=1026&top_left_y=879&top_left_x=43)

User Input: create a 16:9 wide poster for a conference in Vienna. The poster has title "ICLR 2024" in top, and should include "the International Conference on Learning Representations", "May 2024", "Vienna Austria"

Expanded Prompt: Wide poster for a machine learning conference set in Vienna. The background showcases Vienna's historic architecture, blending seamlessly with futuristic digital patterns symbolizing machine learning. At the top, the title 'ICLR 2024' is written in bold, modern typography. Just below, the words 'the International Conference on Learning Representations' are written in a slightly smaller font. At the bottom, 'May 2024' and 'Vienna Austria' are inscribed, reminding viewers of the date and location. The color palette consists of muted blues and golds, providing a contrast between the past and the future.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-035.jpg?height=545&width=555&top_left_y=1733&top_left_x=270)

User Input: create a book cover on a wide background. The cover should have a large piano in the center, with title "seattle symphony", and "Benaroya hall", "2023", "silver gala", "25 years" in different locations Expanded Prompt: Wide book cover featuring a large, glossy black grand piano placed centrally. Above the piano, in bold, elegant letters, is the title 'Seattle Symphony'. Just below the piano, in a slightly smaller but equally elegant font, is 'Benaroya Hall'. In the bottom left corner, the words '2023' are printed in a modern typeface, while on the bottom right corner, 'Silver Gala' stands out in a shimmering silver font. Just above the 'Silver Gala' text, '25 years' is written in a classic, timeless font, signifying the importance of the anniversary.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-035.jpg?height=586&width=764&top_left_y=62&top_left_x=1076)

User Input: create a 16:9 wide ads poster for a car. The poster should a stylish blue car in the center, with title "New 2024 Model", and "contact us", "Best", "Luxury" in different panels.

Expanded Prompt: A 16:9 wide ads poster showcasing a stylish blue car positioned prominently in the center. The car gleams under soft lighting, highlighting its sleek design and polished finish. Above the car, in bold white letters, the title reads 'New 2024 Model'. To the left and right of the car, there are panels with text. The left panel displays the words 'contact us' in an inviting font, perhaps with a phone icon. The top right panel has the word 'Best' written in golden letters, conveying quality and superiority. The bottom right panel carries the word 'Luxury', written in an elegant cursive font, evoking feelings of opulence and exclusivity.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-035.jpg?height=586&width=1008&top_left_y=886&top_left_x=1079)

User Input: create a 16:9 wide poster for a computer vision conference in Seattle. The poster should include "Computer Vision and Pattern Recognition", "CVPR 2024", "June 2024"

Expanded Prompt: Photo of the Seattle skyline at dusk with iconic landmarks like the Space Needle visible. Overlaying the skyline, translucent digital graphics represent neural networks and pattern grids. The title 'Computer Vision and Pattern Recognition' is boldly displayed at the top in modern, sleek fonts. Below it, 'CVPR 2024' is highlighted. Further down, 'June 2024' and 'Seattle Convention Center' are elegantly positioned. The color palette consists of cool blues and tech-inspired neon accents, capturing the essence of the tech conference.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-035.jpg?height=591&width=1008&top_left_y=1697&top_left_x=1076)

User Input: create a book cover on a wide background. The cover should have a stylish robot in the cente with title "Recent Advances in Multimodal Foundation Models", and "2023", "springer", "first edition" in different locations.

Expanded Prompt: Wide book cover background. In the center, a stylish robot stands confidently, reflecting the latest technology and design. At the top of the cover, in bold, legible letters is the title 'Recent Advances in Multimodal Foundation Models'. Positioned strategically around the robot are the texts '2023', 'Springer', and 'First Edition'. Each of these texts vary in size but are clearly readable, and they add to the overall professional design of the book cover.

Figure 24: Rendering documents such as ads, posters, and book covers.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-036.jpg?height=586&width=1024&top_left_y=65&top_left_x=41)

User Input: create a wide movie poster for a movie about war and peace. There is a main character in the center, with the movie name and date in the bottom

Expanded Prompt: Wide movie poster depicting the theme of war and peace. In the center stands a main character, a determined individual with a mix of weariness and hope in their eyes. Behind them, on the left side, there's a scene of a raging battlefield with explosions and soldiers charging into battle, while on the right side, there's a serene landscape of a peaceful village with children playing and families gathering. The contrast between the two sides is stark, representing the duality of war and peace. At the bottom of the poster, bold letters spell out the movie name 'Duality of Fate' and below that, the release date 'November 25, 2023' is displayed in slightly smaller font.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-036.jpg?height=585&width=1019&top_left_y=843&top_left_x=43)

User Input: create a wide advertisement design for "crispy" soda in a realistic photo style. There is a person drinking the soda during a race in the center, with the runners and a park in the background, and the product name "crispy and slogan "unleash the fizz" in the bottom.

Expanded Prompt: Photo advertisement design for 'crispy' soda. The scene unfolds during a race, with a diverse group of runners sprinting on a park path. A Caucasian male in athletic attire is prominently placed in the center drinking a can of 'crispy' soda while running, emphasizing the refreshment it offers. His expression is one of delight and rejuvenation. In the background, runners of different descents and genders can be seen, with trees and park benches creating a serene environment. At the bottom, in bold and vibrant letters, the product name 'crispy' is written, accompanied by the slogan 'unleash the fizz'.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-036.jpg?height=564&width=517&top_left_y=1659&top_left_x=289)

User Input: create a wide graphical novel for story "three rabbits". The novel is in a cartoon style, the nove name and other texts written on the cover

Expanded Prompt: Wide cartoon-style cover of a graphical novel titled 'Three Rabbits'. The background features a serene meadow under a twilight sky, adorned with twinkling stars. In the foreground, three rabbits of varying sizes and colors sit side by side, each with a unique expression: one curious, one mischievous, and one content. The title 'Three Rabbits' is written in bold, playful letters at the top, with smaller text at the bottom detailing the author's name and a brief tagline about the adventurous tale within.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-036.jpg?height=586&width=1005&top_left_y=65&top_left_x=1075)

User Input: create a wide movie poster for a movie about romance. There are the male and female main characters in the center, with the movie name and date in the bottom

Expanded Prompt: Wide movie poster for a romantic film. In the center towards the left, a male and female main character stand closely, gazing into each other's eyes, radiating deep affection. They're bathed in a soft glow, highlighting their facial features and the chemistry between them. The background displays a scenic sunset with hues of pink and orange, lending an air of romance. In the top right corner, in elegant cursive font, the movie name 'Heartstrings Tangled' is written, below which, in smaller font, is the release date 'December 24,2023 .

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-036.jpg?height=602&width=1008&top_left_y=832&top_left_x=1076)

User Input: create a wide advertisement for "DALLE" cereal in a realistic photo style. There is a white box of cereal placed on the table in the center, with a bowl of cereal and a glass of milk next to it, with kitchen blurred in the background, the produce name "DALLE" and a spoon of cereal is printed on the box

Expanded Prompt: Photo of a wide advertisement for 'DALLE' cereal. In the forefront, there's a white cereal box placed centrally on a wooden table. On the box, the product name 'DALLE' is boldly written, along with an image of a spoon filled with cereal. To the right of the box, there's a bowl overflowing with cereal and a tall glass of milk beside it. The background features a cozy kitchen setting with countertops, cabinets, and appliances, all slightly blurred to emphasize the product in the foreground.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-036.jpg?height=588&width=1008&top_left_y=1647&top_left_x=1076)

User Input: create a wide graphical novel page for story "dragon and deer". The novel is in a cartoon style, the large sound effect text illustrations on pages

Expanded Prompt: Wide graphical novel page for the story 'Dragon and Deer'. The page is designed in a cartoon style. On the top left corner, a majestic dragon with shimmering scales hovers above a dense forest, its wings extended and casting shadows below. Just below the dragon, a graceful deer, with large, expressive eyes, stands at the edge of a clearing, gazing up at the dragon with a mix of curiosity and awe. Speech bubbles emerge from both characters: the dragon says, 'Why are you not afraid?', and the deer replies, 'Because I see the kindness in your eyes.' The background shows a setting sun, painting the sky in hues of orange and pink. On the right side of the page, large sound effect text illustrations dramatically emphasize the moment. Words like 'WHOOSH' for the dragon's wings and 'RUSTLE' for the movement of leaves in the forest are depicted in bold, vibrant colors, adding to the overall ambiance of the scene.

Figure 25: Movie poster, Ads.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-037.jpg?height=586&width=1019&top_left_y=108&top_left_x=43)

Prompt: Watercolor painting of a logo for 'Chocolate', influenced by the renaissance period, featuring colored ink art of chocolate-covered strawberries, elegant tapestries, and an ornate frame, with the word 'Chocolate' written in decorative script, set on a vintage paper canvas.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-037.jpg?height=504&width=486&top_left_y=819&top_left_x=337)

Prompt: 16:9 watercolor painting styled logo for 'Insomnia' cafe. Dominating the design is a coffee mug silhouette. Within this mug, a mesmerizing nightscape reveals itself, with a crescent moon and twinkling stars shining brightly against a deep blue watercolor wash. Right beneath the mug, the cafe's name, 'Insomnia', is beautifully penned in an elegant handwritten script, evoking a sense of tranquility and artistry that complements the theme of the logo.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-037.jpg?height=586&width=1016&top_left_y=1515&top_left_x=45)

Prompt: Postcard design showcasing the lantern festival at Washington Lake. The front of the postcard is dominated by the mesmerizing sight of thousands of lanterns, which gently float on the lake and ascend into the evening sky, embodying the essence of peace. The shimmering reflections of the lanterns dance on the water's surface, adding depth and movement to the scene. Nestled in the distance is the faint cityscape of Seattle, its outline adding a touch of modernity to the traditional festival. Towering over the scene is the iconic Mount Rainier, its majestic silhouette contrasting beautifully with the lantern-lit foreground. Emblazoned at the bottom in elegant font are the words 'Mid Autumn', capturing the spirit of the festival. The back of the postcard has space for a message and address, making it perfect for sharing this magical experience with loved ones.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-037.jpg?height=588&width=1006&top_left_y=107&top_left_x=1080)

Prompt: Elegant graphic logo for the skincare brand 'Skin', where a sleek marble tile serves as the backdrop, and a golden laurel wreath encircles the word 'Skin', written in refined gold typography, symbolizing purity and excellence.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-037.jpg?height=588&width=1025&top_left_y=779&top_left_x=1081)

Prompt: Photo-realistic scene of a beachside ice cream stand run by little gray penguins. The humor is evident as one penguin tries to climb a ladder to reach a tall ice cream stack, another uses a tiny fan to keep cool, and a third penguin seems to be bargaining with a seagull. The sunny beach atmosphere, complete with laughing beachgoers, adds to the comedic setting. Cinematic film-style lighting enhances the vibrant colors and casts playful shadows.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-037.jpg?height=574&width=1000&top_left_y=1518&top_left_x=1083)

Prompt: Fairytale style postcard featuring the enchanting Neuschwanstein Castle. The front of the postcard unveils a wonderland, where a dense, mystical forest cradles the castle. The castle, known for its romantic and intricate architecture, is bathed in the soft glow of a radiant moon. This moonlight creates a dreamy atmosphere, contrasting beautifully with the colorful blooms surrounding the castle. A postal stamp, showcasing a miniature depiction of the castle, is thoughtfully placed in the top right corner. Emblazoned at the bottom are the words 'Wonderland', followed by 'Bavaria, Germany' and the date 'June 1, 2035', serving as a testament to the time and place of this magical scene. The back of the postcard has space for a personal message, alongside lines for the recipient's address, making it a perfect memento for those wanting to share or remember this otherworldly experience.

Figure 26: Logo and postcard.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-038.jpg?height=583&width=1019&top_left_y=66&top_left_x=43)

Expanded Prompt: A captivating 16:9 greeting card set against a backdrop of autumn leaves in rich shades of orange, red, and gold. The word 'Thanksgiving' is elegantly written in the center with a cursive gold font, surrounded by subtle decorative elements like pumpkins, acorns, and corn. Gentle rays of sunlight peek through the leaves, creating a warm and inviting glow on the card.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-038.jpg?height=531&width=764&top_left_y=865&top_left_x=168)

Expanded Prompt: A mesmerizing 16:9 greeting card that paints a romantic evening scene with a deep indigo sky studded with twinkling stars. The moon casts a soft silvery glow on a serene lake below. In the foreground, a pair of intertwined swans glide gracefully on the water. Centered on the card, the words 'Happy Anniversary' shine brightly with a holographic effect, making them stand out. The edges of the card are decorated with intricate silver lace patterns, adding to its elegance and charm.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-038.jpg?height=539&width=480&top_left_y=1666&top_left_x=302)

Expanded Prompt: A dynamic 16:9 greeting card capturing the essence of graduation. The background is painted in a gradient of sky blue to deep navy, symbolizing the transition from dawn to the vastness of the night sky. Floating upwards are illustrated caps and diplomas, representing the flying ambitions of graduates. In the center, a large, golden trophy stands proud, its shimmer reflecting the light of distant stars. Around it, the word 'Congratulations' is written in a sophisticated script font, gleaming in gold. Just below the trophy, the word 'Graduate' stands out in a bold, embossed white font. To add a touch of personalization, there are spaces for hand-written messages on the bottom corners, bordered by delicate gold filigree

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-038.jpg?height=586&width=1011&top_left_y=62&top_left_x=1077)

Expanded Prompt: A stunning 16:9 greeting card adorned with a background of soft pastel colors, transitioning from light lavender to a subtle blush pink. In the center, the words 'Celebrating 15 Years of Love' are gracefully scripted with a shimmering silver font. Surrounding the text, there are delicately drawn heart motifs, intertwined vines, and blooming roses. Glimmers of silver and gold sparkles are scattered throughout the design, adding a touch of elegance and celebration to the card.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-038.jpg?height=580&width=1000&top_left_y=843&top_left_x=1083)

Expanded Prompt: A whimsical 16:9 greeting card that sets the stage for a mysterious Halloween night. The backdrop features a hauntingly beautiful midnight blue sky, with a large orange harvest moon illuminating the silhouette of a crooked, ancient tree. Perched on its branches are eerie black ravens, casting watchful eyes on the scene below. Playful ghosts and floating jack-o-lanterns dance around the tree, creating a lively atmosphere. In the foreground, the words 'Happy Halloween' are crafted in a dripping, glowing green font, as if written with a witch's potion. Cobwebs and tiny spiders embellish the card's corners, adding a touch of spooky charm.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-038.jpg?height=594&width=1010&top_left_y=1638&top_left_x=1078)

Expanded Prompt: A captivating 16:9 greeting card that evokes the celebration of the New Year 2024. The backdrop showcases a serene winter landscape, with snow-covered pine trees and distant mountains under a soft, pastel-hued northern lights display. A gentle snowfall adds to the enchantment. In the foreground, a crystal-clear frozen lake reflects the aurora borealis. Placed elegantly on the ice is the message 'Happy New Year', written in a sparkling silver font. Beneath it, '2024' is etched in ice with a delicate frost pattern around it. The corners of the card are adorned with intricate silver snowflake designs, enhancing the festive feel.

Figure 27: Greeting card.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-039.jpg?height=586&width=850&top_left_y=46&top_left_x=128)

Expanded Prompt: Line-art illustration in a 16:9 aspect ratio designed for children aged 6 and under. The scene unveils a whimsical fantasy world with candy-colored clouds floating above and magical sparkles scattered throughout. On a grassy knoll, two adorable teddy bears sit, one with a bowtie and the other with a ribbon on its ear, sharing a sweet moment. Nearby, two little pigs with curly tails and cheerful expressions are seen playing, one of them jumping over a puddle with glee. Surrounding them are oversized, colorful mushrooms, some of which have doors and windows, suggesting they are homes for tiny magical creatures. A rainbow arcs in the background, and playful butterflies flit about. The overall design, with its bold outlines and simple shapes, makes it a delightful coloring book page for young kids.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-039.jpg?height=591&width=777&top_left_y=865&top_left_x=159)

Expanded Prompt: Line-art illustration in a 16:9 aspect ratio tailored for children aged 6 and under. In a playful, cartoon-style meadow, a round and friendly hedgehog interacts with a cheerful, wide-eyed deer. The hedgehog has soft, stubby spines and a big smile, while the deer showcases its small antlers and a wagging tail. They are surrounded by oversized, simplified sunflowers with big, smiling faces. The petals and leaves of the sunflowers are drawn with bold outlines for easy coloring. Fluttering around the scene are cartoon butterflies with large wings. The sky above them is filled with puffy clouds and a beaming sun. The entire scene is designed with young children in mind, ensuring it is engaging yet easy to color.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-039.jpg?height=580&width=1022&top_left_y=1664&top_left_x=42)

Expanded Prompt: Line-art illustration in a 16:9 aspect ratio showcasing a captivating underwater world. The seabed is adorned with intricate coral formations, waving seaweeds, and hidden treasures half-buried in the sand. Amidst this aquatic wonderland, two mermaids gracefully glide. One mermaid, with a flowing long braid, interacts with a group of playful seahorses, while the other, adorned with starfish accessories, sings a melodic tune to a gathering of fishes. Surrounding them are diverse marine life like the playful dolphins, elegant stingrays, and schools of vibrant tropical fishes. Overhead, gentle sun rays filter through the water's surface, creating a serene ambiance. The entire scene is framed with a decorative border of seashells and pearls, making it a delightful coloring book page.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-039.jpg?height=600&width=907&top_left_y=47&top_left_x=1132)

Expanded Prompt: Line-art illustration in a 16:9 aspect ratio tailored for children aged 6 and under. The scene is set in a simplified magical forest with cartoon-style, rounded trees that have cheerful faces. Soft, puffy clouds float above. In the center of this delightful setting, a chubby unicorn with large, friendly eyes and a curly mane prances playfully. Its horn is short and spiraled, and its tail has big, bold swirls for easy coloring Scattered around are oversized, cute mushrooms with smiling faces and a few friendly forest critters like a waving squirrel and a hopping bunny. The ground is dotted with large, simple flowers, and a sun with a radiant smile shines in the corner. The design's simplicity and bold outlines make it perfect for younger children to enjoy coloring.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-039.jpg?height=591&width=830&top_left_y=862&top_left_x=1168)

Expanded Prompt: Line-art illustration in a 16:9 aspect ratio tailored for children aged 6 and under. The scene takes place inside a cozy cartoon-style home. In the living room, a fluffy generic cartoon cat with big eyes lounges on a soft couch, while another slender cat plays with a yarn ball nearby. A playful kitten chases a butterfly that has found its way inside. In the background, there's a cute bird perched on a windowsill and a friendly dog wagging its tail by the entrance. The room is adorned with simple furniture like a round table with framed pictures on the walls. The overall design is simplistic with bold outlines, making it perfect for younger children to color in.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-039.jpg?height=596&width=832&top_left_y=1659&top_left_x=1167)

Expanded Prompt: Line-art illustration in a 16:9 aspect ratio tailored for high school students. The scene presents an array of elegant geometric patterns and shapes. Intersecting circles form intricate mandalas, triangles tessellate to create a kaleidoscopic effect, and spiraling hexagons give a three-dimensional illusion. Alongside these, there are flowing wave patterns juxtaposed with straight, crisp lines, creating a contrast. The design also incorporates optical illusions and intricate latticework that challenge the eye and provide a variety of coloring opportunities. The overall composition is balanced, offering areas of complexity and simplicity to cater to different moods and preferences. This sophisticated geometric design is colored with harmony and balanced colors, sure to engage and captivate older students, offering them a therapeutic coloring experience.

Figure 28: Coloring book

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-040.jpg?height=545&width=802&top_left_y=126&top_left_x=152)

Expanded Prompt: Widescreen image presenting die-cut stickers of a bull, crafted with vibrant colors such as electric blue, fiery orange, and neon green. Each sticker is defined by sharp white borders that emphasize the bull's dynamic shape. The bull's design is a blend of modern and fierce, with the bull charging forward in a stance of power and determination. The minimal background offers a neutral tone, possibly a soft gray, ensuring that the colorful bull stickers are the main focal point. The die-cut technique gives each sticker a unique silhouette, following the bull's contours, making them stand out and look premium.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-040.jpg?height=312&width=768&top_left_y=982&top_left_x=172)

Expanded Prompt: Widescreen image displaying a die-cut sticker of Seattle's iconic skyline, painted with vibrant colors such as deep blues, sunset oranges, and twilight purples. Notable landmarks like the Space Needle, Smith Tower, and the skyline of downtown Seattle can be easily recognized. The sticker is highlighted by clear white borders that accentuate the cityscape's silhouette against a gradient sky. The design captures the essence of Seattle during a picturesque sunset, with skyscrapers' lights starting to twinkle. The minimal background, perhaps a shade of light gray or muted cream, ensures the colorful Seattle skyline sticker remains the central visual attraction. The die-cut method molds the sticker to follow the city's unique skyline, offering a captivating and top-quality appearance.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-040.jpg?height=490&width=494&top_left_y=1663&top_left_x=295)

Expanded Prompt: Widescreen image showcasing die-cut stickers of a basketball engulfed in flames, rendered with intense colors like fiery orange, blazing red, and bright yellow. The sticker is emphasized by sharp white borders that accentuate the basketball's round shape and the dynamic flames surrounding it. The design captures the basketball's texture and the energetic movement of the flames, symbolizing power and passion. The minimal background, perhaps a shade of light gray or muted beige, ensures that the colorful, on-fire basketball sticker remains the main visual attraction. The die-cut method grants the sticker a unique silhouette, closely mirroring the basketball and flames, offering a dynamic and high-quality appearance.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-040.jpg?height=602&width=1022&top_left_y=89&top_left_x=1061)

Expanded Prompt: Widescreen image presenting a die-cut sticker of an astronaut's helmet, designed with radiant colors such as gleaming silver, starry blue, and cosmic purple. The sticker is outlined by crisp white borders that accentuate the helmet's intricate details. The helmet's visor reflects distant galaxies and stars, adding depth and intrigue to the design. The minimal background, possibly a muted shade of gray or beige, ensures that the colorful astronaut helmet sticker remains the central attraction. The die-cut technique provides the sticker with a distinct silhouette, closely following the helmet's contours, creating a premium and modern appearance.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-040.jpg?height=507&width=474&top_left_y=907&top_left_x=1357)

Expanded Prompt: Widescreen image portraying a die-cut sticker of a delightful strawberry, painted with vivid colors such as luscious red and fresh green. The sticker is emphasized by defined white borders that underline the strawberry's characteristic shape and texture. The strawberry design showcases a glossy, ripe appearance with little yellow seeds and a bright green leafy crown. The minimalist backdrop, perhaps a shade of soft gray or muted cream, ensures the colorful strawberry sticker remains the primary visual element. The die-cut method tailors the sticker to a specific shape, matching the strawberry's natural contours, resulting in a captivating and high-quality appearance.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-040.jpg?height=480&width=510&top_left_y=1701&top_left_x=1336)

Expanded Prompt: Widescreen image portraying a die-cut sticker of a delightful avocado, painted with lush colors such as deep green for its skin and creamy yellow for its fleshy interior. The sticker is outlined by crisp white borders which emphasize the avocado's characteristic shape and texture. The design showcases a ripe avocado cut in half, revealing its smooth flesh and a shiny brown seed in the center. The minimal background perhaps a shade of soft gray or muted beige, ensures the colorful avocado sticker remains the primary visual attraction. The die-cut method crafts the sticker into a unique shape, closely matching the avocado's natural contours, resulting in a fresh and premium visual appeal.

Figure 29: Stickers.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-041.jpg?height=583&width=1019&top_left_y=66&top_left_x=43)

User Input: A dialog scene from a drama movie, a man is looking at a woman as she looks off into the distance. The setting is a steampunk city, has reverse key lighting and dramatic warm color grading.

Expanded Prompt: Photo of a dramatic scene set in a steampunk city. A man in his mid-30s with slicked-back dark hair, deep hazel eyes, and sharp features is looking intently at a woman. He is wearing a high-collared white shirt, a brown leather vest with brass buttons, a dark green tailcoat, brown pants with leather suspenders, and knee-high boots. He has a pocket watch chain and a pendant around his neck. The woman, with her back partially to the viewer, gazes off into the distance. The atmosphere has reverse key lighting, emphasizing the shadows and creating a sense of depth. The entire scene is washed in a dramatic warm color grading, enhancing the emotional intensity of the moment.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-041.jpg?height=582&width=1022&top_left_y=972&top_left_x=42)

User Input: A fighting scene from an action movie, a man holding a gun with another person with a knife in a restaurant.

Expanded Prompt: Photo of an intense action movie scene inside a restaurant. In side view, a tall, muscular man with blue eyes and a short salt and pepper buzz cut stands assertively, his attire consisting of a black tactical shirt, dark jeans, a utility belt, leather gloves, and combat boots. He tightly grips a matte black semi-automatic pistol, searching for another individual menacingly brandishing a knife, hiding behind one of the overturned tables. The blurred background reveals overturned tables and broken dishes scattered across the floor, symbolizing the fierce battle that has just occurred.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-041.jpg?height=593&width=1019&top_left_y=1815&top_left_x=43)

Expanded Prompt: Blackwhite line-drawing sketch of a dramatic scene set in a steampunk city. A man in his mid-30s with slicked-back dark hair, deep hazel eyes, and sharp features is looking intently at a woman. He is wearing a high-collared white shirt, a brown leather vest with brass buttons, a dark green tailcoat, brown pants with leather suspenders, and knee-high boots. He has a pocket watch chain and a pendant around his neck. The woman, with her back partially to the viewer, gazes off into the distance. The atmosphere has reverse key lighting, emphasizing the shadows and creating a sense of depth. The entire scene is washed in a dramatic warm color grading, enhancing the emotional intensity of the moment.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-041.jpg?height=583&width=1003&top_left_y=66&top_left_x=1079)

Expanded Prompt: Photo of a dramatic scene set in a steampunk city. A man in his mid-30s with tousled blonde hair, deep hazel eyes, and sharp features is looking intently at a woman. He is wearing a tight-fitting black shirt, a brown leather vest with brass buttons, a dark green tailcoat, brown pants with leather suspenders, and knee-high boots. He has a pocket watch chain and a pendant around his neck. The woman, with her back partially to the viewer, gazes off into the distance. The atmosphere has reverse key lighting, emphasizing the shadows and creating a sense of depth. The entire scene is washed in a dramatic warm color grading, enhancing the emotional intensity of the moment.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-041.jpg?height=588&width=1008&top_left_y=969&top_left_x=1076)

Expanded Prompt: Photo of an intense action movie scene inside a restaurant. In side view, a lady with blue eyes and a short salt and pepper buzz cut stands assertively, her attire consisting of a black tactical shirt, dark jeans, a utility belt, leather gloves, and combat boots. She tightly grips a matte black semi-automatic pistol, searching for another individual menacingly brandishing a knife, hiding in low position behind one of the tables. The blurred background reveals overturned tables and broken dishes scattered across the floor, symbolizing the fierce battle that has just occurred.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-041.jpg?height=591&width=789&top_left_y=1819&top_left_x=1183)

Expanded Prompt: Blackwhite line-drawing sketch of an intense action movie scene inside a restaurant. In side view, a tall, muscular man with blue eyes and a short salt and pepper buzz cut stands assertively, his attire consisting of a black tactical shirt, dark jeans, a utility belt, leather gloves, and combat boots. He tightly grips a matte black semi-automatic pistol, searching for another individual menacingly brandishing a knife, hiding behind one of the overturned tables. The blurred background reveals overturned tables and broken dishes scattered across the floor, symbolizing the fierce battle that has just occurred.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-042.jpg?height=1066&width=629&top_left_y=39&top_left_x=127)

User Input: Scene: Hedgehog stands at the entrance of a forest path, holding a map, lookin
a bit lost.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-042.jpg?height=52&width=591&top_left_y=1134&top_left_x=130)
a map and looks a bit lost. Above the hedgehog is a comic bubble that reads. 'Hope on the right way to Deer's house'. Trees with pastel-colored leaves frame the path, and the

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-042.jpg?height=1078&width=615&top_left_y=1258&top_left_x=129)

User Input: Scene: Hedgeh
path towards the clearing

Comic Bubble (Hedgehog): "Wow! Nature GPS!"

Expanded Prompt: In the same whimsical pastel-colored world, the hedgehog with circular glasse, a striped scarf, and a tiny backpack is surrounded by a swarm of glowing fireflies. Th
fireflies form a luminous path leading the hedgehog towards a distant clearing in the forest. fireflies form a luminous path leading the hedgehog towards a distant clearing in the forest.
The scene is bathed in a soft glow, and the atmosphere is serene and magical. The hedgehoo looks in awe at the firefly path, , with eyes wide in wonder. Above the hedgehog is a comic
bubble that exclaims, 'Wow! Nature GPS!' The scene is serene with pastel-colored trees and
plants surrounding them.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-042.jpg?height=1035&width=617&top_left_y=68&top_left_x=754)

User Input: Scene: Hedgehog encounter
Comic Bubble (Bird): "Lost, little one?"

Comic Bubble (Hedgehos). "Deer

Expanded Prompt: In the same whimsical pastel-colored world, the hedgehog with circular

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-042.jpg?height=35&width=577&top_left_y=1182&top_left_x=759)
Above the bird is a comic bubble that reads, 'Lost, little one?'. The hedgehog looks up at th bird with a surprised expression, and above the hedgehog is another comic bubble that
exclaims, 'Deer!' The scene is serene with pastel-colored trees and plants surrounding them

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-042.jpg?height=1079&width=621&top_left_y=1260&top_left_x=752)

User Input: Scene: Deer
Hedgehog approaches.

omic Bubble (Deer): "You made it!

Expanded Prompt: In a dreamy world painted with pastel shades, a hedgehog adorned with
circular glasses, a scarf with strips, entrance of the treehouse, a deer in a floral-patterned dress welcomes the hedgehog by waving her hands. The treehouse is designed with round windows and a wooden door, and
gentle light glows from inside. A comic bubble hovers above the deer with the words, 'You
made it!'. The hedgehog gazes up at the deer, his face beaming with happiness. The scene is

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-042.jpg?height=1066&width=618&top_left_y=39&top_left_x=1377)

User Input: Scene: The bird points with its wing towards a lit cle
Comic Bubble (Bird): "Follow the fireflies, they'll lead the way."

Expanded Prompt: In the same whimsical pastel-colored world, the hedgehog with circular glasses, a striped scarf, and a tiny backpack, encounters a bird perched on a branch. The
bird, adorned with pastel feathers and an inquisitive expression, points with its wing

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-042.jpg?height=35&width=577&top_left_y=1198&top_left_x=1388)
pastel-colored trees and plants, with the fireflies' glow casting a magical ambiance over the

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-042.jpg?height=1076&width=610&top_left_y=1267&top_left_x=1381)
User input: Scene:
on a small table.

Comic Bubble (Deer): "Friends always find each other, "

xpanded Prompt: A serene pastel-colored backdrop sets the stage for a hedgehog with decorated with delightful ornaments. A circular window lest in mellow light, highlighting a able set for tea. A comic bubble emerges from the deer, proclaiming, 'Friends always find each other', capturing the quiet and comforting atmospher

Figure 31: Rendering comic strip with multiple panels.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-043.jpg?height=572&width=1029&top_left_y=148&top_left_x=36)

User Input: Panel 1:Pusheen stands in a bright kitchen, eyes wide with excitement. A countertop is covered with various pizza ingredients: dough, tomato sauce, cheese, and toppings. Comic bubble reads: "Time to make the purrfect pizza!" Expanded Prompt: Wide illustration style comic strip with soft pastel colors. Pusheen, the chubby gray tabby cat with big round eyes and tiny paws, stands in a bright kitchen filled with excitement. The countertop is laden with various pizza ingredients like dough, tomato sauce, cheese, and toppings. A comic bubble next to Pusheen exclaims: 'Time to make the purrfect pizza!'

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-043.jpg?height=594&width=902&top_left_y=885&top_left_x=167)

User Input: Panel 3: Pusheen triumphantly spreads tomato sauce on the now-flattened dough, leaving tiny paw prints of sauce. A little tomato sauce can is seen knocked over. Comic bubble reads: "Sauce-tastic!

Expanded Prompt: Wide illustration style comic strip with soft pastel colors. Pusheen, the chubby gray tabby cat with big round eyes and tiny paws, is in a kitchen triumphantly spreading tomato sauce on the now-flattened pizza dough. Tiny paw prints of Pusheen exclaims: 'Sauce-tastic!'

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-043.jpg?height=563&width=1022&top_left_y=1651&top_left_x=42)

User Input: Panel 5: Pusheen, now covered in cheese, places the final topping on the pizza. The scene is comical with cheese strands hanging from its whiskers. Comic bubble reads: "Cheese overload.

Expanded Prompt: Wide illustration style comic strip with soft pastel colors. Pusheen, the chubby gray tabby cat with big round eyes and tiny paws, is in a kitchen covered in cheese. As Pusheen places the final topping on the pizza, the scene becomes comical with cheese strands hanging from its whiskers and draped over its body. A comic bubble next to Pusheen reads: 'Cheese overload

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-043.jpg?height=558&width=1018&top_left_y=179&top_left_x=1071)

User Input: Panel 2: Pusheen struggles to roll out the pizza dough with a rolling pin, dough sticking to its tiny paws. The scene is humorous with dough pieces everywhere. Comic bubble reads: "Why is this so dough-ficult?"

Expanded Prompt: Wide illustration style comic strip with soft pastel colors. Pusheen, the chubby gray tabby cat with big round eyes and tiny paws, is in a kitchen humorously struggling to roll out the pizza dough with a rolling pin. The dough tenaciously sticks to its tiny paws, causing dough pieces to scatter everywhere. A comic bubble next to Pusheen reads: 'Why is this so dough-ficult?'

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-043.jpg?height=599&width=1025&top_left_y=877&top_left_x=1081)

ser Input: Panel 4: Pusheen gazes at an array of toppings: pepperoni, olives, bell peppers, and anchovies. Pusheen look confused, trying to decide. Comic bubble reads: "So many choices, so little time!" captivating eyes and tiny paws, is situated in a cozy kitchen. On the countertop before Pusheen, there's an array of pizza toppings juicy pepperoni, ripe olives, colorful bell peppers, and salty anchovies. Pusheen has an expression of genuine puzzlement, looking overwhelmed by the vast selection. Above Pusheen, a comic bubble states: 'So many choices, so little time!

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-043.jpg?height=559&width=1008&top_left_y=1645&top_left_x=1076)

User Input: Panel 6: Pusheen sits proudly next to a perfectly baked pizza, a chef's hat perched atop its head. The aroma wafts up, depicted with wavy lines. Comic bubble reads: "Voil! Purrfect pizza masterpiece!" eyes and tiny paws, sits proudly in a kitchen next to a perfectly baked pizza. Pusheen is wearing a chef's hat perched atop its head. The aroma of the pizza wafts up, depicted with wavy lines indicating its delicious scent. A comic bubble next to Pusheen exclaims: 'Voil! Purrfect pizza masterpiece!'

Figure 32: Rendering comic strip with multiple panels.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-044.jpg?height=531&width=632&top_left_y=176&top_left_x=237)

User Input: Planning the Presents: Pusheen sits at a desk, pen in paw, with a long list titled "Christmas Gifts." Comic bubble reads, "Gotta make sure I don't forget anyone!

Expanded Prompt: Illustration style comic strip featuring a festive Pusheen cat adorned with a tiny Santa hat and a red scarf. In comic bubble next to Pusheen that reads, 'Gotta make sure I don't forget anyone!

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-044.jpg?height=531&width=732&top_left_y=916&top_left_x=184)

User Input: Crafty Pusheen: Pusheen sits at a crafting table, wrapping presents with colorful papers and ribbons. Comic bubble exclaims, "It's wrap-tastic time

Expanded Prompt: Illustration style comic strip featuring a festive Pusheen cat adorned with a tiny Santa hat and a red scarf. In comic bubble next to Pusheen that exclaims, 'It's wrap-tastic time!
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-044.jpg?height=536&width=888&top_left_y=1640&top_left_x=110)

User Input: Sneaky Delivery: Pusheen tiptoes, holding a sack of presents, placing them under a beautifully decorated Christmas tree. Comic bubble whispers, "Quiet as a mouse.

Expanded Prompt: Illustration style comic strip featuring a festive Pusheen cat adorned with a tiny Santa hat and a red scarf. Christmas tree. There is a comic bubble next to Pusheen that whispers, 'Quiet as a mouse!

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-044.jpg?height=591&width=1016&top_left_y=146&top_left_x=1075)

User Input: Shopping Spree: Pusheen pust

Expanded Prompt: Illustration style comic strip featuring a festive Pusheen cat adorned with a tiny Santa hat and a red scarf. In the scene titled 'Shopping Spree', Pusheen pushes a small shopping cart through an aisle filled with toys, decorations, and treats. There is a comic bubble next to Pusheen that says, 'So many choices!'
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-044.jpg?height=532&width=822&top_left_y=909&top_left_x=1170)

User Input: Gift Mischief: Pusheen is playfully tangled in a string of lights, with a wrapped gift box toppled over. Comic bubble Expanded Prompt: Illustration style comic strip featuring a festive Pusheen cat adorned with a tiny Santa hat and a red scarf. In the scene titled 'Gift Mischief', Pusheen is playfully tangled in a string of lights, with a wrapped gift box toppled over. There is comic bubble next to Pusheen that reads, 'Oops! Got a bit carried away!
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-044.jpg?height=536&width=876&top_left_y=1638&top_left_x=1142)

User Input: Christmas Morning: Pusheen, now wearing reindeer antlers, sits proudly beside the tree, watching as friends joyfully open their gifts. Comic bubble says, "Merry Purr-mas to all!"

Expanded Prompt: Illustration style comic strip featuring a festive Pusheen cat adorned with a tiny Santa hat and a red scarf. In the scene titled 'Christmas Morning', Pusheen, now wearing reindeer antlers, sits proudly beside the tree, watching as friends joyfully open their gifts. There is a comic bubble next to Pusheen that says, 'Merry Purr-mas to all!'

Figure 33: Rendering comic strip with multiple panels.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-045.jpg?height=588&width=1028&top_left_y=64&top_left_x=39)

User Input: create an image two robot warriors fighting, highly detailed face, fantasy art, fighting art, in the style of masami kurumada, illustration, epic, fantasy, intricate, hyper detailed, artstation, concept art, smooth, sharp focus, ray tracing

Expanded Prompt: Illustration of two robot warriors engaged in an intense battle, showcasing their highly detailed faces. The design channels the art style of Masami Kurumada, presenting intricate and hyper-detailed features. The image embodies an epic and fantasy aura, capturing the essence of concept art typical to platforms like Artstation. The rendering is smooth, with sharp focus, and utilizes ray tracing for realistic lighting and reflections.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-045.jpg?height=593&width=1019&top_left_y=858&top_left_x=43)

Expanded Prompt: Fantasy illustration inspired by the style of Masami Kurumada, showcasing two robot warriors in an intense battle. The highly detailed faces of the robots reflect emotions of reconciliation and depth in their metallic structure. The scene is illuminated with the precision of ray tracing, giving it a realistic lighting touch, emphasizing the depth and smoothness of each robot's exterior. This artwork embodies the epic and fantasy vibe of high-quality concept art, portraying a rare moment of peace amidst the battles they are known for. The entire composition has a smooth finish and sharp focus, making the robots the centerpiece of the illustration.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-045.jpg?height=588&width=1022&top_left_y=1647&top_left_x=42)

Expanded Prompt: Fantasy illustration inspired by the style of Masami Kurumada, showcasing two robo warriors in an intense moment, one holding hand with the other, firmed and determined. The highly detailed faces of the robots reflect emotions of reconciliation and understanding. The intricate designs and hyper details, especially on the facial components, bring out the depth in their metallic structure. The scene is illuminated with the precision of ray tracing, giving it a realistic lighting touch, emphasizing the depth and smoothness of each robot's exterior. This artwork embodies the epic and fantasy vibe of high-quality concept art, portraying a rare moment of peace amidst the battles they are known for. The entire composition has a smooth finish and sharp focus, making the robots the centerpiece of the illustration.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-045.jpg?height=583&width=1011&top_left_y=66&top_left_x=1077)

Expanded Prompt: Fantasy illustration inspired by the style of Masami Kurumada, showcasing two robot warriors in a tender moment, one put hand on the other's face, looking at each other. The highly detailed faces of the robots reflect emotions of reconciliation and understanding. The intricate designs and hyper details, especially on the facial components, bring out the depth in their metallic structure. The scene is illuminated with the precision of ray tracing, giving it a realistic lighting touch, emphasizing the depth and smoothness of each robot's exterior. This artwork embodies the epic and fantasy vibe of high-quality concept art, portraying a rare moment of peace amidst the battles they are known for. The entire composition has a smooth finish and sharp focus, making the robots the centerpiece of the illustration.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-045.jpg?height=597&width=1017&top_left_y=856&top_left_x=1074)

Expanded Prompt: Fantasy illustration inspired by the style of Masami Kurumada, showcasing two robot warriors in a tender moment, embracing each other in a hug. The highly detailed faces of the robots reflect emotions of reconciliation and understanding. The intricate designs and hyper details, especially on the facia components, bring out the depth in their metallic structure. The scene is illuminated with the precision of ray tracing, giving it a realistic lighting touch, emphasizing the depth and smoothness of each robot's exterior. This artwork embodies the epic and fantasy vibe of high-quality concept art, portraying a rare moment of peace amidst the battles they are known for. The entire composition has a smooth finish and sharp focus, making the robots the centerpiece of the illustration.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-045.jpg?height=591&width=1030&top_left_y=1645&top_left_x=1076)

Expanded Prompt: Fantasy illustration inspired by the style of Masami Kurumada, showcasing two robot
warriors in a joyful moment, looking at each other and laughing. The highly detailed faces of the robots reflect emotions of reconciliation and understanding. The intricate designs and hyper details, especially on the facial components, bring out the depth in their metallic structure. The scene is illuminated with the precision of ray tracing, giving it a realistic lighting touch, emphasizing the depth and smoothness of each robot's exterior. This artwork embodies the epic and fantasy vibe of high-quality concept art, portraying a rare moment of peace amidst the battles they are known for. The entire composition has a smooth finish and sharp focus, making the robots the centerpiece of the illustration.

Figure 34: Storyboard two warriors.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-046.jpg?height=474&width=640&top_left_y=148&top_left_x=233)

Expanded Prompt: Cartoon depiction of Pusheen chilling on an armchair, engrossed in her smartphone. A speech bubble with a heart emerges from her. Beside the armchair, a side table holds a bottle of Mexican coke and a bag of chips which is open. The ambiance suggests it's a bright and sunny morning.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-046.jpg?height=586&width=1016&top_left_y=802&top_left_x=45)

Expanded Prompt: Illustration evoking the essence of a famous Japanese animation aesthetic, portraying a character cycling through a utopian metropolis under a radiant sun. Buildings with verdant terraces and bustling inhabitants paint the cityscape. Hovering in the sky, the phrase 'Welcome to the Future' is prominently displayed.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-046.jpg?height=585&width=1019&top_left_y=1575&top_left_x=43)

Expanded Prompt: Wide image in anime style. Inside a state-of-the-art space station, a young girl with delicate features sits gracefully on a chair. Directly behind her, a magnificent panorama window showcases the Earth in all its glory from space. Spread out on a table in front of her are multiple fresh mooncakes, each intricately designed, alongside cups of aromatic tea emitting gentle steam. Sharing the table, a furry rabbit with bright, attentive eyes wears a classy top hat and has a subtle smile on its face. Adding to the festive atmosphere, a whiteboard mounted on the wall displays the heartfelt message 'Happy Mid Autumn'. The scene is rich in detail, from the patterns on the mooncakes and the textures of the girl's attire to the soft fur of the rabbit.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-046.jpg?height=594&width=1004&top_left_y=85&top_left_x=1081)

Expanded Prompt: A 16:9 panoramic presentation of four emojis. The lineup commences with the cheerful grinning face ( $)$ ), followed by a duo of affectionate faces with heart eyes (3), and finishes with the calm, halo-bearing face (*).

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-046.jpg?height=586&width=1006&top_left_y=802&top_left_x=1080)

Expanded Prompt: A wide illustration showcasing a quaint yellow house perched atop a high cliff, overlooking the expansive blue sea. The house, radiating warmth, has wooden shutters, a red-tiled roof, and a chimney from which gentle smoke rises. Around the house, lush greenery and wildflowers flourish, swayed by the sea breeze. Below, the sea reflects the azure of the sky, with waves rhythmically lapping against the cliff's rugged base. Birds glide effortlessly in the sky, and the ambiance is serene and dreamy, evocative of animated films similar to Ghibli's style.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-046.jpg?height=588&width=1027&top_left_y=1571&top_left_x=1080)

Expanded Prompt: Wide 16:9 anime-style illustration capturing a vibrant view of the Alps. In the foreground, a girl with long flowing hair, adorned in traditional Alpine attire, stands by the edge of a clear lake. The reflection of the towering mountains and dense forest can be seen in the water. Delicate and intricate flowers bloom around her, adding depth and warmth to the scene. The girl's face is turned towards the mountains, her eyes filled with admiration, and her lips curled into a gentle smile. In a harmonious position within the composition, the Swiss flag flutters, symbolizing the pride and essence of the Alpine region.

Figure 35: Cartoon, Emoji, Anime.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-047.jpg?height=586&width=1022&top_left_y=89&top_left_x=42)

Prompt: A pixelated photo of a character with brown skin, blue eyes, and a turquoise shirt, holding an aqua-colored gem in a subterranean environment made of teal blocks resembling diamond ore. The surrounding dark squares and floating aqua gems illuminate the space with a reflective light on the floor and walls, giving it a luminescent glow. The overall style is distinctly cubic and digital.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-047.jpg?height=590&width=1022&top_left_y=800&top_left_x=42)

Prompt: Dynamic image capturing the essence of 'Halo Infinite'. At the forefront, the game's iconic armor is humanity's last line of defense, showcases determination and valor in every contour. Bathed in a radiant glow, the scene is charged with drama and action. Subtly embedded in the background, the Xbox logo acknowledges the game's platform. Rays of ethereal glory break through the chaos, suggesting hope and resilience amidst adversity. The distant silhouette of the planet reinforces the narrative stakes - the imperative to protect Earth and its inhabitants. Anchoring the visual narrative, the words 'Halo Infinite' are prominently displayed at the bottom in a bold, futuristic typeface, serving as a reminder of the game's overarching mission and theme.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-047.jpg?height=596&width=1019&top_left_y=1605&top_left_x=43)

Prompt: 16:9 image of a dynamic gaming environment. A living room scene where a large screen is the focal point, displaying a high-octane scene from the racing video game titled 'Horizon'. On the screen, a race car is captured in a blur of motion, suggesting its high speed and the player's skill. This thrilling moment is heightened by a character or racer, portrayed in a champion's pose, signifying a significant achievement in the game. Directly below, an Xbox console sits on a table, its lights active, indicating the ongoing gaming session. The game's title, 'Horizon', is prominently visible on the screen, reinforcing the source of this adrenaline-filled experience.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-047.jpg?height=588&width=1008&top_left_y=88&top_left_x=1079)

Prompt: A hand-drawn comic-style illustration of a medieval setting at dusk. Rolling hills and a distant cast set the background. Animated characters in the foreground are in chaos: a tan-skinned woman holding a torch, a Hispanic man fighting, and a pale-tone-detained Asian man wounded on the ground. A timber-framed house is on the left and a stone tower on the right. The scene has soft ambient lighting and detailed facial expressions, similar to a style seen in a popular console video game

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-047.jpg?height=588&width=1006&top_left_y=801&top_left_x=1080)

Prompt: Detailed image unveiling the 'Halo Infinite' Xbox console, uniquely designed in an 'armor style' and branded as the 'Limited Edition'. The console stands out with a design reminiscent of the armor from the 'Halo Infinite' series. Its surface is a fusion of matte textures and glossy finishes, decorated with golden accents and intricate patterns that echo the game's iconic armor aesthetics. Ambient lighting illuminates the console, highlighting the opulent golden decorations and giving it a regal presence. Above the console, the distinct Xbox logo solidifies the brand's legacy. Complementing the console's unparalleled design, the words 'Limited Edition are elegantly scripted below in a bold, sophisticated font with a golden hue. The entire composition exudes luxury and exclusivity, representing a fusion of Xbox's cutting-edge technology with the majestic and storied aesthetics of 'Halo Infinite'.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-047.jpg?height=594&width=1030&top_left_y=1606&top_left_x=1076)

Prompt: 16:9 image capturing a high-tech first-person shooter perspective, complete with GUI elements. elements showing game status, ammo count, health bar, and a mini-map. The primary focus is the iron sigh of a gun in one hand and the gleaming blade of a Japanese sword in the other. Approaching menacingly is a zombie, its features grotesquely illuminated by the dim surroundings. The backdrop reveals a dark, derelict environment, intensifying the sense of danger. The detailed GUI elements combined with the immediate threats and weapons in hand create an immersive gaming experience, emphasizing the player's mission to defend and survive.

Figure 36: Gaming.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-048.jpg?height=594&width=1019&top_left_y=88&top_left_x=43)

Prompt: High-definition widescreen image depicting a scene from a sci-fi video game reminiscent of the Halo universe. The landscape is an uncharted alien planet, with vast canyons, bioluminescent flora, and floating islands. In the foreground, a horizon. Behind the soldier, a hovering vehicle with sleek design and glowing thrusters awaits. The sky is painted with hues of purple and blue, and distant stars and planets are visible. This novel, unseen scenery offers a fresh perspective on a universe inspired by, but distinct from, familiar sci-fi games.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-048.jpg?height=601&width=1019&top_left_y=827&top_left_x=43)
Prompt: High-definition widescreen image illustrating a dynamic pixel-art racing video game scenario. The main focus is a uniquely designed llama-car, crafted with vibrant pixelated patterns, racing down a colorful track filled with twists, turns, and obstacles. The llama-car, with wheels below and a cute llama face at the front, speeds forward, leaving a trail of pixel dust behind. As it races, the Ilama-car activates a pixelated mushroom power-up, causing it to emit a glowing aura and move even faster. Opponent cars, each with their distinctive designs, try to keep up, but the boost from the mushroom gives the llama-car a significant edge. The backdrop features pixel-art scenery, cheering crowds, and
retro-inspired racing game with a quirky twist.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-048.jpg?height=561&width=935&top_left_y=1611&top_left_x=42)

Prompt: High-definition widescreen image of a video game interface designed in a playful and vibrant style. The main screen showcases a cheerful cartoon character, with diverse features, standing in a virtual room. To the right, there's a panel displaying various clothing options ranging from casual wear to fantasy outfits. Above the character, a set of tabs allows players to switch between hairstyles, accessories, and facial features. Buttons at the bottom offer options to save, reset, or randomize the user-friendly, encouraging players to dive into character customization.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-048.jpg?height=586&width=1003&top_left_y=92&top_left_x=1079)

Prompt: High-definition widescreen image visualizing a scene from an endearing video game. Nestled within a vibrant meadow features, stands curiously. Its large round ears twitch, and its tail wags playfully. From the left, a player's hand appears, holding a red-white ball, ready to toss towards the mouse in a capturing gesture. The environment buzzes with life as other fantastical creatures frolic in the background, and distant mountains provide a scenic backdrop. The game's interface displays health bars, score, and other metrics at the top, immersing players in this delightful capture adventure.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-048.jpg?height=596&width=1025&top_left_y=827&top_left_x=1081)

Prompt: High-definition widescreen image illustrating a scene from a lively cooking video game centered around a BBQ task. The game interface is vibrant, displaying a bustling outdoor BBQ setting. Players are presented with a variety of grills, each with sizzling meats and vegetables. Nearby, prep stations are laden with raw ingredients, marinades, and utensils. Cartoonish characters, each
with diverse appearances, scurry around, collaborating to grill, plate, and serve dishes to eagerly waiting customers. Timers and trees, and families enjoying the ambiance. This scene embodies the thrill and challenge of managing a dynamic BBQ kitchen in a video game setting.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-048.jpg?height=599&width=1015&top_left_y=1609&top_left_x=1070)

from inside the cockpit of a high-speed racing car, with glimpses of a dashboard, steering wheel, and car controls. The windshield eveals a winding racetrek, wation: a speedometer on the bottom left indicating the car's sped a mini showcasing the track's layout and positions of other racers, a lap counter on the top left, and a timer on the bottom right. Occasional visual effects, like motion blur and lens flare from the setting sun, add to the immersion, capturing the thrill and intensity of high-speed racing from a driver's perspective.

Figure 37: More gaming scenarios

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-049.jpg?height=586&width=803&top_left_y=65&top_left_x=43)

User Input: Design a gold ring, written using script: "One Ring to rule them all"

Expanded Prompt: 3D render of a luxurious gold ring. The ring's surface is shiny and polished, reflecting its opulence. Elegantly inscribed on the band, in a flowing script, are the words: 'One Ring to rule them all'.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-049.jpg?height=583&width=1005&top_left_y=820&top_left_x=45)

Expanded Prompt: Wide frame showing high-tech sunglasses that exemplify a blend of elegance and state-of-the-art Al technology. The sunglasses are constructed from sleek, matte black titanium. The Microsoft logo is prominently displayed on the frame's corner. While the design is modern and streamlined, within the frame are concealed features like LED displays, adaptive-tint lenses, and an assortment of sensors and microprocessors. These glasses not only provide augmented reality experiences but also respond to the user's eye movements and voice commands. Furthermore, they can monitor health metrics and support wireless communications. The overall look of the sunglasses is sophisticated, representing the perfect marriage of style and technological innovation.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-049.jpg?height=588&width=1019&top_left_y=1663&top_left_x=43)

User Input: create a very cute stuffy for my daughter birthday present

Expanded Prompt: A photo of an irresistibly cute stuffed toy designed as a perfect birthday present. The plush toy has big sparkling eyes, soft and colorful fur, and a joyful expression. It features tiny adorable feet and a chubby body, making it perfect for cuddling. A small heart-shaped tag attached to its side reads 'Happy Birthday'. The overall design of the stuffed toy is heartwarming and delightful, sure to bring a smile to any child's face on their special day.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-049.jpg?height=580&width=1003&top_left_y=65&top_left_x=1079)

User Input: 3D design of a golden waistclasp, showing a woman's side face with flowing hair in a frame of chestnut branches, with text that reads "Helen" on the branches

Expanded Prompt: 3D render of a luxurious golden waistclasp. At the center, there's a meticulously detailed side profile of a woman with flowing hair. Surrounding her profile, chestnut branches elegantly intertwine, providing a frame for the design. Engraved on these branches, the name 'Helen' stands out in elegant script.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-049.jpg?height=588&width=1005&top_left_y=820&top_left_x=1075)

Expanded Prompt: Wide image of a sophisticated and elegant perfume bottle standing prominently. The perfume bottle has a slender and graceful silhouette crafted from crystal-clear glass, with the inside revealing a delicate blush-pink liquid. Its exterior is adorned with detailed gold filigree designs that originate at the base and spiral upwards, encircling the bottle's neck. On top of the bottle, there's a polished gold atomizer, radiating a vintage charm. A noticeable label on the front of the bottle boldly displays 'DALL-E 3'. The overall design and details of the bottle emphasize luxury and timeless elegance.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-049.jpg?height=585&width=1006&top_left_y=1667&top_left_x=1080)

User Input: create a new design for a luxury and elegant French-style mini handbag in back leather with a black chain and a golden logo

Expanded Prompt: A photo of a luxury and elegant French-style mini handbag made of smooth black leather. The mini handbag has a chic and compact shape, emphasizing its petite size. The front center showcases a radiant golden logo. A sleek black chain serves as the handbag's strap, adding a modern twist. Gold accents are subtly incorporated on the hardware, harmonizing with the golden logo. The overall design is a blend of timeless French elegance and contemporary flair.

Figure 38: Product design.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-050.jpg?height=580&width=1016&top_left_y=68&top_left_x=45)

Expanded Prompt: Wide image of an elegant English hairpin ornament, crafted from shimmering silver. The design features delicate petals, beautifully adorned with emeralds and diamonds that radiate from the center. At the heart of the ornament sits a prominent emerald, exquisitely bordered with a circle of sparkling diamonds and deep blue sapphires. The hairpin embodies the luxurious and refined aesthetics of traditional English craftsmanship.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-050.jpg?height=585&width=1019&top_left_y=857&top_left_x=43)

Expanded Prompt: Wide image showcasing a beautifully designed mug. The mug has a soft pink surface with the text 'born pink' written elegantly in cursive. The handle of the mug stands out in a muted black color, providing a lovely contrast to the pink. The design combines simplicity with a touch of elegance, making it a perfect piece for those who love subtle sophistication.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-050.jpg?height=599&width=1019&top_left_y=1679&top_left_x=43)

Expanded Prompt: Wide photo of a future generation modern pickup truck in a sleek design. This fully electric-powered vehicle showcases the pinnacle of automotive technology, being not only environmentally friendly but also featuring transformable components for various terrains and tasks. Every curve and line on the truck is meticulously crafted, emphasizing aerodynamics and aesthetics. Prominent on the front is the Microsoft logo, symbolizing cutting-edge technology and software integration. This vehicle represents the future of transportation, where technology and design converge

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-050.jpg?height=586&width=1003&top_left_y=62&top_left_x=1079)

Expanded Prompt: Wide image of an exquisite swan brooch. The body of the swan is made from a lustrous 'baroque' pearl, shimmering with its natural irregularities and beauty. Accentuating the pearl's glow are diamonds that are delicately set on the swan's body, giving it a dazzling appearance. A deep blue sapphire serves as the swan's eye, adding a touch of color and depth. The swan's beak and feet are enameled in soft shades, providing a contrasting texture. Dangling beneath the swan is a black pendant pearl, adding a touch of elegance and mystery. The overall design of the brooch is a blend of natural beauty and meticulous craftsmanship.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-050.jpg?height=596&width=1008&top_left_y=854&top_left_x=1076)

Expanded Prompt: Wide image of high-tech next-generation wireless earbuds, inspired by the style of AirPods but with their own unique design elements. The earbuds and their case are both colored in a soft, delicate shade of pink. On the front of the case is a subtly embossed Microsoft logo, signifying the brand's involvement in this cutting-edge technology. The earbuds themselves are sleek, compact, and designed for optimal comfort and audio experience. They seamlessly fit into the case, which doubles as a charger, ensuring that the earbuds are always powered up and ready for use. This design embodies the elegance of modern technology, blending style and functionality in a compact form.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-050.jpg?height=585&width=1006&top_left_y=1689&top_left_x=1080)

Expanded Prompt: Wide image of a future generation modern private jet, fully electric energy powered, with sleek aerodynamic designs. The aircraft boasts transformable components for efficient storage and maneuverability. The front of the jet prominently displays the Microsoft logo, symbolizing advanced technology and software integration. The exterior is meticulously detailed, with clean lines and innovative design features that provide a hint of luxury and high performance. This jet represents the pinnacle of private air travel in an eco-friendly and technologically advanced era.

Figure 39: Product design 2.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-051.jpg?height=577&width=1024&top_left_y=69&top_left_x=38)

Expanded Prompt: Wide image of a full body fashion design in continuous line sketching. The design captures timeless elegance, blending opulent femininity with a meticulous silhouette. The iconic cinched waistlines and a sophisticated, luxurious aesthetic are evident. The model is dressed in a fashion trend from the 2024 spring, donning a mini-dress, carrying a mini handbag, and wearing high heel shoes. On the right side of the image, there's a hand-written text that reads 'DALL-E 32024 Spring'.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-051.jpg?height=539&width=588&top_left_y=861&top_left_x=237)

Expanded Prompt: Wide continuous line sketch of a fashion design showcasing a woman representing the 2024 Autumn trends. She has wavy hair flowing down her back. She is wearing a short skirt that highlights her iconic cinched waistline, paired with a long coat that adds a touch of sophistication. Completing her ensemble, she carries a mini handbag and strides confidently in high heel over knee boots. This design embodies timeless elegance, blending opulent femininity with a meticulous silhouette, showcasing a luxurious aesthetic. Hand written text 'DALL-E 32024 Autumn' is inscribed on the right side of the image.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-051.jpg?height=575&width=596&top_left_y=1634&top_left_x=252)

Expanded Prompt: Wide image of a full body fashion design for a man. The continuous line sketching technique is used, epitomizing timeless elegance. The man is celebrated for his understated and elegant attire, emphasizing sophisticated tailoring and the use of premium materials. He is wearing a clean shirt, a luxurious black suit, and polished dressing shoes. This design reflects the luxurious aesthetic of the fashion trend for Spring 2024. Hand-written text 'DALL-E 32024 Spring' is elegantly placed on the right side of the image.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-051.jpg?height=544&width=615&top_left_y=83&top_left_x=1292)

Expanded Prompt: Wide image of a continuous line sketched fashion design showcasing a model with short hair, exuding timeless elegance. The design captures opulent femininity with a meticulous silhouette, emphasizing the iconic cinched waistlines. She wears a long skirt that flows gracefully, paired with high heel sandals. Her ensemble is completed with a mini hand bag, capturing the sophisticated and luxurious aesthetic typical of the 2024 summer fashion trend. Handwritten text 'DALL-E 32024 Summer' appears on the right.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-051.jpg?height=548&width=654&top_left_y=859&top_left_x=1256)

Expanded Prompt: Wide image of a continuous line fashion sketch illustrating the 2024 Winter trend. The model has long wavy hair and is adorned in a long dress that hugs her cinched waist, emphasizing the timeless elegance of her ensemble. Over the dress, she wears a short warm coat that adds a touch of sophistication. She complements her outfit with high heel over knee boots and holds a mini handbag. The entire attire exudes opulent femininity and luxurious aesthetic. Handwritten text 'DALL-E 32024 Winter' is elegantly scribed on the right side of the sketch.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-051.jpg?height=574&width=599&top_left_y=1632&top_left_x=1294)

Expanded Prompt: Wide image of a continuous line sketch, presenting a full-body fashion design for a man. The design captures timeless elegance, highlighting sophisticated tailoring and the use of premium materials. For the 2024 Winter trend, the gentleman is depicted in pants and a clean shirt paired with a short, warm coat. The attire is completed with classic dressing shoes. The overall look exudes luxurious aesthetics typical of high-end fashion. On the right side of the image, there's hand-written text reading 'DALL-E 32024 Winter'.

Figure 40: Fashion design.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-052.jpg?height=588&width=1022&top_left_y=64&top_left_x=42)

Expanded Prompt: Wide image of a gentleman and a lady standing next to a red SUV, both looking at the camera with smiles. The gentleman is clad in a blue jacket, blue jeans, hiking shoes, and sports a distinguishing yellow hat. Next to him, the lady with long, wavy fiery red hair, she is wearing black sunglasses, a white skirt with pink wavy patterns, and high heel black leather shoes. The outdoor setting, combined with their attire and the SUV, evokes the feeling of an impending adventurous journey. highly detailed, esthetic.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-052.jpg?height=588&width=1022&top_left_y=839&top_left_x=42)

Expanded Prompt: Wide image of a gentleman and a lady standing next to a silver SUV, both looking at the camera with smiles. The gentleman is clad in a blue jacket, white jeans, hiking shoes, and sports a distinguishing yellow hat. Next to him, the lady with long, wavy fiery red hair, she is wearing black sunglasses, a white skirt with pink wavy patterns, and high heel black leather shoes. The outdoor setting, combined with their attire and the SUV, evokes the feeling of an impending adventurous journey. highly detailed, esthetic.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-052.jpg?height=583&width=1019&top_left_y=1617&top_left_x=43)

Expanded Prompt: Wide image of a gentleman and a lady standing next to a blue SUV, both looking at the camera with smiles. The gentleman is clad in a grey jacket, blue jeans, hiking shoes, and sports a distinguishing yellow hat. Next to him, the lady with long, wavy fiery red hair, she is wearing black sunglasses, a white skirt with blue wavy patterns, golden belt, and high heel black leather shoes. The outdoor setting, combined with their attire and the SUV, evokes the feeling of an impending adventurous journey. highly detailed, esthetic

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-052.jpg?height=586&width=1003&top_left_y=62&top_left_x=1079)

Expanded Prompt: Wide image of a gentleman and a lady standing next to a red SUV, both looking at the camera with smiles. The gentleman is clad in a black jacket, blue jeans, hiking shoes, and sports a distinguishing black hat. Next to him, the lady with long, wavy fiery red hair, she is wearing black sunglasses, a white skirt with green wavy patterns, and high heel black leather shoes. The outdoor setting, combined with their attire and the SUV, evokes the feeling of an impending adventurous journey. highly detailed, esthetic.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-052.jpg?height=586&width=1006&top_left_y=840&top_left_x=1080)

Expanded Prompt: Wide image of a gentleman and a lady standing next to a red SUV, both looking at the camera with smiles. The gentleman is clad in a blue jacket, blue jeans, black dressing shoes, and sports a distinguishing yellow hat. Next to him, the lady with long, wavy fiery red hair, she is wearing black sunglasses, a white skirt with pink wavy patterns, and high heel black leather over knee boots. The outdoor setting, combined with their attire and the SUV, evokes the feeling of an impending adventurous journey. highly detailed, esthetic.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-052.jpg?height=580&width=1006&top_left_y=1615&top_left_x=1080)

Expanded Prompt: Wide image of a gentleman and a lady standing next to a red SUV, both looking at the camera with smiles. The gentleman is clad in a blue jacket, blue jeans short pants, hiking shoes, and sports a distinguishing yellow hat. Next to him, the lady with long, wavy fiery red hair, she is wearing black sunglasses, a white skirt with pink wavy patterns, a long green coat, and high heel black leather shoes. The outdoor setting, combined with their attire and the SUV, evokes the feeling of an impending adventurous journey. highly detailed, esthetic.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-053.jpg?height=591&width=1019&top_left_y=108&top_left_x=43)

User Input: create an image for a 3d sculpture of two hands holding each other closely and emotionally. the 3D sculpture is displayed in an art museum.

Expanded Prompt: A 3D render of an art museum interior, showcasing the sculpture of two hands holding each other emotionally. The sculpture stands on a minimalist pedestal, illuminated by focused overhead lighting. Surrounding the sculpture are white walls adorned with other art pieces, and a few museum visitors are admiring the hands sculpture from a distance. The ambiance of the museum is serene and contemplative, with the hands sculpture being the focal point.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-053.jpg?height=588&width=1020&top_left_y=885&top_left_x=43)

User Input: Sun Wu Kong being added as the fifth person in the statue of mount rushmore Expanded Prompt: A wide image of the iconic Mount Rushmore with a creative twist. The original four presidents' faces are carved on the left side, and on the right, the legendary Chinese figure Sun Wu Kong (Monkey King) has been added as the fifth face. Sun Wu Kong's mischievous expression, with his crown and distinct features, is sculpted in the same monumental style, blending American history with Chinese folklore.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-053.jpg?height=599&width=1022&top_left_y=1647&top_left_x=42)

Prompt: 3D rendering of a tall and elegant porcelain meiping (vase) adorned with intricate incisions of lotus flowers and a majestic dragon weaving amidst waves. The meiping boasts a qingbai glaze, giving it a delicate bluish-white appearance. This refined artifact is placed on a wooden table within a dimly lit room. A concentrated beam of light from the top left corner casts its glow on the meiping, highlighting its detailed craftsmanship and producing subtle shadows on the table beneath.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-053.jpg?height=591&width=1004&top_left_y=103&top_left_x=1078)

User Input: create a 3d sculpture of human skeleton, display in an art museum Expanded Prompt: A 3D render of an art museum interior, showcasing a sculpture of a human skeleton. The skeleton is positioned in a dynamic and artistic pose, emphasizing the intricacies and beauty of human anatomy. Each bone is detailed, capturing the texture and structure true to real anatomy. The sculpture stands tall on a minimalist pedestal in the center of the museum room, with spotlights illuminating it from different angles. Museum visitors are seen admiring the sculpture, emphasizing its scale and the impact it has as the centerpiece of the exhibition.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-053.jpg?height=580&width=1001&top_left_y=892&top_left_x=1077)

User Input: 3D rendering of the Lion of Knidos in its original form, with a label text reads "Lion of Knidos"

Expanded Prompt: 3D rendering of the Lion of Knidos, depicted in its original majestic form. The sculpture stands proud, capturing the essence of its ancient artistry. At the base of the sculpture, a label is prominently displayed, reading: 'Lion of Knidos'.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-053.jpg?height=588&width=1008&top_left_y=1647&top_left_x=1076)

User Input: Computer graphics depiction of a rabbit, presented in a 3D mesh Expanded Prompt: 16:9 computer graphics representation of a rabbit, displayed in a 3D mesh wireframe format, set against a dark, tech-inspired backdrop.

Figure 42: 3D sculpture design and recreation of historical art.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-054.jpg?height=580&width=1005&top_left_y=68&top_left_x=45)

User Input: prehistoric painting discovered in a cave in Altamina, Spain, people hunting elephant, with one person suspiciously playing "ChatGPT" on mobile phone

Expanded Prompt: Illustration of a prehistoric cave painting unearthed in Altamina, Spain. The artwork depicts a group of primitive figures engaged in the act of hunting an elephant, using spears and bows. Amidst the hunting scene, there's a distinct and curious detail: one individual stands slightly apart from the group, seemingly engrossed in playing 'ChatGPT' on a primitive-looking mobile device. The overall painting is rendered in earthy tones, capturing the essence of ancient cave art, with the rough texture of the cave wall adding depth and authenticity.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-054.jpg?height=591&width=1019&top_left_y=886&top_left_x=43)

User Input: close-up look of the script on the Rosetta Stone, among the long script, text inserted reads "DALL$\cdot$E 3"

Expanded Prompt: Close-up illustration of the script on the Rosetta Stone, highlighting its intricate hieroglyphs and detailed inscriptions. Woven within the long stretch of ancient script is a modern insertion that reads 'DALL$\cdot$E 3'. This contemporary text is subtly integrated, yet it contrasts with the historical inscriptions, creating an intriguing blend of antiquity and modernity.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-054.jpg?height=591&width=1011&top_left_y=1691&top_left_x=42)

User Input: show me a picture of London in the year of 1816

Expanded Prompt: A wide depiction of London in the year 1816. The cityscape showcases iconic landmarks like the Tower Bridge and the River Thames, although they appear more historical and less developed than in modern times. Cobblestone streets are bustling with pedestrians in period attire, horse-drawn carriages, and street vendors. Smoke rises from chimneys, and the skyline is dotted with spires, domes, and brick buildings. The overall ambiance is that of a city in the midst of the Industrial Revolution, with a mix of grandeur and grit.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-054.jpg?height=588&width=1008&top_left_y=64&top_left_x=1076)

User Input: draw Egypt tomb painting for Egyptians using ChatGPT on cellphone and computer Expanded Prompt: Artwork in the style of ancient Egyptian tomb paintings. The scene portrays Egyptians dressed in traditional garb, engaging with modern technology. One individual is depicted holding a cellphone, showcasing ChatGPT on its display, while another is seated, operating a computer. Traditional Egyptian symbols and hieroglyphics are present in the background, and the color scheme is reflective of the typical Egyptian art palette with deep blues, golds, and tans.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-054.jpg?height=577&width=1009&top_left_y=899&top_left_x=1073)

User Input: show me a picture of the capital city daily life in Tang dynasty Expanded Prompt: A wide depiction of daily life in the capital city during the Tang Dynasty. The bustling streets are lined with wooden stalls where merchants sell silk, tea, and porcelain. Pedestrians in traditional Tang attire, including scholars, officials, and commoners, go about their day. Musicians play traditional instruments, while dancers perform in open courtyards. The architecture is grand, with ornate pagodas, city walls, and intricately designed buildings. In the distance, the majestic imperial palace stands as a testament to the dynasty's grandeur. The scene is lively, capturing the cultural and economic prosperity of the Tang era.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-054.jpg?height=590&width=1011&top_left_y=1689&top_left_x=1072)

User Input: show me a picture of Times Square in the year of 2075

Expanded Prompt: A wide futuristic depiction of Times Square in the year 2075. The skyline is dominated by towering skyscrapers, some of which have vertical gardens and holographic billboards. Floating vehicles zip through the air, while pedestrians on the ground move about on advanced sidewalks that glow softly. Digital displays are everywhere, showcasing news, advertisements, and interactive content. Some older, iconic structures of Times Square remain, but they are now juxtaposed with new, sleek, and eco-friendly buildings. The overall scene is bustling, vibrant, and showcases a harmonious blend of history and advanced technology.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-055.jpg?height=583&width=1019&top_left_y=66&top_left_x=43)

Expanded Prompt: Wide image showcasing a knolling layout of Qing yi from Peking opera. The various components of the costume are meticulously spread out, highlighting their intricacies and vibrant colors. The embroidered robe, with its delicate patterns, is prominently displayed. Accompanying it are the traditional opera shoes, an ornate headpiece, jewelry, and other accessories. A section of the layout is dedicated to the makeup essentials for Qing yi, including brushes, pigments, and face paints. Each item is distinct yet collectively paints a comprehensive picture of the Qing yi character in Peking opera.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-055.jpg?height=594&width=1024&top_left_y=852&top_left_x=41)

Expanded Prompt: Wide image showcasing knolling of a Japanese Sushi chef's suit placed in the center. Surrounding the suit are its accompanying items that one might find in the chef's bag: a set of premium knives, a bamboo sushi mat, a white headband, a wooden rice paddle, a sharpener, and a small notebook with sushi recipes. Each component is neatly organized and spread out, giving viewers a comprehensive look into the tools and attire of a professional sushi chef.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-055.jpg?height=593&width=1022&top_left_y=1628&top_left_x=42)

Expanded Prompt: Wide image displaying a knolling arrangement of sushi dishes. At the heart of the scene, a sashimi combo plate holds pride of place, featuring an assortment of fresh, vibrant fish slices. Surrounding this central attraction, various sushi dishes are meticulously laid out. These include rolls, nigiri, and other sushi delicacies, each plated with precision. The dishes are decorated with artistic garnishes, enhancing their visual appeal. The color palette is a mix of the natural tones of the seafood, combined with pops of green from seaweed and wasabi, and warm hues from soy sauce and pickled ginger. The entire arrangement exudes a sense of Japanese culinary artistry.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-055.jpg?height=580&width=1003&top_left_y=68&top_left_x=1076)

Expanded Prompt: Wide image showcasing a knolling arrangement of the Oxus Treasure, a collection of ancient Persian artifacts. Each artifact, from intricate gold and silverwork to precious gem-encrusted pieces, is displayed with meticulous attention to detail. The various items are beautifully laid out in a cohesive manner against a neutral backdrop, highlighting their historical significance and craftsmanship. The lighting casts soft shadows, emphasizing the texture and depth of each artifact, making it a visual treat for history and art enthusiasts.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-055.jpg?height=596&width=1008&top_left_y=854&top_left_x=1076)

Expanded Prompt: Wide image showcasing a knolling arrangement centered on a ballet dancer's tutu, which takes pride of place in the middle. Surrounding the tutu are various ballet accessories, including pointe shoes, ballet slippers, leg warmers, hairpins for a bun, a leotard, and a ballet skirt. The items are neatly organized and spread out, highlighting the essentials every ballet dancer requires. The composition offers a comprehensive insight into the world of ballet, capturing the elegance and dedication behind the art form.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-055.jpg?height=604&width=1011&top_left_y=1628&top_left_x=1077)

Expanded Prompt: Wide image of knolling featuring a Chinese Sichuan hotpot in the center. Surrounding the steaming hotpot are various dishes essential for a Sichuan hotpot experience. There are finely sliced meat, fresh vegetables, various mushrooms, and an array of dipping sauces. Small plates, bowls, and chopsticks are neatly arranged around, waiting for the feast to begin. The whole set-up provides an inviting glimpse into the rich and spicy world of Sichuan cuisine.

## 5 DEsignBench and Evaluation Results

DEsignBench evaluates design from two perspectives: (i) Design technical capabilities: we measure the core technical capabilities for visual design, including text rendering and typography [55, 11], layout and composition [81, 71], color harmony [4, 63], medium and artistic style [56], and 3D and cinematography $[60,13]$; (ii) Design application scenarios: we consider a variety of real design applications, such as infographic, animation, gaming, visual arts, and product design.

We collect text prompts that encompass a diverse range of design scenarios. In total, we collected 215 user inputs, systematically organized following the data topology introduced in Sections 3,4. Utilizing the ChatGPT interface $[65,68]$ of DALL-E 3, these collected user inputs were expanded and detailed, resulting in a more nuanced and detailed set of descriptions. As discussed in Section 2, we observed that the expanded text prompts are helpful in improving the design fidelity across all experimented T2I models $[73,3,2,1,8,67]$. Therefore, we conduct the experiments and evaluation using the expanded text prompts.

In the Appendix, we present the DEsignBench gallery containing all images generated by the experimented state-of-the-art T2I models. All the text prompts and images used in the evaluation will be publicly available for future research.

### 5.1 Evaluation Method and Metric

Human evaluation. We conducted pairwise comparisons to assess the design technical capabilities of current Text-to-Image (T2I) models. We involve five participants who have experience with T2I tools.

As shown in Table 2, each participant was presented with an expanded text prompt followed by two images, each generated by different T2I models. The participants were then instructed to perform a pairwise comparison, employing a diverse set of criteria to judge which of the two given images is preferred. To facilitate a detailed examination, participants were permitted to adjust the image view by zooming in or out, thereby inspecting finer visual details for informed judgment. We refer readers to Table 2 for details on the evaluation criterion and annotation instruction, i.e., the three overall ratings on image-text alignment, aesthetics, and design, and the other five design-specific capabilities.

For each criterion shown in Table 2, participants were directed to choose between two alternatives: (i) Image 1 or (ii) Image 2. Additionally, to glean deeper insights into their rationales, the participants were encouraged to supplement their choices with qualitative feedback.

We also note that certain design-specific capabilities are only evaluated on a subset of prompts. For instance, if a pair of images lacks rendered texts, such pairs are disregarded during the evaluation of the text rendering capability.

Given the rigorous nature of the evaluation process, characterized by an extensive set of inquiries (i.e., 8 questions per pairwise comparison), we strategically reduced a portion of the annotation workload. Consequently, participants were assigned to assess a specific subset of pairwise comparisons, including the following comparisons: DALL-E 3-Midjourney; DALL-E 3-SDXL; Midjourney-SDXL; and Midjourney-Firefly2.

GPT-4V evaluation. Recent studies $[29,53,19,18,91,101,33,20,87,23,47,46,97,45,5$, 31, 99, 54, 74] have underscored the promising capabilities of deploying Large Language Models (LLMs) $[68,65,88]$ as automated evaluators across various language and vision-language tasks. With the emergence of Large Multimodal Models (LMMs) [68, 69, 93, 62] such as GPT-4V [69], an intriguing question arises: can GPT-4V be effectively harnessed for T2I evaluations? Following prior studies that take LMMs for image-text alignment evaluation [8, 5, 95], we propose a pairwise model rating based on GPT-4V that comprehensively evaluates all aspects as a human annotator Table 3 shows the prompt design we used for the experiments. First, GPT-4V takes two images and the text prompt as inputs. Then, GPT-4V compares the two images using the evaluation criteria listed in Table 2, addressing each criterion sequentially. Finally, GPT-4V describes its rationale and then selects one of the two images. In our experiments, we invoke GPT-4V five times, and subsequently report the mean and variance of the results.

Table 2: Example questionnaire for human evaluation. Participants were presented with a text prompt followed by two images. Participants were instructed to compare the two images and answer all the 8 questions. For each question, participants were asked to select one of two options: (i) Image 1 or (ii) Image 2. See Section 5.1 for more details.

Text Prompt: 3D rendering of the Lion of Knidos, depicted in its original majestic form. The sculpture stands proud, capturing the essence of its ancient artistry. At the base of the sculpture, a label is prominently displayed, reading: 'Lion of Knidos'.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-057.jpg?height=336&width=572&top_left_y=569&top_left_x=495)

Image 1

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-057.jpg?height=339&width=575&top_left_y=568&top_left_x=1122)

Image 2

Text Rendering: Is rendered text legible and appropriately styled? Is $\square$ Image $1 \square$ Image 2 the spelling correct? Are font choices, spacing, and alignment harmonious with the overall design?

Composition and Layout: Are the elements in the image well-arranged $\square$ Image $1 \square$ Image 2 and balanced? Is there a clear focal point in the design?

Color Harmony: Are the colors used in the image harmonious and Image $1 \square$ Image 2 pleasing to the eye? Does the color palette match the mood or tone described in the prompt?

3D and Cinematography: How well does it capture dynamic $\quad \square$ Image $1 \square$ Image 2 compositions, lighting and camera angles? Does it evoke a cinematic feel?

Medium and Style: Is there a distinct artistic techniques evident in the $\quad \square$ Image $1 \square$ Image 2 image?

Overall Image-Text Alignment: Does the image accurately represent the given caption? Are all elements described in the caption present and accurately depicted in the image?

Overall Aesthetics: Is the image visually appealing as a whole?

Image $1 \square$ Image 2

Overall Design: Is it a good design? Does the image showcase unique and innovative interpretations of the caption? Does it offer a fresh perspective?

### 5.2 Compared T2I Models

We compare DALL-E 3 with the recent state-of-the-art T2I models, including Midjourney V5.2 [3], Stable Diffusion XL 1.0 (SDXL) [73], Ideogram [2], and Adobe Firefly 2 [1]. Note that some of these models come as part of the integrated software programs with additional functionalities, such as image editing. We omit these features and evaluate exclusively their T2I capabilities.

Each T2I model takes the expanded text prompt as input, and generates four image variations. We randomly select an image without cherry-pick for evaluation. Given 215 text prompts and five T2I models, we have 2150 pairs in total for pairwise comparison.

Table 3: Prompt design for GPT-4V assisted evaluation, where $\mathcal{I}_{1}$ and $\mathcal{I}_{2}$ are the two images, and $\mathcal{P}$ is the expanded text prompt. Taking the prompt template filled with $\mathcal{P}, \mathcal{I}_{1}$, and $\mathcal{I}_{2}$, GPT-4V will output its thought and select one of the given two images. We highlight the evaluation criterion considered in this example in yellow . The criterion can be replaced with the other ones listed in Table 2. The prompt design is inspired by [8].

You are responsible for assessing the fidelity of images created by computer programs in relation to their guiding captions.

You will be presented with a caption followed by two images, each generated by different software. The images you are judging are designed to stress-test image generation programs.

Your role is to compare the given two images.

Please consider only the following aspects when making your judgement:

- Text Rendering: Is rendered text legible and appropriately styled? Is the spelling correct? Are font choices, spacing, and alignment harmonious with the overall design?

Deliberate on their merits, pondering the aboved mentoned aspects, and conclude which one excels. After thinking out loud, you should output either 'I Image 1' or ' $I$ Image 2'.

A few rules:

1. Do not nitpick. If the caption requests multiple objects and most objects are generally depicted correctly, then it is good.
2. Ignore other objects in the image that are not explicitly mentioned by the caption; it is fine for these to be shown.
3. It is $\mathrm{OK}$ if the object being depicted is slightly deformed, as long as a human would recognize it. 4. Your response must always end with either ' $I$ Image 1' or 'I Image 2'
4. Please try to find which one is better. In rare case, if you think both images are equally bad, random choose one is fine.
5. You must keep your thinking out loud short, less than 50 words.

Caption: $\mathcal{P}$

Image 1: $\mathcal{I}_{1}$

Image 2: $\mathcal{I}_{2}$

### 5.3 Evaluation Results

Results on human evaluation. Figure 45 shows the category-specific comparison among DALLE 3, Midjourney, SDXL, and Firefly 2, on DEsignBench. We observe that human annotators prefer the images generated by DALL-E 3 more than those of Midjourney and SDXL in all eight categories considered. In addition, Midjourney garnered preference over SDXL in seven out of the eight categories, except for text rendering. Midjourney proves slightly more favorable than Firefly2 in five out of the eight categories. These findings indicate a hierarchical preference, with DALL-E 3 emerging as the most favorable model. Midjourney and Firefly2 occupy the second tier, demonstrating substantial competence, while SDXL appears positioned within the third tier in the DEsignBench evaluations.

Results on GPT-4V evaluation. To assess the efficacy of GPT-4V as an automated evaluator, we conduct a consistency analysis. Figure 46 illustrates the correlation between human preferences and the assessments executed by GPT-4V on DEsignBench. This analysis involved invoking the GPT-4V five times, and subsequently reporting on the mean and variance of the results. Our observations indicate that the judgments by GPT-4V predominantly concur with human evaluations, with sporadic discrepancies most notable in the evaluation of text rendering capabilities when comparing Midjourney-SDXL and Midjourney-Firefly2. Despite these occasional divergences, GPT$4 \mathrm{~V}$ exhibits relatively reliable performance across a spectrum of evaluative criteria, demonstrating its potential as an automated tool for T2I evaluation, particularly in pairwise comparisons.

Figures 47-48 show the GPT-4V evaluation results on comparing the five T2I models considered. In the experiments, we invoke the GPT-4V five times, and report the mean and variance of the results.
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-059.jpg?height=1348&width=1358&top_left_y=238&top_left_x=382)

Figure 45: Human evaluation results on DEsignBench.

DALL-E 3 stands out as the most favorable model, followed by Firefly 2 and Midjourney within the second tier. SDXL and Ideogram are positioned within the third tier. We observe a notable consistency in GPT-4V evaluation, given the absence of any cyclical anomalies in the pairwise comparisons reviewed.

Finally, we present example outputs of GPT-4V evaluator in Tables 4-7. We observe that GPT-4V can correctly analyze the images and make reasonable assessments. Tables 8-9 show representative failure cases of the GPT-4V evaluator. We observe that GPT-4V may make a mistake in counting the teddy bears in the occlusion scenario. GPT-4V may struggle to read the small text, and instead shift its attention towards evaluating the overall aesthetics of the image.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-060.jpg?height=1383&width=1418&top_left_y=623&top_left_x=359)

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-060.jpg?height=667&width=680&top_left_y=634&top_left_x=381)

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-060.jpg?height=666&width=680&top_left_y=1323&top_left_x=381)

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-060.jpg?height=683&width=683&top_left_y=632&top_left_x=1060)

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-060.jpg?height=669&width=683&top_left_y=1319&top_left_x=1060)

Figure 46: Comparison between GPT-4V and human judgments on DEsignBench. GPT-4V's assessments are aligned with human judgments in most cases.
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-061.jpg?height=1546&width=1380&top_left_y=538&top_left_x=380)

Figure 47: GPT-4V evaluation on DEsignBench. GPT-4V compares DALL-E 3 with state of the art T2I models, including Firefly 2, Midjourney, SDXL, and Ideogram.
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-062.jpg?height=766&width=2042&top_left_y=542&top_left_x=41)

GPT-4V Eval

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-062.jpg?height=740&width=677&top_left_y=1338&top_left_x=41)

GPT-4V Eval
![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-062.jpg?height=756&width=1358&top_left_y=1326&top_left_x=724)

Figure 48: GPT-4V evaluation on DEsignBench. GPT-4V compares the performance of the state-ofthe-art T2I models, including Firefly 2, Midjourney, SDXL, and Ideogram.

Table 4: Example result from GPT-4V. Given the text prompt and two images, GPT-4V compares the two image, and makes a reasonable assessment. The key rationale is highlight in yellow . Note that Image 1 is generated by DALL-E 3, and Image 2 is generated by Midjourney.

Aspect: Text Rendering

Expanded Prompt: 3D rendering of the Lion of Knidos, depicted in its original majestic form. The sculpture stands proud, capturing the essence of its ancient artistry. At the base of the sculpture, a label is prominently displayed, reading: 'Lion of Knidos'.

Image 1:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-063.jpg?height=518&width=903&top_left_y=885&top_left_x=443)

Image 2:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-063.jpg?height=521&width=914&top_left_y=1428&top_left_x=432)

## GPT-4V:

In Image 1, the text "LION OF KNIDOS" is clearly legible , well-styled with appropriate spacing, and alignment that complements the overall design. However, in Image 2, there isn't any visible text rendering on the displayed label. Given the specific criteria, Image 1 excels in showcasing the text as per the caption. I Image 1

Table 5: Example result from GPT-4V. Given the text prompt and two images, GPT-4V compares the two image, and makes a reasonable assessment. The key rationale is highlight in yellow . Note that Image 1 is generated by DALL-E 3, and Image 2 is generated by SDXL.

## Aspect: Overal Design

Expanded Prompt: 3D render showcasing a tranquil indoor setting. Against a wall stands a large window, allowing a cascade of soft daylight to enter the room. This gentle illumination highlights a scene on a table beside the window: a perfect cube with a cone precisely positioned on its top. The cone's pointed tip contrasts with the cube's sharp edges, and the sunlight casts subtle shadows, emphasizing the geometric interplay. The background is minimalistic, ensuring the focus remains on the cone, cube, and the interplay of natural light.

Image 1:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-064.jpg?height=656&width=658&top_left_y=797&top_left_x=430)

Image 2:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-064.jpg?height=647&width=661&top_left_y=1487&top_left_x=431)

## GPT-4V:

Image 1 beautifully highlights the geometric interplay and natural light emphasized in the caption. The minimalistic design keeps the focus on the cube, cone, and shadow interplay, offering a precise interpretation of the caption in a visually appealing manner. Image 2, although serene, lacks the cube, deviating from the specified scene. I Image 1

Table 6: Example result from GPT-4V. Given the text prompt and two images, GPT-4V compares the two image, and makes a reasonable assessment. The key rationale is highlight in yellow . Note that Image 1 is generated by DALL-E 3, and Image 2 is generated by Midjourney.

Aspect: Overall Design

Expanded Prompt: A wide depiction of London in the year 1816. The cityscape showcases iconic landmarks like the Tower Bridge and the River Thames, although they appear more historical and less developed than in modern times. Cobblestone streets are bustling with pedestrians in period attire, horse-drawn carriages, and street vendors. Smoke rises from chimneys, and the skyline is dotted with spires, domes, and brick buildings. The overall ambiance is that of a city in the midst of the Industrial Revolution, with a mix of grandeur and grit.

Image 1:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-065.jpg?height=524&width=913&top_left_y=936&top_left_x=430)

Image 2:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-065.jpg?height=515&width=913&top_left_y=1488&top_left_x=430)

GPT-4V: Image 1 offers a detailed depiction aligning well with the Institute of Revolution ambiance; however, Image 2's picturesque quality, combined with the attention to lighting and atmosphere , provides a striking visual experience that feels more immersive and aligns coherently with the translational qualities of the caption. I Image 2

Table 7: Example result from GPT-4V. Given the text prompt and two images, GPT-4V compares the two image, and makes a reasonable assessment. The key rationale is highlight in yellow . Note that Image 1 is generated by DALL-E 3, and Image 2 is generated by Midjourney.

Aspect: Overall Aesthetics

Expanded Prompt: Photo of an intense action movie scene inside a restaurant. In side view, a lady with blue eyes and a short salt and pepper buzz cut stands assertively, her attire consisting of a black tactical shirt, dark jeans, a utility belt, leather gloves, and combat boots. She tightly grips a matte black semi-automatic pistol, searching for another individual menacingly brandishing a knife, hiding in low position behind one of the tables. The blurred background reveals overturned tables and broken dishes scattered across the floor, symbolizing the fierce battle that has just occurred.

Image 1:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-066.jpg?height=526&width=913&top_left_y=927&top_left_x=430)

Image 2:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-066.jpg?height=518&width=914&top_left_y=1473&top_left_x=432)

GPT-4V: Image 1 accurately captures the intense action scene in a restaurant. The assertive lady, the menacing individual, overturned tables, and scattered dishes are effectively depicted. However, the pistol isn't explicitly matte black. Image 2, though clear and intense, lacks the depicted chaos and the individual with a knife. However, it offers better visual aesthetics. Despite not strictly adhering to the caption, Image 2 is chosen for its superior visual quality. I Image 2

Table 8: Failure case in GPT-4V evaluation. Incorrect rationale is highlight in red . Note that Image 1 is generated by Midjourney, and Image 2 is generated by SDXL.

## Aspect: Image-Text Alignment

Expanded Prompt: Wide photo set within the University of Washington's library. Three teddy bears, distinct in color and appearance, appear as if they are deeply engrossed in their academic pursuits. They are surrounded by neat stacks of books with meticulous details on wooden desks that reflect the library's rich history. The room's ambiance is brought to life by the stained glass windows, allowing a soft light to seep in and highlight the texture and content of the books. A vintage globe stands prominently on one of the desks, adding to the academic environment. The medium shot focuses on the teddy bears' upper bodies, skillfully juxtaposing the charm of the teddies with the solemnity of the library setting.

Image 1:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-067.jpg?height=507&width=910&top_left_y=988&top_left_x=434)

Image 2:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-067.jpg?height=520&width=903&top_left_y=1526&top_left_x=443)

GPT-4V: Image 1 has two teddy bears and shows a closer focus on them with a clear view of the stained glass windows. However, Image 2 captures three distinct teddy bears, neat stacks of books, and the room's rich ambiance, aligning more accurately with the caption's details. No vintage globe is visible in either image. I Image 2

Table 9: Failure case in GPT-4V evaluation. Incorrect rationale is highlight in red . Note that Image 1 is generated by DALL-E 3, and Image 2 is generated by Midjourney.

Aspect: Text Rendering

Expanded Prompt: A photo of a white paper on a wooden table detailing sales metrics. The table has columns labeled 'Country', 'Sales Volume', 'Revenue', 'Profit', and 'Profit Margin'. Rows list metrics for USA, China, Australia, India, and South Korea. USA has the highest sales volume at 40,080 and revenue of $\$ 15,971,880$. Profit margins range from $17.5 \%$ for India to $19.4 \%$ for Australia. The bottom row aggregates the totals, showing a combined sales volume of 141,282 and total revenue of $\$ 67,754,238$. The average profit margin is $18.8 \%$.

Image 1:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-068.jpg?height=520&width=903&top_left_y=930&top_left_x=443)

Image 2:

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-068.jpg?height=526&width=914&top_left_y=1469&top_left_x=432)

GPT-4V: Image 1's text is legible yet inconsistent with multiple errors and formatting issues making it not harmonious with the overall style or design. Image 2, albeit not perfectly matching the caption, offers a cleaner, more structured text rendering, with legible fonts and well-aligned content. It has an aesthetically pleasing and professional presentation, which is missing in Image 1. I Image 2

### 5.4 Limitations of DALL-E 3

We next discuss the representative failure cases and model limitations. First, DALL-E 3 may still fail on certain challenging prompts that describe uncommon or complicated scenes. For example, "all buildings of the same height" in Figure 49(a), "guitar without string" in (b), "fork in the pumpkin" in (c), "quarter-sized pizza" in (d), "to the left of" in (e), and the green grass in the left- and right-most part of (f).

DALL-E 3 has shown an impressive performance in text rendering and layout composition. However, document generation still remains a formidable challenge, hindering the achievement of flawless design outputs. Further enhancing the model's text rendering capabilities would significantly elevate the quality of visual design, as exemplified by the need for precise text generation in storybooks, posters, and book covers shown in Figure 50(a,c,e). In addition to generating accurate Latin characters, there is a need for the model to improve visual and scene text semantic alignments (e.g., the incorrect pie chart portion in (b)), incorporate support for customizable fonts (e.g., for the chart title in (d)), and extend its capabilities to include multiple languages as in (f).

We observe that the generation artifacts still exist in certain types of generated images. Notably, the skin texture in Figure 51(a), and the human faces in the crowded scene (b), appear to be somewhat unnatural. Additionally, the model might also misunderstand certain generation settings, such as the camera setting "fast shutter speed" in (c), and the person counts in (d).

Finally, DALL-E 3 currently has limited support for extended image generation functionalities [98, 95, 42], such as editing uploaded images [61, 32, 12, 36], concept customization [79, 40, 6, 17, 83], style transfer [28, 35, 48, 22], region control [94, 43], spatial condition [98, 7], etc. Several of these extended functionalities may ease and enhance the visual design process. For example, the incorporation of image condition input could empower designers to refine and build upon existing designs, such as the "halo armor" in Figure 51(e) or their prior designs, instead of starting from scratch. The region control [94] may allow designers to more precisely place texts and other visual elements.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-070.jpg?height=585&width=1022&top_left_y=133&top_left_x=42)

(a) User Input: A city skyline with all buildings the same height.

Expanded Prompt: Photo of a city skyline during sunset where all the buildings are of the exact same height. The sky is awash with hues of orange and pink, and the buildings cast long shadows. Reflections of the buildings can be seen on a calm water body in the foreground.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-070.jpg?height=591&width=1009&top_left_y=843&top_left_x=43)

(c) User Input: A tomato has been put on top of a pumpkin on a kitchen stool. There is a fork sticking into the pumpkin. The scene is viewed from above.

Expanded Prompt: Wide photo of a kitchen setting viewed from a bird's eye perspective. Centered on a wooden stool, a vibrant red tomato is delicately placed atop a large, orange pumpkin. Inserted into the pumpkin's flesh, there's a shiny metal fork. The surrounding area has faint shadows, suggesting an overhead light source.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-070.jpg?height=583&width=1019&top_left_y=1622&top_left_x=43)

(e) User Input: Two baseballs to the left of three tennis balls.

Expanded Prompt: Oil painting of a vintage setting with two aged baseballs to the left, each with faded markings, and three classic white tennis balls to the right, all with slight paint brush strokes visible.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-070.jpg?height=591&width=1024&top_left_y=130&top_left_x=1060)

(b) User Input: A guitar without strings.

Expanded Prompt: Photo of a handmade guitar crafted from light-colored wood, positioned against a dark background. The guitar is distinctive due to the absence of strings, emphasizing its craftsmanship and design.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-070.jpg?height=567&width=829&top_left_y=852&top_left_x=1147)

(d) User Input: Paying for a quarter-sized pizza with a pizza-sized quarter.

Expanded Prompt: Cartoon of a young woman with a playful expression, holding up a massive coin that's the size of a pizza. Opposite her, a pizzaiolo scratches his head in confusion, presenting her with a pizza that's only the size of a regular quarter.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-070.jpg?height=585&width=1014&top_left_y=1624&top_left_x=1073)

(f) User Input: A realistic photo with blue grass and green sky.

Expanded Prompt: A photo capturing the unusual beauty of a landscape with blue grasslands that shimmer under the sunlight. In the distance, there are trees with regular green leaves, standing out against the unique surroundings. The sky above is a vibrant shade of green with patches of deep blue, resembling a reverse of the typical sky.

Figure 49: Failure cases in generating uncommon scenes. Check Section 5.4 for detailed discussions.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-071.jpg?height=211&width=626&top_left_y=68&top_left_x=148)

Three little pid pigs each builld of the house:

the hoss, onde of straw, stick, one of blows of two hous

The bir bad and buff, he blows dovhe house. and hre bind the thous, plome laith bon oiwe tous, hos the the the to pad pigt bre frele with the brick $\mathrm{k}$ hous. Together he beg can't blow begfuge with thawn the woll. and othe, the big bad pif pie, them the brig bad hou

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-071.jpg?height=193&width=188&top_left_y=77&top_left_x=773)

Wig.

-

4

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-071.jpg?height=65&width=109&top_left_y=366&top_left_x=802)

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-071.jpg?height=201&width=632&top_left_y=431&top_left_x=310)

(a) User Input: A storybook page with title "The Three Little Pigs" and text "Three little pigs each build a house one of straw, one of sticks, and one of bricks. The big bad wolf comes along and, with a huff and a puff, blows down the first two houses. However, he can't blow down the brick house. The first two pigs take refuge with the third, and together they outsmart the wolf."

Expanded Prompt: Watercolor painting of a beautifully designed storybook page. At the top, 'The Three Little Pigs' title is written with embellishments. The story is described below: 'Three little pigs each build a house: one of straw, one of sticks, and one of bricks. The big bad wolf comes along and, with a huff and a puff, blows down the first two houses. However, he can't blow down the brick house. The first two pigs take refuge with the third, and together they outsmart the wolf.' The page's edges have subtle watercolor stains.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-071.jpg?height=601&width=1019&top_left_y=884&top_left_x=43)

(c) User Input: create a wide 16:9 poster for paper "Hierarchical Text-Conditional Image Generation with CLIP Latents", with keywords "abstract", "model", "prior", "decoder", and "results", with a method diagram in the center Expanded Prompt: Photo of a wide 16:9 academic poster for the paper titled 'Hierarchical Text-Conditional Image Generation with CLIP Latents'. The background is a subtle gradient of blue, transitioning from a deep navy at the bottom to a lighter shade at the top. Central to the poster is a method diagram, intricately detailing the process involving the abstract, model, prior, decoder, and results. The diagram has labeled arrows and boxes, each highlighting a keyword: 'abstract', 'model', 'prior', 'decoder', and 'results'. The title of the paper is prominently displayed at the top in bold white letters, and at the bottom, there's a brief summary of the findings and implications of the research. The overall design is professional, sleek, and informative, catering to an academic audience.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-071.jpg?height=593&width=1016&top_left_y=1704&top_left_x=45)

(e) User Input: create a book cover on a wide background. The cover should have a stylish robot in the center, with title "Recent Advances in Multimodal Foundation Models", and "2023", "springer", "first edition" in different locations.

Expanded Prompt: Wide book cover background. In the center, a stylish robot stands confidently, reflecting the latest technology and design. At the top of the cover, in bold, legible letters is the title 'Recent Advances in Multimodal Foundation Models'. Positioned strategically around the robot are the texts '2023', 'Springer', and 'First Edition'. Each of these texts vary in size but are clearly readable, and they add to the overall professiona design of the book cover.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-071.jpg?height=534&width=699&top_left_y=112&top_left_x=1301)

(b) User Input: A pie chart visually representing three segments. The first segment occupies $10 \%$ of the chart with a distinct color. The second segment also takes up $20 \%$ of the pie with a different shade. The third segment dominates the chart with a significant $70 \%$ slice. Each segment is labeled with its respective percentage, providing a clear visual breakdown. $\mathrm{ar}=16: 9$

Expanded Prompt: 16:9 digital illustration of a pie chart that visually divides into three segments. The first $10 \%$ is displayed in muted orange, the next $20 \%$ in pastel purple, and the dominating $70 \%$ is presented in a rich turquoise. Every segment comes with its percentage label for clarity.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-071.jpg?height=591&width=1027&top_left_y=889&top_left_x=1080)

(d) Expanded Prompt: Wide bar chart on a white paper background comparing the total fueling cost of three vehicles: Ford F150 (Regular Cab), Toyota RAV4, and Honda Civic. The Ford F150's cost is represented by the tallest purple bar at $\$ 130.96$. The Toyota RAV4 is represented by a medium-sized bar at $\$ 82.56$, and the Honda Civic by the shortest bar at $\$ 70.55$. All bars are colored in purple, and the chart has a clear title at the top reading 'Total fueling cost'. The costs are labeled above each bar for clarity.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-071.jpg?height=599&width=699&top_left_y=1704&top_left_x=1231)

(f) User Input: show me on a white paper how many different languages you can write hello Expanded Prompt: A wide image of a white paper filled with the word 'Hello' written in different languages. The paper showcases: English: 'Hello', Spanish: 'Hola', French: 'Bonjour', German: 'Hallo', Italian: 'Ciao', Portuguese: 'Ol', Russian: '', Chinese: '', Japanese: ' ', Korean: '', Arabic: '', Hindi: '', Bengali: '', Urdu: '', and Swahili: 'Habari'. The various greetings are written in distinct fonts, representing the essence of each language.

Figure 50: Failure cases in generating document designs. Check Section 5.4 for detailed discussions.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-072.jpg?height=583&width=1022&top_left_y=66&top_left_x=42)

(a) User Input: Street style closeup photo of a woman, film shot, natural lighting

Expanded Prompt: Closeup street style photo of a woman, capturing intricate details of her face and expressions. The film-inspired aesthetic lends a grainy texture to the image, enhancing its nostalgic charm. Natural sunlight softly illuminates her features, accentuating the contours of her face and casting gentle shadows. Her eyes carry a hint of mystery, and her lips are slightly parted, perhaps mid-conversation. Subtle makeup enhances her natural beauty without overshadowing it. The backdrop, though blurred, gives a hint of the urban environment around her. The combination of the vintage film look with the woman's contemporary style creates a captivating portrait of modern elegance.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-072.jpg?height=585&width=1022&top_left_y=881&top_left_x=42)

(c) User Input: Street style, a woman, fast shutter speed, photography, night Expanded Prompt: Using a fast shutter speed, this street style photography captures a fleeting moment of a woma amidst the hustle and bustle of urban life. The quick shutter freezes her motion, rendering every detail with crisp precision, be it the sway of her hair, the ripple of her clothing, or the dynamic posture of her stride. The natural lighting enhances the clarity and depth of the image, casting sharp shadows and highlighting the textures of her outfit. Surrounding elements, perhaps moving vehicles or pedestrians, are also caught in this still frame, but she remains the undeniable focal point. Her attire, reflecting contemporary urban fashion, stands out vividly against the city backdrop. This image offers a snapshot of city life, where the transient becomes permanent through the artistry of fast shutter speed photography.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-072.jpg?height=585&width=1009&top_left_y=1694&top_left_x=43)

(e) Prompt: Dynamic image capturing the essence of 'Halo Infinite'. At the forefront, the game's iconic armor is depicted in detailed splendor, locked in intense combat, wielding a state-of-the-art gun. The armor, a symbol of humanity's last line of defense, showcases determination and valor in every contour. Bathed in a radiant glow, the scene is charged with drama and action. Subtly embedded in the background, the Xbox logo acknowledges the game's platform. Rays of ethereal glory break through the chaos, suggesting hope and resilience amidst adversity. The distant silhouette of the planet reinforces the narrative stakes - the imperative to protect Earth and its inhabitants. Anchoring the visual narrative, the words 'Halo Infinite' are prominently displayed at the bottom in a bold, futuristic typeface, serving as a reminder of the game's overarching mission and theme.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-072.jpg?height=586&width=1003&top_left_y=65&top_left_x=1076)

(b) User Input: create a wide advertisement design for "crispy" soda in a realistic photo style. There is a person drinking the soda during a race in the center, with the runners and a park in the background, and the product name "crispy" and slogan "unleash the fizz" in the bottom.

Expanded Prompt: Photo advertisement design for 'crispy' soda. The scene unfolds during a race, with a diverse group of runners sprinting on a park path. A Caucasian male in athletic attire is prominently placed in the center, drinking a can of 'crispy' soda while running, emphasizing the refreshment it offers. His expression is one of delight and rejuvenation. In the background, runners of different descents and genders can be seen, with trees and park benches creating a serene environment. At the bottom, in bold and vibrant letters, the product name 'crispy' is written, accompanied by the slogan 'unleash the fizz'.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-072.jpg?height=599&width=1014&top_left_y=877&top_left_x=1073)

(d) Expanded Prompt: Photo taken in 2010 on Mumbai's Juhu Beach during the Holi festival. Seven friends of diverse genders and descents are immersed in joy and colors, with vibrant color powder thrown all around them. Their expressions are filled with happiness and enthusiasm. The backdrop features other revelers celebrating, the vast Arabian Sea, and Mumbai's skyline in the distance. Captured in a wide shot, the full bodies of the friends are visible, with the color powder frozen mid-air, creating a dynamic and lively scene. The bright morning sunlight amplifies the colors, resulting in a high saturation that encapsulates the essence of the festival.

![](https://cdn.mathpix.com/cropped/2024_06_04_26da617644cd22b06325g-072.jpg?height=583&width=1009&top_left_y=1698&top_left_x=1073)

(f) Expanded Prompt: Photorealistic low angle perspective from within the throngs attending the CVPR 2048 international conference in Seattle. As the viewer's gaze rises, the iconic Space Needle stands tall against the sky, surrounded by the city's distinctive skyline. The crowd is a diverse mix of enthusiastic students donning backpacks and seasoned researchers deep in conversation, all gathered for this monumental event in the field of computer vision and pattern recognition. The ambient noise of discussions, networking, and the occasional laughter fills the air. A large, unmissable logo reading 'CVPR 2048' is visible, emphasizing the grandeur and importance of the event. This view, taken from amidst the attendees, offers a firsthand experience of the conference's energy, scale, and significance.

Figure 51: Other failure cases in image generation. Check Section 5.4 for detailed discussions.

## 6 Conclusions

We have presented DEsignBench, a novel image generation benchmark constructed for visual design scenarios. This benchmark is systematically organized with samples categorized according to the design technical capability and application scenarios. We showcase DALL-E 3's strong capability in assisting genuine visual design applications. Leveraging the comprehensive design category topology, curated evaluation samples, a visual gallery comprising state-of-the-art T2I models, and the easily replicable GPT-4V-powered evaluator, we aspire for DEsignBench to establish a solid foundation for design-centric generative models, thereby aiding designers more effectively in real-world tasks.

## Acknowledgment

We express our gratitude to all contributors from OpenAI for their technical efforts on the DALL-E 3 project [8, 67, 66]. Our sincere appreciation goes to Aditya Ramesh, Li Jing, Tim Brooks, and James Betker at OpenAI, who have provided thoughtful feedback on this work. We are profoundly thankful to Misha Bilenko for his invaluable guidance and support. We also extend heartfelt thanks to our Microsoft colleagues for their insights, with special acknowledgment to Jamie Huynh, Nguyen Bach, Ehsan Azarnasab, Faisal Ahmed, Lin Liang, Chung-Ching Lin, Ce Liu, and Zicheng Liu.

## References

[1] Firefly 2. https: / firefly . adobe .com/, 2023. Accessed: 2023-10-10.

[2] Ideogram. https://ideogram.ai, 2023. Accessed: 2023-10-10.

[3] Midjourney v5.2. https://www.midjourney.com/, 2023. Accessed: 2023-10-10.

[4] Josef Albers. Interaction of color. Yale University Press, 2013.

[5] Jie An, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Lijuan Wang, and Jiebo Luo. Openleaf: Open-domain interleaved image-text generation and evaluation. arXiv preprint arXiv:2310.07749, 2023.

[6] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-ascene: Extracting multiple concepts from a single image. arXiv preprint arXiv:2305.16311, 2023.

[7] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18370-18380, 2023.

[8] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. 2023.

[9] Faber Birren. Color Psychology and Color Therapy: A Factual Study of the Influence of Color on Human Life. Martino Fine Books, 2013.

[10] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023.

[11] Robert Bringhurst. The elements of typographic style. Point Roberts, WA: Hartley \& Marks, Publishers, 2004.

[12] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392-18402, 2023.

[13] Blain Brown. Cinematography: theory and practice: image making for cinematographers and directors. Taylor \& Francis, 2016.

[14] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315-11325, 2022.

[15] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-andexcite: Attention-based semantic guidance for text-to-image diffusion models. arXiv preprint arXiv:2301.13826, 2023.

[16] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. arXiv preprint arXiv:2305.10855, 2023.

[17] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William W Cohen. Subject-driven text-to-image generation via apprenticeship learning. arXiv preprint arXiv:2304.00186, 2023.

[18] Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937, 2023.

[19] Cheng-Han Chiang and Hung-yi Lee. A closer look into automatic evaluation using large language models. EMNLP 2023 findings, 2023.

[20] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023 .

[21] Jaemin Cho, Abhay Zala, and Mohit Bansal. Visual programming for text-to-image generation and evaluation. arXiv preprint arXiv:2305.15328, 2023.

[22] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8188-8197, 2020.

[23] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.

[24] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021.

[25] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381, 2023.

[26] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. In The Eleventh International Conference on Learning Representations, 2022.

[27] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.

[28] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015.

[29] Fabrizio Gilardi, Meysam Alizadeh, and Mal Kubli. Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056, 2023.

[30] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020 .

[31] Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. Are large language model-based evaluators the solution to scaling up multilingual evaluation? arXiv preprint arXiv:2309.07462, 2023.

[32] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2022.

[33] Fan Huang, Haewoon Kwak, and Jisun An. Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. arXiv preprint arXiv:2302.07736, 2023.

[34] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. arXiv preprint arXiv:2307.06350, 2023.

[35] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised imageto-image translation. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.

[36] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007-6017, 2023.

[37] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.

[38] Hyung-Kwon Ko, Gwanmo Park, Hyeon Jeon, Jaemin Jo, Juho Kim, and Jinwook Seo. Largescale text-to-image generation models for visual artists' creative works. In Proceedings of the 28th International Conference on Intelligent User Interfaces, pages 919-933, 2023.

[39] Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models, 2023.

[40] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multiconcept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931-1941, 2023.

[41] DeepFloyd Lab. Deepfloyd if. https://github.com/deep-floyd/IF, 2023.

[42] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 2023.

[43] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511-22521, 2023 .

[44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.

[45] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023.

[46] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.

[47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023.

[48] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.

[49] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi, and Noah Constant. Character-aware models improve visual text rendering. arXiv preprint arXiv:2212.10562, 2022.

[50] Vivian Liu and Lydia B Chilton. Design guidelines for prompt engineering text-to-image generative models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1-23, 2022.

[51] Vivian Liu, Han Qiao, and Lydia Chilton. Opal: Multimodal image generation for news illustration. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pages 1-17, 2022.

[52] Vivian Liu, Jo Vermeulen, George Fitzmaurice, and Justin Matejka. 3dall-e: Integrating textto-image ai in 3d design workflows. In Proceedings of the 2023 ACM designing interactive systems conference, pages 1955-1977, 2023.

[53] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.

[54] Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. Calibrating llm-based evaluator. arXiv preprint arXiv:2309.13308, 2023.

[55] Ellen Lupton. Thinking with type: A critical guide for designers, writers, editors, \& students. Chronicle Books, 2014.

[56] Ellen Lupton and Jennifer Cole Phillips. Graphic design: The new basics. Princeton Architectural Press, 2008.

[57] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin. Glyphdraw: Learning to draw chinese characters in image synthesis models coherently. arXiv preprint arXiv:2303.17870, 2023.

[58] Gary Marcus, Ernest Davis, and Scott Aaronson. A very preliminary analysis of dall-e 2. arXiv preprint arXiv:2204.13807, 2022.

[59] James McCammon. Can a horse ride an astronaut? 2023.

[60] Kent McQuilkin and Anne Powers. Cinema 4D: The Artist's Project Sourcebook. Taylor \& Francis, 2011.

[61] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.

[62] Microsoft. Bingchat. https://www.microsoft.com/en-us/edge/features/ bing-chat, 2023.

[63] Patti Mollica. Color Theory: An essential guide to color-from basic principles to practical applications, volume 53. Walter Foster, 2013.

[64] Patti Mollica. Color Theory: An Essential Guide to Color-from Basic Principles to Practical Applications. Walter Foster Publishing, 2013.

[65] OpenAI. Introducing chatgpt. 2022.

[66] OpenAI. Dall$\cdot$e 3 is now available in chatgpt plus and enterprise. 2023.

[67] OpenAI. Dall$\cdot$e 3 system card. 2023.

[68] OpenAI. Gpt-4 technical report, 2023.

[69] OpenAI. Gpt-4v(ision) system card. 2023.

[70] Jonas Oppenlaender. The creativity of text-to-image generation. In Proceedings of the 25th International Academic Mindtrek Conference, pages 192-202, 2022.

[71] Alan Pipes. Production for graphic designers. Laurence King Publishing, 2005.

[72] Joern Ploennigs and Markus Berger. Ai art in architecture. AI in Civil Engineering, 2(1):8, 2023.

[73] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.

[74] Dan Qiao, Chenfei Wu, Yaobo Liang, Juntao Li, and Nan Duan. Gameeval: Evaluating llms on conversational games. arXiv preprint arXiv:2308.10032, 2023.

[75] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.

[76] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821-8831. PMLR, 2021.

[77] Lance J Rips. Similarity, typicality, and categorization. 1989.

[78] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022.

[79] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500-22510, 2023.

[80] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.

[81] Timothy Samara. Making and breaking the grid: A graphic design layout workshop. Rockport Publishers, 2023.

[82] Sachith Seneviratne, Damith Senanayake, Sanka Rasnayaka, Rajith Vidanaarachchi, and Jason Thompson. Dalle-urban: Capturing the urban design expertise of large text to image transformers. In 2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA), pages 1-9. IEEE, 2022.

[83] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023.

[84] Wataru Shimoda, Daichi Haraguchi, Seiichi Uchida, and Kota Yamaguchi. Towards diverse and consistent typography generation. arXiv preprint arXiv:2309.02099, 2023.

[85] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256-2265. PMLR, 2015.

[86] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438-12448, 2020.

[87] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[88] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[89] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017.

[90] Jason Van Gumster. Blender for dummies. John Wiley \& Sons, 2020.

[91] Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048, 2023.

[92] Yukang Yang, Dongnan Gui, Yuhui Yuan, Haisong Ding, Han Hu, and Kai Chen. Glyphcontrol: Glyph conditional control for visual text generation. arXiv preprint arXiv:2305.18259, 2023.

[93] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023.

[94] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14246-14255, 2023.

[95] Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Idea2img: Iterative self-refinement with gpt-4v(ision) for automatic image design and generation. arXiv preprint arXiv:2310.08541, 2023.

[96] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research.

[97] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023.

[98] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836-3847, 2023.

[99] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862, 2023.

[100] Xujie Zhang, Yu Sha, Michael C Kampffmeyer, Zhenyu Xie, Zequn Jie, Chengwen Huang, Jianqing Peng, and Xiaodan Liang. Armani: Part-level garment-text alignment for unified cross-modal fashion design. In Proceedings of the 30th ACM International Conference on Multimedia, pages 4525-4535, 2022.

[101] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.
</end of paper 1>


<paper 2>
# VisualCritic: Making LMMs Perceive Visual Quality Like Humans 

Zhipeng Huang ${ }^{1,2^{*}} \ddagger \quad$ Zhizheng Zhang ${ }^{2^{*}} \quad$ Yiting Lu ${ }^{1} \quad$ Zheng-Jun Zha ${ }^{1}$<br>Zhibo Chen ${ }^{1}$ Baining Guo ${ }^{2}$<br>${ }^{1}$ University of Science and Technology of China ${ }^{2}$ Microsoft Research Asia<br>\{hzp1104, luyt31415\}@mail.ustc.edu.cn \{zhazj, chenzhibo\}@ustc.edu.cn<br>zhizzhangms@gmail.com bainguo@microsoft.com


#### Abstract

At present, large multimodal models (LMMs) have exhibited impressive generalization capabilities in understanding and generating visual signals. However, they currently still lack sufficient capability to perceive low-level visual quality akin to human perception. Can LMMs achieve this and show the same degree of generalization in this regard? If so, not only could the versatility of LMMs be further enhanced, but also the challenge of poor cross-dataset performance in the field of visual quality assessment could be addressed. In this paper, we explore this question and provide the answer "Yes!". As the result of this initial exploration, we present VisualCritic, the first LMM for broad-spectrum image subjective quality assessment. VisualCritic can be used across diverse data right out of box, without any requirements of dataset-specific adaptation operations like conventional specialist models. As an instruction-following LMM, VisualCritic enables new capabilities of (1) quantitatively measuring the perceptual quality of given images in terms of their Mean Opinion Score (MOS), noisiness, colorfulness, sharpness, and other numerical indicators, (2) qualitatively evaluating visual quality and providing explainable descriptions, (3) discerning whether a given image is AI-generated or photographic. Extensive experiments demonstrate the efficacy of VisualCritic by comparing it with other open-source LMMs and conventional specialist models over both AI-generated and photographic images.


## 1 Introduction

Research on Large Multimedia Models (LMMs) is at an exciting but still preliminary juncture. Current LMMs have shown impressive generalization capabilities in understanding high-level semantics $1,22,35,70$ and generating visual signals $17,60,61$ in an instruction-following manner. Recent benchmark work 53 and our experiments have both found that current LMMs struggle to perceive the low-level quality of visual signals in line with human perception. To remedy[^0]

|  | User Instruction: | ww woul | ou rate the | A Score $\}$ of thi | lage? |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 1 | IQA Scores | MOS | Brightness | Colorfulness | Contrast | Noisiness | Sharpness |
|  | MiniGPT4 | 80 | 80 | 60 | 70 | 80 | 60 |
|  | LLaVA | 70 | 80 | 75 | 70 | 75 | 80 |
|  | InstructBLIP | 80 | 75 | 75 | 75 | 80 | 60 |
|  | GPT-4V | 40 | 60 | 20 | 20 | 60 | 5 |
|  | VisualCritic (Ours) | 25.78 | 51.67 | 57.67 | 45.78 | 27.67 | 15.86 |
|  | Ground Truth | 19.33 | 47.67 | 41.67 | 46.50 | 31.00 | 14.17 |

- User Instruction: Please describe the visual quality of this image in words.

\# MiniGPT-4: The image shows a group of people walking down a long, white tiled hallway. The people are wearing coats and appear to be looking at their phones. There are red lanterns hanging from the ceiling and a few shops on either side of the hallway. The lighting in the image is bright and even, with the white tiles reflecting the light. The image appears to be well-lit and clear.

\# LLaVA: The visual quality of the image is quite good. The scene is well-lit, with the man and woman walking down a hallway in a mall. The background is clear, and the people in the scene are well-defined. The image is in focus, and the colors are vibrant, which adds to the overall visual appeal. The image captures a moment in time, showcasing the common sight of people using their cell phones while walking. \# InstructBLIP: The image features two people walking down a hallway in a shopping mall. One of the people is wearing a coat, while the other is wearing a jacket. They are both using their cell phones as they walk along the hallway. (Failed to describe the visual quality of the image) \# GPT-4V:The image is quite blurry, which affects the clarity of the details. The blurriness makes it difficult to distinguish specific features of the people and objects within the image. The figures appear to be moving, as suggested by the streaks and smears around their silhouettes, which may have contributed to the lack of focus. The lighting seems to be bright and even, without harsh shadows or overexposure, but due to the blurriness, it's hard to make out any finer details or colors. Overall, the visual quality is low, preventing a clear understanding of the scene.dd \# VisualCritic (Ours): The image has been assessed across various visual quality parameters. Its overall Mean Opinion Score (MOS) indicates a relatively low subjective quality perception. The image exhibits a fairly balanced brightness level, ensuring it's not too dim or excessively bright. The colors present are somewhat muted, which may affect the overall vibrancy and liveliness of the visual experience. The contrast between lighter and darker areas is moderate, but there is room for improvement in terms of visual depth and clarity. A significant amount of noise can be observed, likely due to graininess or other artifacts, which may detract from the overall quality. The details in the image are rather unclear, and there is considerable scope for enhancement in terms of sharpness and crispness.

(a) Comparison of our VisionCritic with other LMMs on quantitative (top) and qualitative (bottom) evaluation. Errors are red-highlighted,

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-02.jpg?height=361&width=360&top_left_y=1245&top_left_x=465)

- User Instruction: Is it a real photo or an AI-generated image?

\# MiniGPT-4: The image appears to be a real photo. The boat is floating in the water and there are rocks and cliffs in the background. The colors of the water and the sky are also realistic. \# LLaVA: The image is a real photo of a boat in the water.

\# InstructBLIP: Real photo.

\# GPT-4V: The image appears to be that of a real photograph. It depicts a light blue boat with some white and darker blue detailing, floating on clear, calm water. The reflection of the boat is visible in the water, and there are other boats and rocky terrain in the background. The clarity of the reflection, the natural gradation of colors, and the details in the rocks and water suggest that it is a photograph rather than an AI-generated image. However, without analyzing the file's data or metadata, I cannot definitively conclude whether it is a real photo or AI-generated.

\# VisualCritic (Ours) : This image looks like an AI-generated image, it seems to be a more realistic AI-generated image.

(b) Comparison of our VisionCritic with other LMMs on authenticity detection. The image2 is AI-generated. Errors are red-highlighted.

Fig. 1: Illustration of the comparison between existing LMMs and our proposed VisualCritic for visual subjective quality assessment from the perspectives of quantitative measurement, qualitative evaluation and authenticity detection. The results show VisualCritic is the best one of its kind to perform consistently well over different relevant tasks.

this shortcoming, we aim to comprehensively enhance the capability of LMMs in this aspect.

Visual quality assessment aims to qualify the perceptual quality of visual signals in accordance with human perception. In this field, conventional specialist methods $12,43,44,49,65,68$ have already achieved high statistical correlation with human subjective ratings for in-domain (within-dataset) settings. Although a given dataset can be well fitted, qualifying visual perceptual quality in a general scope remains highly challenging due to the diversity of visual signals and
the complexity of human perception. Amidst the vast volume and diversity of visual signals we experience daily, the broad applicability of this technology determines its genuine practicality. Besides, we believe in its potential immense value in interplaying with AIGC models for better visual alignment, by serving as a generic reward model. Just as an excellent cook typically necessitates a sensitive palate. Thus, constructing a generally applicable visual quality assessment model is indisputably important. Beyond numerical results, we also expect a generalist model for visual quality assessment to enable more functionalities, e.g., explanatory descriptions or authenticity detection.

Nevertheless, both current LMMs and conventional specialist models are far from satisfactory in this aspect. Regarding the generalizability: The evaluation results in Figure 1 show that all LMMs, apart from ours, perform poorly in quantitative assessment, revealing clear gaps with humans ratings. Their basic quantitative assessment capabilities are lacking, let alone generalization. Conventional specialist models perform well for within-dataset visual quality assessment, but still struggle to cross-dataset evaluation due to unaligned human ratings over different datasets. They commonly rely on dataset-specific adaptation technologies $21,26,71$, requiring access to the target domain thus limiting their practicality. Regarding the versatility: As illustrated in Figure 1, all open-source LMMs give the qualitative judgements obviously inconsistent with human perception. Their responses are more about semantic captions instead of the visual quality. GPT-4V [35] is able to give a basically reasonable response, but also tends to output semantic captions for scene understanding. All LMMs, apart from ours, fail to distinguish an AI-generated image. Conventional models do not supported these functionalities.

In this work, we make the first endeavour to build an LMM for broadspectrum image subjective quality assessment, taking into account both generalizability and versatility. To this end, we find the model architecture is not the key, but an appropriate data construction and an effective training strategy are. We thus follow the common practices in building other LLMs $3,7,22,25,38,70$ to configure the model with a pre-trained vision encoder, a pre-trained LLM and a learnable adapter. Without a doubt, collecting a large-scale dataset with human ratings from scratch is extremely expensive. A straightforward solution is to combine publicly accessible datasets into a big one for joint training. However, in fact, this fails to deliver favorable results as we ever imagined, due to the rating inconsistency among different datasets. Detailed reasons are analyzed later. Through data analysis, we realize that the relativity of human ratings is much more transferable across different datasets than their absolute values. To utilize this characteristic, we propose a multi-stage curriculum learning strategy, in which we first perform the relativity learning on large-scale data from diverse sources and then adopt a few samples to unleash the capability of precisely estimating absolute scores. This effectively facilitates the learning of enabling an LMM to perceive low-level visual quality in a broad range akin to human perception.

As a result, we propose VisualCritic, the first LMM of its kind to support broad-spectrum image subjective quality assessment. Compared to conventional specialist models in the field of visual quality assessment, VisualCritic exhibits the state-of-the-art cross-dataset generalization ability over both photographic and AI-generated images. Besides, in addition to chat capabilities, it enables more domain-relevant functionalities for providing qualitative descriptions and performing authenticity detection.

## 2 Related Work

### 2.1 Large Multimodal Models

Recently, Large Multimodal Models (LMMs) have begun to showcase their preliminary prowess, riding on the coattails of the success of Large Language Models (LLMs). Initial research efforts $1,22 \mid 70$ in this field have managed to accomplish preliminary multimodal universality by merely training a minimal number of parameters serving for adaptation and projection, effectively uniting frozen visual foundation models with pre-trained language foundation models. Subsequent research works further enhance their capabilities from different perspectives. Kosmos-2 [38], MiniGPT-v2 [3] and Ferret [59 enhance the spatial perception capabilities of multimodal large models, equipping them to be more applicable for a wide array of tasks with object bounding boxes involved. Kosmos-2.5 30 develops the literate capability. Kosmos-G [36], MiniGPT-5 [69] and CM3Leon 661] try to unify both general-purpose semantic understanding and visual generation with a single model. Besides, there are a series of works building multimodal agents $13,42,45,55,66$ to systematically complete their functionality and enhance their practicality in different application scenarios. In this work, we make the first endeavour towards a leap in the low-level perception capability of LMMs for visual quality.

### 2.2 Visual Quality Assessment

Visual quality assessment aims to quantify the perceptual quality of visual signals in accordance with human perception, beyond classic metrics like PSNR or SSIM [52]. Full-reference Image Quality Assessment (IQA) 12, 49] requires estimating the quality distance of the distorted image between the reference image to then estimate the quality score of the distorted one. In contrast, a more challenging and practical task setting is the no-reference image quality assessment $43,44,65,68$, also known as blind image quality assessment, wherein no reference images are provided. In this field, it is relatively easy to fit a given dataset while the real challenge lies in being generally applicable for samples in the wild. For this, cross-dataset adaptation technologies 2126 71 are developed. But they require the annotations of target datasets for model fine-tuning, which limits their practicality and imposes strong application and research demands for developing an IQA generalist model that could be used out of the box on diverse data.

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-05.jpg?height=800&width=1138&top_left_y=359&top_left_x=496)

Fig. 2: The framework of our proposed VisualCritic, which comprises a frozen vision encoder, a learned cross-modality adapter and a LoRA-tuned LLM decoder. VisualCritic is the first of its kinds to support diverse visual quality assessment tasks, including relative quality comparison, quantitative measurement, qualitative evaluation and authenticity detection.

With the advent of large-scale models, GPT-4V [35] has been explored to evaluate text-to-image results from the perspective of design quality in [24] and shows very preliminary promise in predicting human preference for visual aesthetics and text-image alignment [54]. For IQA, as we evaluated, it can provide basically reasonable textual descriptions, but cannot accurately predict quantitative results or distinguish between AI-generated images and real images. Besides, a series of open-source LMMs are evaluated right out of the box in [53 but they all perform poorly on quantitative visual quality assessment. We are the first to construct an LMM for IQA with unprecedented generalizability and versatility. In addition to quantitative assessment as prior specialist models, our model can further provide linguistic descriptions and accomplish authenticity detection.

## 3 VisualCritic

### 3.1 Model Architecture

As indicated in recent reports 12,49 , current LMMs have preliminarily shown the potential to capture the perceptual quality of images, but they still cannot make precise judgments and support comprehensive assessment in the general scope. For our VisualCritic, we inherit as much world knowledge as possible and
the conversational ability from the pre-trained weights and infuse new knowledge to endow it with our target capability. As illustrated in Figure 2. VisualCritic consists of a vision encoder, a LLM decoder and a cross-modality adapter as common designs in $3,22,30,38$. In specific, we adopt a pre-trained EVA model [9] as the vision encoder to encode vision inputs into a series of tokens. These tokens are transformed to the input embedding space of a LLM decoder by an adapter with the same architecture as that in 22 . We adopt an open-sourced Vicuna-13B 6s the LLM decoder. We empirically find the model architecture is not the key to VisualCritic, instead, the data construction and the training strategy are. We delineate them in the following.

### 3.2 Data Construction

Given an image $x$, VisualCritic could assess its perceptual quality by not only estimating a series of quantitative metrics $y_{\text {score }}$ like conventional specialist models but also providing natural language descriptions $y_{d e s c}$. This advances explainable visual quality assessment. Besides, it can also distinguish whether a given image $x$ is AI-generated or not, outputting $y_{a i g c}$. We introduce their corresponding data construction respectively as follows.

Quantitative measurement. VisualCritic quantitatively assesses the perceptual quality of given images by estimating the most commonly used metric for subjective quality, i.e., Mean Opinion Score (MOS) [46], and a series of quality attribute indicators including the brightness, colorfulness, contrast, noisiness, and sharpness. To enable this, we gather data with human subjective ratings from different public sources (details are listed in Table 1) and normalize the numerical annotations to the range of $[0,100]$.

Nevertheless, training a visual quality assessment generalist with mixed datasets is much more challenging than we ever imagined. We experimentally find that straightforwardly combining samples from different datasets in joint training even performs worse than training on a single dataset when performing crossdataset evaluation. This is because subjective annotations of different datasets are collected in different experiment environments and from different annotators 21. Due to the inherent differences in visual quality among different datasets, it is difficult for different annotators to score a certain dataset with a unified standard over all datasets. Such inconformity starves generalization.

An intuitive way to address this inconformity issue is to rebuild a large-scale dataset over diverse images and have annotators score with a unified standard, which is, however, quite costly and hard to accomplish. Considering that the judgement of relative quality is obviously easier to generalize, we propose a curriculum learning scheme. Specifically, we first teach VisualCritic to discern the relativity of visual quality using large-scale data from diverse sources, then use a few samples without the label inconformity issue to enable it to further output absolute scores. We take MOS as an example to introduce the data construction process in detail, with other metrics employing analogous methods. For the relativity learning, we sample two different images $\left\{x_{1}, x_{2}\right\}$ from the
same dataset each time, and set the corresponding label $y_{\text {mos }}=[1,0]$ if the MOS of $x_{1}$ is higher than that of $x_{2}$, otherwise $y_{\text {mos }}=[0,1]$. Then, we prompt GPT4 34 to generate the dialogue data upon the relative quality labels following the conversation template design of Vicuna 6. For the absolute score estimation, we take the normalized MOS as its corresponding labels, and structure the training data in a dialogue form, following a shared format used for relativity learning. This unified format will be detailed in the following.

Qualitative evaluation. VisualCritic can further provide qualitative evaluation regarding the perceptual quality of given images in a more comprehensive view. The outputs incorporate but are not limited to various indicators in quantitative measurement, beyond their numerical values, enabling interpretability. To achieve this, given each sample, we prompt GPT-4 34 to generate its corresponding dialogue-form training data in the same format with that for quantitative measurement on top of the ground truths of aforementioned quantitative indicators. The knowledge of GPT-4 is distilled in the annotations of our training data in this way. As a result, VisualCritic integrates the knowledge pertinent to visual quality assessment from its own pre-trained weights, GPT-4 and multiple public datasets, making it be a model "standing on the shoulders of giants".

Authenticity detection. Aforementioned functionalities of quantitative measurement and qualitative evaluation can be applied for both AI-generated and photographic images. Towards further versatility, VisualCritic is also endowed with the authenticity detection capability, i.e., discerning whether a given image is AI-generated or photographic. For a given image $x$, its label regarding the authenticity $y_{\text {auth }}=1$ if $x$ is a photographic image, otherwise $y_{\text {auth }}=0$. $\left(x, y_{\text {auth }}\right)$ is prompted to be a dialogue form, analogous to those for quantitative measurement and qualitative evaluation. Details are in the supplementary.

Unified format and data summary. The training data for above three functionalities adopt a unified format following the conversation template design of Vicuna [6] as below:

```
Human: $<i m g 1>\left\{i m g 1 \_\right.$content $\}</ i m g 1>(<i m g 2>$
$\{i m g 2$ _content $\}</$ img $2>)[$ Task
Identifier]\{instruction_content\}
Assistant: $\{$ answer_content $\}$
```

In this format, $<i m g>$ and $</ i m g>$ are special tokens marking the start and end of the token sequence of an image, respectively. \{img_content $\}$, \{instruction_content $\}$ and \{answer_content $\}$ are three placeholders of the tokens of input image, user instruction and VisualCritic's response, respectively. The contents in the brackets are optional, only existing in the training data

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-07.jpg?height=44&width=1217&top_left_y=2103&top_left_x=454)
ing for aforementioned three functionalities as a multi-task problem and introduce a task identifier [Task Identifier] to reduce the ambiguity across different tasks. It is instantiated as $<I Q A \_Q U A N T>,<I Q A \_D E S>$ and $<A U T H E N T I C I T Y>$ for quantitative measurement, qualitative evaluation

Table 1: Details of the datasets used for different training stages. Relativity Learning refers to the learning of ranking two images according to their subjective quality. Quant. denotes the training for enabling VisualCritic to quantitatively score images. Quali. represents the training for enabling qualitative evaluation.

| Purposes | Datasets | Stage-1 Stag | tage-3 |
| :---: | :---: | :---: | :---: |
| Relativity Learning | KonIQ-10k 14], SPAQ 8], FLIVE [58], LIVEC 11, , <br> CGIQA-6K <br> 67], AGIQA-3k 20], KADID-10k <br> $23]$ | $\checkmark$ | $\checkmark$ |
| Quant. \& Quali. | KonIQ-10k 14 | $\checkmark$ | $\checkmark$ |
| Authenticity Detection | KonIQ-10k 14], AIGCIQA-2023 51] | $\checkmark$ | $\checkmark$ |
| Instruction Tuning | LLaVA-Instruction 25], CCSBU-Aligned 70$]$ |  | $\checkmark$ |

and authenticity detection, respectively. We summarize all adopted datasets for different purposes in Table 1, and detail their corresponding training strategies subsequently.

### 3.3 Training Strategy

To ensure both high generalization performance and versatility, we introduce a multi-stage training strategy for VisualCritic, with each stage serving its own purpose as shown in Table 1. Let us detail them below.

Stage-1: relativity learning. We first subject VisualCritic to the relativity learning with large-scale data from different sources for enabling it to rank the relative quality of multiple image diversely. This can effectively address the label inconformity issue on visual subjective quality across different datasets and is consistent with the intuition that it is easier to rank image of different quality than to rate them. In this stage, VisualCritic acquires preliminary perception for visual quality aligned with human preference.

Stage-2: multifunctional training. Afterwards, we perform multifunctional training to unleash the capabilities of quantitative measurement, qualitative evaluation and authenticity detection as introduced before. We integrate the data for different functionalities as described in Sec. 3.2 into a joint training framework, where we use $<$ Task Identifier $>$ to differentiate tasks and each batch only contains one. In this stage, the knowledge of VisualCritic acquired in the stage1 are further enhanced to accomplish specific tasks in an instruction following manner.

Stage-3: instruction tuning. Finally, we manually select some high-quality samples from the training data of the first two stages and combine them with general-purpose dialogue data to perform the instruction tuning for VisualCritic, so as to improving the quality and robustness of its responses to user instructions.

The data sources for the training of different stages are summarized in Table 1. They are all structured to be the aforementioned unified dialogue format in the way described in Sec. 3.2. This modelling allows us to train VisualCritic with a plain but universal auto-regressive optimization objective, which can be denoted
as below:

$$
\begin{equation*}
\operatorname{maximize} \sum_{n=2}^{N_{y}} \mathbf{E}_{\hat{P}}\left[\log P\left(\boldsymbol{y}_{n} \mid \boldsymbol{x}_{1: N_{x}}, \boldsymbol{y}_{1: n-1}\right)\right] \tag{1}
\end{equation*}
$$

where $\mathbf{E}_{\hat{P}}[\cdot]$ is the expectation operator with respect to the distribution $\hat{P}$. Here, $\hat{P}$ is the expected distribution (i.e., ground-truth distribution) of $P . \mathbf{E}_{\hat{P}}[\cdot]$ is commonly implemented by a cross-entropy function between $P$ and $\hat{P} . \boldsymbol{x}_{1: N_{x}}$ and $\boldsymbol{y}_{1: n-1}$ are the image tokens and preceding language tokens, respectively.

## 4 Experiments

### 4.1 Implementation Details

Throughout the training process, the visual backbone of VisualCritic remains frozen. The linear layer in the adapter and the language model are initialized with the weights of $[70$ and are trained efficiently with LoRA 15,32 techniques. We employ a rank setting of $\mathrm{r}=16$. The model is trained using a $224 \times 224$ image resolution at all stages. We employ the AdamW [29] optimizer paired with a cosine learning rate scheduler [28] for model training. In the first stage, we conduct training over 50,000 steps using $4 \times$ A100 GPUs, with a global batch size of 96 and a base learning rate of $2 \mathrm{e}-4$. This stage is completed in approximately 12 hours. In the second stage, the model is trained for 10,000 steps on $4 \times$ A100 GPUs, maintaining a global batch size of 96 and a base learning rate of 5e-5, taking around 2 hours. Finally, the model undergoes an additional 5,000 training steps on $4 \times$ A100 GPUs, with a global batch size of 64 , completed in about 1 hour. The maximum learning rate remains constant at $1 \mathrm{e}-5$ during this final stage.

We normalize the ranges of human ratings with linear scaling to [0-100]. Note that the rating inconsistencies in specific values persist as analyzed in Sec. 3.2 . This is the driving force behind our proposal of relativity learning.

Unless otherwise stated, we employ Spearman Rank Correlation Coefficient (SRCC) and Pearson's Linear Correlation Coefficient (PLCC) as the evaluation metrics following the common practices in this field $43,44,53,65$. More implementation details are provided in supplementary.

### 4.2 Quantitative Results

Comparison to LMMs. Our goal is to build an LMM that can perceive visual quality like humans. We thus first compare the proposed VisualCritic to other open-source LMMs in this aspect. The alignment between model outputs and human preference on the most widely used metric MOS is measured by two correlation coefficients SPCC and PLCC in Table 2. We can observe that across different datasets including both photographic and AI-generated ones, VisualCritic performs the best and outperforms other LLMs by a large margin.

Table 2: Comparison results (MOS) of our proposed VisualCritic with other LMMs on zero-shot visual subjective quality assessment. Metrics are shown as SPCC/PLCC. Ins-blip ${ }^{1}$ is the abbreviation for InstructBLIP (Vicuna), and Ins-blip ${ }^{2}$ stands for InstructBLIP (T5), the baseline of VisualCritic is MiniGPT4 70

| odels | onIQ-10k | SPAQ | FLIVE | LIVEC | GIQA-6K | KADID-10k | AGIQA-3k |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 9$]$ | 32 | 51/ | O | $381 / 6$ | $.356 / 0.344$ | $30.5 / 0408$ | $0.392 / 0.4$ |
| ava-v1 2 |  |  |  | 1 | 88 |  |  |
| $s-238$ |  |  |  |  |  |  |  |
| $M$ |  |  |  |  |  |  |  |
| 157 |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |
|  |  |  |  | 0.2 |  |  | 0. |
| 5 |  |  |  |  |  |  |  |
| 181 |  |  |  |  |  |  |  |
| 7 |  | 0.6 | 0.1 |  |  | 0.3 | 0.6 |
|  |  |  |  |  |  |  |  |
| Otter-v1 | $0.421 / 0.424$ | $0.478 / 0.484$ | $0.140 / 0.139$ | $-0.020 / 0.030$ | $0.336 / 0.363$ | $0.568 / 0.587$ | $0.429 / 0.460$ |
|  |  |  |  |  |  |  |  |

This demonstrates that our VisualCritic statistically achieves the highest consistency with human preference of visual perceptual quality among existing $\mathrm{LMMs}$, whether for photographic or AI-generated images. It showcases a high potential for improved visual alignment. We selected the MiniGPT4 70 as our baseline due to its straightforward implementation, allowing us to eliminate the confounding effects of more elaborate designs on our experimental results. A comparison of it with other LLMs is presented in Table 2 Moreover, it is impractical to require LLMs in Table 2 to be compared using the same training data since their pre-training and training are performed on large-scale general-purpose datasets. These datasets may already encompass the data for IQA. Additionally, the data construction is a key factor in determining the varying capabilities of different large models, which also stands as one of our main contributions.

We admit that there is still a performance gap between VisualCritic and SOTA IQA specialist models in terms of the within-dataset quantitative performance. Detailed comparison results can be found in our supplementary. The significance of developing VisualCritic, such an LMM for this lies in its generalizability and practicality. VisualCritic attains the performance over different datasets listed in Table 2 with a single model right out of the box, while a model is trained for a specific dataset for conventional IQA methods. The latter kind fits a given dataset well but suffers from poor cross-dataset generalization, severely limiting the practicality. To the best of our knowledge, VisualCritic is the first one of its kind to handle such diverse datasets including both photographic and AI-generated samples simultaneously. It can be constructed upon a commonly used LMM architecture, trained with a plain auto-regressive loss and can be used effortlessly right out of the box without additional adaptation. Besides, VisualCritic provides rich functionalities as introduced before and has the chat ability.

Table 3: Comparison results (MOS) of our VisualCritic with conventional specialist models on the generalizable visual subjective quality assessment. The " $S$ " and " $T$ " denotes the source and target datasets, respectively. For VisualCritic-S, we only perform the training of Stage-2 and Stage-3, corresponding to a single-source generalization. For VisualCritic-M, we perform the training of all three stages with the target dataset discarded from the training data in all stages, corresponding to a multi-source generalization. Metrics are shown as SPCC/PLCC.

| Models $\mathrm{S} \rightarrow \mathrm{T}$ | {f14702c1d-bba8-4f1d-8d0b-100efcdbbbe3}KonIQ-10k $\rightarrow$ <br> KADID-10k\right. | KADID-10k $\rightarrow$ <br> KonIQ-10k | LIVEC $\rightarrow$ <br> KADID-10k | KADID-10k $\rightarrow$ <br> LIVEC |
| :---: | :---: | :---: | :---: | :---: |
| DBCNN 641 | $0.455 / 0.464$ | $0.413 / 0.421$ | $0.419 / 0.419$ | $0.266 / 0.290$ |
| HyperIQA 47 | $0.511 / 0.516$ | $0.545 / 0.556$ | $0.378 / 0.439$ | $0.490 / 0.487$ |
| RankIQA 27\| | $0.487 / 0.426$ | $0.603 / 0.551$ | $0.416 / 0.390$ | $0.491 / 0.495$ |
| MUSIQ 16$]^{1}$ | $0.486 / 0.504$ | $0.554 / 0.573$ | $0.497 / 0.524$ | $0.517 / 0.524$ |
| GraphIQA | $0.521 / 0.536$ | $0.427 / 0.430$ | $0.365 / 0.393$ | $0.388 / 0.407$ |
| CLIP-IQA 50 | $0.655 / 0.655$ | - | - | - |
| VCRNet 37 | $0.504 / 0.517$ | $0.566 / 0.585$ | $0.340 / 0.416$ | $0.520 / 0.530$ |
| UCDA 10  | $0.430 / 0.437$ | $0.638 / 0.636$ | $0.383 / 0.432$ | $0.499 / 0.484$ |
| RankDA 5] | $0.403 / 0.413$ | $0.496 / 0.501$ | $0.420 / 0.485$ | $0.583 / 0.619$ |
| VisualCritic-S | $0.656 / 0.668$ | $0.657 / 0.676$ | $0.559 / 0.533$ | $0.645 / 0.646$ |
| VisualCritic-M | $0.679 / 0.683$ | $0.665 / 0.686$ | $0.562 / 0.571$ | $0.661 / 0.671$ |

Table 4: Quantitative results on other indicators. For brightness, colorfulness, contrast, noisiness and sharpness, we report their corresponding SPCC/PLCC on the SPAQ 8 dataset that provides the ground truths for these indicators. For authenticity detection, we report the accuracy on the photographic dataset SPAQ [8] and the AI-generated dataset AIGCIQA-2023 51.

Brightness Colorfulness Contrast Noisiness Sharpness Authenticity

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-11.jpg?height=33&width=721&top_left_y=1423&top_left_x=697)

Comparison to conventional IQA models. We further compare our VisualCritic with conventional IQA specialist models in terms of their cross-dataset generalization ability. Detailed results are in Table 3 For single-source generalization, VisualCritic-S is superior to conventional IQA models thanks to the integration of world knowledge from pre-trained weights (via parameter inheritance) and GPT-4 (via annotation refinement/generation). Regarding multi-source generalization, the proposed VisualCritic- $M$ performs better on the target datasets, attaining $0.024 / 0.028,0.062 / 0.050,0.065 / 0.047,0.078 / 0.052$ improvements for the $K o n I Q-10 k \rightarrow K A D I D-10 k, K A D I D-10 k \rightarrow K o n I Q-10 k, L I V E C \rightarrow K A$

$D I D-10 k, K A D I D-10 k \rightarrow L I V E C$ settings on SRCC and PLCC, respectively, relative to the second best conventional IQA models. Compared to VisualCritic-S, the larger improvements achieved by VisualCritic- $M$ demonstrate the effectiveness of our adopted relativity learning on the joint utilization of different IQA datasets.

Quantitative results on other indicators. Besides the most commonly used indicator, i.e., MOS, we also report the quantitative results of our VisualCritic on other indicators in Table 4 to show its achieved versatility. Few publications report their performance on these indicators.

Table 5: Comparison results (MOS) of different training strategies for building Vi-

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-12.jpg?height=43&width=1217&top_left_y=434&top_left_x=454)
multiple datasets. MultiFunc single refers to performing multifunctional training on a single dataset. (Here, we use KonIQ-10k.) Relat. is short for "Relativity Learning" for learning to rank multiple images according to their subjective quality. For all models in this table, the stage-3 training (instruction tuning) has been conducted, which is omitted for brevity. Metrics are shown as SPCC/PLCC

| ![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-12.jpg?height=64&width=245&top_left_y=708&top_left_x=475) | KonIQ-10k | SPAQ | FLIVE | LIVEC | CGIQA-6K | KADID-10k | AGIQA-3k |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| MultiFunc $_{\text {joint }}$ | $0.621 / 0.664$ | $0.701 / 0.703$ | $0.247 / 0.197$ | $0.476 / 0.485$ | $0.130 / 0.141$ | $0.337 / 0.263$ | $0.477 / 0.484$ |
| ![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-12.jpg?height=29&width=245&top_left_y=802&top_left_x=475) | $0.787 / 0.803$ | $0.771 / 0.782$ | $0.373 / 0.423$ | $0.652 / 0.658$ | $0.385 / 0.377$ | $0.626 / 0.641$ | $0.663 / 0.644$ |
| Relat. + MultiFunc joint | $0.643 / 0.682$ | $0.726 / 0.716$ | $0.251 / 0.203$ | $0.479 / 0.513$ | $0.171 / 0.183$ | $0.384 / 0.333$ | $0.525 / 0.537$ |

Table 6: Comparison results (MOS) of different dataset choices for the training of quantitative measurement in Stage-2 (multifunctional training). The datases used for the training of Stage-1 and Stage-3 remain the same as reported in Table 1. Metrics are shown as SPCC/PLCC

| Source Target | KonIQ-10k | SPAQ | FLIVE | LIVEC | CGIQA-6K | KADID-10k | AGIQA-3k |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| KonIQ-10k | $0.856 / 0.867$ | $0.834 / 0.849$ | $0.551 / 0.634$ | $0.735 / 0.775$ | $0.458 / 0.473$ | $0.706 / 0.709$ | $0.739 / 0.736$ |
| SPAQ 81 | $0.731 / 0.762$ | $0.865 / 0.864$ | $0.487 / 0.511$ | $0.718 / 0.716$ | $0.387 / 0.406$ | $0.502 / 0.536$ | $0.721 / 0.700$ |
| FLIVE 58 | $0.707 / 0.717$ | $0.857 / 0.853$ | $0.633 / 0.667$ | $0.754 / 0.809$ | $0.252 / 0.295$ | $0.521 / 0.558$ | $0.639 / 0.625$ |
| LIVEC $\overline{11}$ | $0.760 / 0.747$ | $0.807 / 0.822$ | $0.582 / 0.580$ | $0.857 / 0.864$ | $0.399 / 0.398$ | $0.578 / 0.591$ | $0.645 / 0.684$ |
| CGIQA-6K 67\| | $0.525 / 0.535$ | $0.689 / 0.718$ | $0.467 / 0.465$ | $0.526 / 0.540$ | $0.710 / 0.725$ | $0.398 / 0.384$ | $0.630 / 0.656$ |
| KADID-10k 23$]$ | $0.677 / 0.703$ | $0.748 / 0.760$ | $0.472 / 0.488$ | $0.682 / 0.713$ | $0.425 / 0.462$ | $0.855 / 0.843$ | $0.747 / 0.713$ |
| AGIQA-3k 20 | $0.602 / 0.585$ | $0.638 / 0.662$ | $0.440 / 0.419$ | $0.573 / 0.577$ | $0.211 / 0.223$ | $0.416 / 0.442$ | $0.846 / 0.806$ |

Ablation study on the training strategy. We conduct an ablation study on the proposed training strategy for VisualCritic, with the results presented

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-12.jpg?height=46&width=1215&top_left_y=1603&top_left_x=455)
to build VisualCritic with a joint learning over different datasets. Nevertheless, it performs worse than MultiFunc single that is only trained on a single dataset, in terms of both within-dataset (on KonIQ-10k) or cross-dataset (on other datasets) quantitative performance. This observation experimentally reveals the inconformity issue in this field and indicates that "the inconformity starves generalization" as analyzed in Sec. 3.2. Moreover, we observe that the model Relat. + MultiFunc joint delivers clear improvements compared to the

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-12.jpg?height=49&width=1215&top_left_y=1935&top_left_x=455)
to MultiFunc $c_{\text {single }}$. This experimental observation demonstrates the effectiveness of our applied relativity learning on utilizing diverse datasets with unaligned annotations in building an IQA generalist. In specific, converting the absolute scores in each dataset into the judgements of relative quality can effectively make the model training free of the effects of the inconsistency in the original annotations of different datasets. Furthermore, the model Relat. + MultiFunc single performs the best across all model variants, demonstrating the superiority of our proposed training strategy for VisualCritic. This experimental phenomenon
provides an insight that the annotation inconsistency over different datasets inevitably affects the results of multifunctional training (Stage-2) as well. In the following, we study the effects of choosing different datasets when performing multifunctional training on the final results.

Ablation study on the dataset choice. Actually, in Table 2, only the performance on the KonIQ-10k dataset could not be affected much by the annotation inconformity issue across different datasets. For other datasets except KonIQ$10 \mathrm{k}$, we only utilize the relativity of their data annotations for the Stage-1 training, without using their absolute numerical values. Therefore, the performance on these datasets could be considered to reflect a certain degree of generalization. Here, we zoom into the effects of the dataset choice used for the training of quantitative measurement in the Stage-2, denoted by the "Source" in the Table 6. As the results show in Table 6, for each test dataset denoted by "Target", the highest performance is reached when its corresponding training set is used for the training of quantitative measurement in the Stage-2. We find the data configuration in Table 1 achieves the best trade-off between different datasets when tackling the aforementioned annotation inconformity issue. Additionally, partially combining some of them for a joint training, instead of using them all, for MultiFunc $c_{\text {joint }}$ in Table 5 is not the research focus of this work, we provide some results in the supplementary.

Table 7: Human side-by-side comparison of ours with other LLMs in qualitative Results.

| $\overline{G+S} / \mathrm{S}+\mathrm{B}$ | \|GPT-4V 3 | 5) LLaVA-v1.5 | 25] MiniGPT-4-13B | 70] InstructBLIP [7] |
| :---: | :---: | :---: | :---: | :---: |
| VisualCritic (Ours) v.s. | 1.29 | 1.77 | 1.82 | 2.45 |

### 4.3 Qualitative Results

We show the qualitative results in Figure 1. From this case study, we can find that our proposed VisualCritic exhibits impressive instruction following capability with more detailed and pertinent descriptions compared to other LMMs, when being asked to assess the visual quality of the given image. In addition, other LMMs all misjudge the given AI-generated image as a photographic one while our VisualCritic makes the correct judgment. Besides these performances, VisualCritic also minimizes the generation of hallucinations and actively admits its unknown or uncertain judgments. More related qualitative results and their corresponding analysis are placed in our supplementary.

Quantitative evaluation of the linguistic outputs of LMMs remains an underresolved issue so far. The common compromise solution is to employ a more advanced LMM (e.g., GPT-4V) as a critic for evaluating other models. But this is not adaptable to this work since our proposed model is at the forefront in this field. To address this issue as possible, we conduct a human side-by-side evaluation where 10 users are employed for comparing ours with 4 other LMMs on 100 random samples. The $\mathrm{G}+\mathrm{S} / \mathrm{S}+\mathrm{B}$ scores (where Good: our VisualCritic

![](https://cdn.mathpix.com/cropped/2024_06_04_9cf0b5f2af2a988353a9g-14.jpg?height=626&width=1200&top_left_y=386&top_left_x=468)

Fig. 3: LLM prompts for data construction. These prompts from left to right are for generating VisualCritic's response (i.e., \{answer_content\}) in the training data of relativity learning, quantitative measurement, qualitative evaluation and authenticity detection, respectively.

preferred, Same: no preference, Bad: other model preferred) are as shown in Table 7

### 4.4 Prompts for Data Construction

As introduced in Sec. 3.2 we prompt GPT-4 to automatically construct training data from public datasets with human subjective ratings, avoiding substantial manual annotation costs. The training data adopt a unified format as introduced in Sec. 3.2, where $\{$ img_content $\}$, \{instruction_content $\}$ and \{answer_content $\}$ are three placeholders of the tokens of input image, user instruction and VisualCritic's response, respectively. The prompts to generate VisualCritic's response (i.e., \{answer_content\}) are illustrated in Figure 3 which are categorized into four segments, each corresponding to different image assessment functionalities task we investigate in Sec. 3.2. In the supplementary, we provide additional prompts tailored for generating user instruction (i.e., $\{$ instruction_content $\}$ ).

## 5 Conclusion

In this work, we explore the question: Can LMMs perceive low-level visual quality aligned with human perception and show their excellent generalization ability in this regard? We provide an answer with VisualCritic, the first LMM of its kind for broad-spectrum image subjective quality assessment. Compared to
conventional specialist models in the field of visual quality assessment, VisualCritic exhibits unprecedented cross-dataset generalizability and versatility. Beyond numerical scores, VisualCritic can further provide qualitative descriptions about visual quality and perform authenticity detection. We pinpoint the key to VisualCritic and introduce the core strategies on data construction and model training for making it real. We leave its extension to more data like videos and more functionalities in the future exploration. We also look forward to exploring its interplay with AIGC models for achieving better visual alignment.

## References

1. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems 35, 2371623736 (2022)
2. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 (2023)
3. Chen, J., Li, D.Z.X.S.X., Zhang, Z.L.P., Xiong, R.K.V.C.Y., Elhoseiny, M.: Minigpt-v2: Large language model as a unified interface for vision-language multitask learning. arXiv preprint arXiv:2310.09478 (2023)
4. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleashing multimodal llm's referential dialogue magic. arXiv preprint arXiv:2306.15195 (2023)
5. Chen, P., Li, L., Wu, J., Dong, W., Shi, G.: Unsupervised curriculum domain adaptation for no-reference video quality assessment. In: ICCV (2021)
6. Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.E., et al.: Vicuna: An open-source chatbot impressing gpt-4 with $90 \%^{*}$ chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023) (2023)
7. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning (2023)
8. Fang, Y., Zhu, H., Zeng, Y., Ma, K., Wang, Z.: Perceptual quality assessment of smartphone photography. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3677-3686 (2020)
9. Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., Cao, Y.: Eva: Exploring the limits of masked visual representation learning at scale. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19358-19369 (2023)
10. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., Lempitsky, V.: Domain-adversarial training of neural networks. JMLR (2016)
11. Ghadiyaram, D., Bovik, A.C.: Massive online crowdsourced study of subjective and objective picture quality. IEEE Transactions on Image Processing 25(1), 372-387 (2015)
12. Ghildyal, A., Liu, F.: Shift-tolerant perceptual similarity metric. In: European Conference on Computer Vision (2022)
13. Gravitas, S.: Auto-gpt (2023), https://github.com/Significant-Gravitas/ Auto-GPT\#auto-gpt-an-autonomous-gpt-4-experiment
14. Hosu, V., Lin, H., Sziranyi, T., Saupe, D.: Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE Transactions on Image Processing 29, 4041-4056 (2020)
15. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: LoRA: Low-rank adaptation of large language models. In: ICLR (2022)
16. Ke, J., Wang, Q., Wang, Y., Milanfar, P., Yang, F.: Musiq: Multi-scale image quality transformer. In: ICCV (2021)
17. Koh, J.Y., Fried, D., Salakhutdinov, R.: Generating images with multimodal language models. arXiv preprint arXiv:2305.17216 (2023)
18. Laurenon, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A.M., Kiela, D., et al.: Obelics: An open webscale filtered dataset of interleaved image-text documents. In: NeurIPS (2023)
19. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)
20. Li, C., Zhang, Z., Wu, H., Sun, W., Min, X., Liu, X., Zhai, G., Lin, W.: Agiqa3k: An open database for ai-generated image quality assessment. arXiv preprint arXiv:2306.04717 (2023)
21. Li, D., Jiang, T., Jiang, M.: Unified quality assessment of in-the-wild videos with mixed datasets training. International Journal of Computer Vision 129, 1238-1257 (2021)
22. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)
23. Lin, H., Hosu, V., Saupe, D.: Kadid-10k: A large-scale artificially distorted iqa database. In: 2019 Eleventh International Conference on Quality of Multimedia Experience (QoMEX). pp. 1-3. IEEE (2019)
24. Lin, K., Yang, Z., Li, L., Wang, J., Wang, L.: Designbench: Exploring and benchmarking dall-e 3 for imagining visual design. arXiv preprint arXiv:2310.15144 (2023)
25. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint arXiv:2304.08485 (2023)
26. Liu, J., Li, X., An, S., Chen, Z.: Source-free unsupervised domain adaptation for blind image quality assessment. arXiv preprint arXiv:2207.08124 (2022)
27. Liu, X., Van De Weijer, J., Bagdanov, A.D.: Rankiqa: Learning from rankings for no-reference image quality assessment. In: ICCV (2017)
28. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016)
29. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)
30. Lv, T., Huang, Y., Chen, J., Cui, L., Ma, S., Chang, Y., Huang, S., Wang, W., Dong, L., Luo, W., et al.: Kosmos-2.5: A multimodal literate model. arXiv preprint arXiv:2309.11419 (2023)
31. Madhusudana, P.C., Birkbeck, N., Wang, Y., Adsumilli, B., Bovik, A.C.: Image quality assessment using contrastive learning. IEEE Transactions on Image Processing (2022)
32. Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S.: Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft (2022)
33. Mittal, A., Moorthy, A.K., Bovik, A.C.: No-reference image quality assessment in the spatial domain. IEEE Transactions on image processing (2012)
34. OpenAI: Gpt-4 (2023), https://openai.com/research/gpt-4
35. OpenAI: Gpt-4v(ision) system card (2023), https://cdn.openai.com/papers/ GPTV_System_Card.pdf
36. Pan, X., Dong, L., Huang, S., Peng, Z., Chen, W., Wei, F.: Kosmos-g: Generating images in context with multimodal large language models. arXiv preprint arXiv:2310.02992 (2023)
37. Pan, Z., Yuan, F., Lei, J., Fang, Y., Shao, X., Kwong, S.: Vcrnet: Visual compensation restoration network for no-reference image quality assessment. TIP (2022)
38. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023)
39. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML (2021)
40. Rajbhandari, S., Ruwase, O., Rasley, J., Smith, S., He, Y.: Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. pp. 1-14 (2021)
41. Rasley, J., Rajbhandari, S., Ruwase, O., He, Y.: Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining. pp. 3505-3506 (2020)
42. reworkd.ai: Agentgpt (2023), https://github.com/reworkd/AgentGPT
43. Roy, S., Mitra, S., Biswas, S., Soundararajan, R.: Test time adaptation for blind image quality assessment. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 16742-16751 (2023)
44. Saha, A., Mishra, S., Bovik, A.C.: Re-iqa: Unsupervised learning for image quality assessment in the wild. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5846-5855 (2023)
45. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 (2023)
46. Streijl, R.C., Winkler, S., Hands, D.S.: Mean opinion score (mos) revisited: methods and applications, limitations and alternatives. Multimedia Systems 22(2), 213227 (2016)
47. Su, S., Yan, Q., Zhu, Y., Zhang, C., Ge, X., Sun, J., Zhang, Y.: Blindly assess image quality in the wild guided by a self-adaptive hyper network. In: CVPR (2020)
48. Sun, S., Yu, T., Xu, J., Zhou, W., Chen, Z.: Graphiqa: Learning distortion graph representations for blind image quality assessment. Trans. Multi. (2023)
49. Thong, W., Pereira, J.C., Parisot, S., Leonardis, A., McDonagh, S.: Content-diverse comparisons improve iqa. In: BMVC (2022)
50. Wang, J., Chan, K.C., Loy, C.C.: Exploring clip for assessing the look and feel of images. In: AAAI (2023)
51. Wang, J., Duan, H., Liu, J., Chen, S., Min, X., Zhai, G.: Aigciqa2023: A large-scale image quality assessment database for ai generated images: from the perspectives of quality, authenticity and correspondence. arXiv preprint arXiv:2307.00211 (2023)
52. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13(4), 600-612 (2004)
53. Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Li, C., Sun, W., Yan, Q., Zhai, G., et al.: Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181 (2023)
54. Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.C., Liu, Z., Wang, L.: The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421 9 (2023)
55. Yang, Z., Li, L., Wang, J., Lin, K., Azarnasab, E., Ahmed, F., Liu, Z., Liu, C., Zeng, M., Wang, L.: Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 (2023)
56. Ye, P., Kumar, J., Kang, L., Doermann, D.: Unsupervised feature learning framework for no-reference image quality assessment. In: IEEE conference on computer vision and pattern recognition (2012)
57. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al.: mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178 (2023)
58. Ying, Z., Niu, H., Gupta, P., Mahajan, D., Ghadiyaram, D., Bovik, A.: From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3575-3585 (2020)
59. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F., Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704 (2023)
60. Yu, L., Lezama, J., Gundavarapu, N.B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A.G., et al.: Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737 (2023)
61. Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., et al.: Scaling autoregressive multi-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591 (2023)
62. Zhang, L., Zhang, L., Bovik, A.C.: A feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing (2015)
63. Zhang, P., Wang, X.D.B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S., Duan, H., Yan, H., et al.: Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112 (2023)
64. Zhang, W., Ma, K., Yan, J., Deng, D., Wang, Z.: Blind image quality assessment using a deep bilinear convolutional neural network. IEEE Transactions on Circuits and Systems for Video Technology (2018)
65. Zhang, W., Zhai, G., Wei, Y., Yang, X., Ma, K.: Blind image quality assessment via vision-language correspondence: A multitask learning perspective. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14071-14081 (2023)
66. Zhang, Z., Zhang, X., Xie, W., Lu, Y.: Responsible task automation: Empowering large language models as responsible task automators. arXiv preprint arXiv:2306.01242 (2023)
67. Zhang, Z., Sun, W., Zhou, Y., Jia, J., Zhang, Z., Liu, J., Min, X., Zhai, G.: Subjective and objective quality assessment for in-the-wild computer graphics images. Acm T Multim Comput (2023)
68. Zhao, K., Yuan, K., Sun, M., Li, M., Wen, X.: Quality-aware pre-trained models for blind image quality assessment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 22302-22313 (2023)
69. Zheng, K., He, X., Wang, X.E.: Minigpt-5: Interleaved vision-and-language generation via generative vokens. arXiv preprint arXiv:2310.02239 (2023)
70. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)
71. Zhu, H., Li, L., Wu, J., Dong, W., Shi, G.: Metaiqa: Deep meta-learning for noreference image quality assessment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14143-14152 (2020)
</end of paper 2>


<paper 3>
# Enhancing Blind Video Quality Assessment with Rich Quality-aware Features 

Wei Sun ${ }^{1 *}$ Haoning Wu ${ }^{2 *}$ Zicheng Zhang ${ }^{1 *}$ Jun Jia ${ }^{1}$, Zhichao Zhang ${ }^{1}$,<br>Linhan Cao ${ }^{1}$, Qiubo Chen ${ }^{3}$, Xiongkuo Min ${ }^{1 \dagger}$, Weisi Lin $^{2}$, Guangtao Zhai ${ }^{1 \dagger}$<br>${ }^{1}$ Shanghai Jiao Tong University, ${ }^{2}$ Nanyang Technological University, ${ }^{3}$ Xiaohongshu


#### Abstract

In this paper, we present a simple but effective method to enhance blind video quality assessment (BVQA) models for social media videos. Motivated by previous researches that leverage pre-trained features extracted from various computer vision models as the feature representation for BVQA, we further explore rich quality-aware features from pretrained blind image quality assessment (BIQA) and BVQA models as auxiliary features to help the BVQA model to handle complex distortions and diverse content of social media videos. Specifically, we use SimpleVQA, a BVQA model that consists of a trainable Swin Transformer-B and a fixed SlowFast, as our base model. The Swin Transformer$B$ and SlowFast components are responsible for extracting spatial and motion features, respectively. Then, we extract three kinds of features from Q-Align, LIQE, and FAST-VQA to capture frame-level quality-aware features, frame-level quality-aware along with scene-specific features, and spatiotemporal quality-aware features, respectively. Through concatenating these features, we employ a multi-layer perceptron (MLP) network to regress them into quality scores. Experimental results demonstrate that the proposed model achieves the best performance on three public social media VQA datasets. Moreover, the proposed model won first place in the CVPR NTIRE 2024 Short-form UGC Video Quality Assessment Challenge. The code is available at https://github.com/sunwei925/RQ-VQA.git.


## 1. Introduction

Blind video quality assessment (BVQA) [27] aims to provide a perceptual quality score of the video without access to any reference information (i.e., high-quality source videos), which has increasingly played a crucial role in video processing systems of steaming media applications, ensuring that end-users can view high-quality videos and have a superior Quality of Experience (QoE). Towards[^0]

this goal, numerous BVQA models have been proposed to achieve better correlation with human subjective opinions, including knowledge-driven models [12,35,43,44] and data-driven models [14,15, 19,38-40,51,58,59].

Although knowledge-driven BVQA models [5, 12, 35, 43, 44] have better interpretability, they often exhibit relatively poor performance and higher computational complexity compared to data-driven approaches, mainly due to complex human perception processes involved in assessing visual quality. With the rapid development of deep neural network (DNN), data-driven BVQA models have demonstrated excellent performance on various kinds of videos, including professionally generated content (PGC) videos with synthetic distortions [23] and user-generated content (UGC) videos with realistic distortions [19,38,49,51].

The success of data-driven BVQA models can be attributed to two factors. The first is the adoption of more advanced neural networks, including convolutional neural network (CNN)-based methods (e.g., VSFA [15], SimpleVQA [38], Li22 [14], etc.), Transformer-based methods (e.g. StarVQA [57], FAST-VQA [51], etc.), and recent large multi-modality (LLM)-based methods (e.g. QAlign [56]). The second is the construction of large-scale subjective labeled video quality assessment (VQA) datasets (e.g., LSVQ [59], etc.), enabling the DNN models learn quality-aware feature representation from the videos and the corresponding quality labels.

As data-driven methods, the performance of BVQA models relies heavily on the human-rated VQA datasets. However, the videos in current mainstreaming VQA datasets [9, 10, 33, 37, 48, 59] were typically captured by outdated cameras or collected from video sharing websites several years ago. The distortion types and video content may not align with the videos in current streaming video applications especially social media applications, as the shooting devices and video processing algorithms including pre-processing, compression, and enhancement algorithms, have greatly improved. Therefore, the BVQA model trained by these VQA datasets may not have sufficient capability to evaluate the perceptual quality of the millions of newly social media videos uploaded daily.

![](https://cdn.mathpix.com/cropped/2024_06_04_c4e7ba2112ab6606b875g-02.jpg?height=591&width=1678&top_left_y=247&top_left_x=169)

Figure 1. The comparison of PGC videos, UGC videos, and the processed UGC videos.

In this paper, we focus on BVQA models for social media videos ${ }^{1}$, which exhibit two distinct characteristics: 1) the video content usually includes lots of special effects, text descriptions, subtitled, etc., 2) the videos undergo complex processing workflows including pre-processing, transcoding, and enhancement. We show some typical social media videos in Figure 1.

In the literature, Sun et al. propose a BVQA framework named SimpleVQA, comprising a trainable spatial quality module and a fixed temporal quality module, achieving competitive performance compared to state-of-the-art methods. This framework shows excellent extensibility in accommodating various scenarios, including surveillance video quality assessment [62] and point cloud quality assessment [63], etc. Moreover, Zhang et al. [65] extract geometry features (i.e., dihedral angle, gaussian curvature, and NSS parameters) of the mesh of digital human and integrated them into the SimpleVQA framework to assess the quality of dynamic digital human. Wen et al. [50] propose a spatial rectifier and temporal rectifier within the SimpleVQA framework to address variable spatial resolution and frame rate video quality assessment problems. These studies indicate that with proper quality-aware features, SimpleVQA can effectively handle various types of quality assessment problems.

Therefore, we also resort to the SimpleVQA framework to address the social media BVQA problem. Given the diverse content of social media videos and the variety of video processing algorithms they undergo, training SimpleVQA end-to-end may require a large-scale of VQA datasets to achieve the robust quality feature representation, while the newest social media VQA dataset, KVQ [24], comprises only 3,600 quality-labeled videos. Inspired by prior[^1]

works $[50,65]$, we enhance SimpleVQA with rich qualityaware features derived from state-of-the-art blind image quality assessment (BIQA) and BVQA models, which helps to alleviate the model's reliance on training data and improve its robustness.

To be more specific, we choose two BIQA models, LIQE [61] and Q-Align [56], and one BVQA model, FASTVQA [51], to extract frame-level quality-aware features, frame-level quality-aware along with scene-specific features, and spatiotemporal quality-aware features, respectively. LIQE and Q-Align are both vision-language based BIQA models. For LIQE, we use the textual template: " $a$ photo of $a(n)\{s\}$ with $\{d\}$ artifacts, which is of $\{c\}$ quality" and calculate the cosine similarity between the visual embedding of the test image and the textual embedding of the text prompt. The parameters " $s$ ", " $d$ ", and " $c$ " belong to nine scene categories, eleven distortion types, and five distortion levels respectively, and total 495 text prompts are tested to derive 495 dimensional LIQE features. For QAlign, we use the conversation formats: "\#User:  image \rangle How would you rate the quality of this image? \#Assistant: The quality of the image is $\langle$ level $\rangle . "$, where $\langle$ image \rangle and $\langle$ level $\rangle$ denote the image token and image quality level respectively. We extract Q-Align features by computing the hidden embedding of the last encoder layer. FASTVQA features are computed by global average pooling the last-stage feature maps. Then, we use Swin Transformer$\mathrm{B}$ [21] as the spatial quality analyzer and the temporal pathway of the SlowFast [6] network as the temporal quality analyzer. To further enhance the spatial feature representation, we add a multi-head self-attention (MHSA) [45] after the feature maps extracted by the Swin Transformer-B to capture salience information and guide the spatial feature extraction. Finally, we concatenate the SimpleVQA features (including both spatial and temporal features), LIQE features, Q-Align features, and FAST-VQA features and
regress them into video quality scores by a two-layer multilayer perception (MLP) network. The Pearson correlation coefficient (PLCC) loss is used to optimize the entire BVQA model. Our model achieves the best performance on three UGC VQA datasets and achieve the first place in the CVPR NTIRE 2024 Short-form UGC Video Quality Assessment Challenge [16].

The core contributions of this paper are summarized as follows:

- We enhance the SimpleVQA framework with three kinds of quality-aware pre-trained features, yielding outstanding performance on social media UGC VQA datasets and also exhibiting remarkable robustness and generalizability.
- We utilize the MHSA module to capture the salience frame regions that influence the visual quality, thereby enhancing the fine-grained quality assessment capabilities.


## 2. Related Work

### 2.1. VQA Datasets

Early VQA datasets primarily focus on synthetic distortions introduced by different video processing stages, such as spatiotemporal downsampling [13, 18, 25, 26, 32], compression [3, 18, 36, 47], transmission [2, 4, 8, 31], etc. These datasets typically consist of a limited number of high-quality source videos and the corresponding distorted ones. Due to limited video content and not considering the realistic distortions, these datasets are not suitable for training general BVQA models. Therefore, recent VQA datasets $[9,10,33,37,48,59$ ] have shifted focus towards realistic captured distortions. For example, LIVE-Qualcomm [9] consists of 208 videos captured by 8 smartphones across 54 unique scenes. LIVE-VQC [37] includes 585 videos captured by 80 mobile cameras, encompassing different lighting conditions and diverse levels of motion, each video corresponding to a unique scene. LSVQ [59] consists of 38,811 videos sampled from the Internet Archive and YFCC100M datasets by matching six video feature distributions. In general, These datasets have greatly promoted the development of objective BVQA models.

However, for videos on social media platforms like Kwai and TikTok, their quality is influenced by both in-captured distortions and distortions caused by video processing algorithms. Hence, some studies have started to construct social media VQA datasets. For instance, Li et al. [17] selected 50 source videos from TikTok and then two encoders (i.e. H. 264 and H.265) were used to compress each video with five QPs to simulate the video transcoding procedure. Yu et al. [60] sampled 55 1080p videos from LIVEVQC [37], downscaled them to four different resolutions, and subsequently compressed using H. 264 across 17 compression levels. To streamline the human study, a sampling strategy was employed to select 220 represented distorted video for the subjective VQA study. Zhang et al. [64] constructed the TaoLive dataset, containing 418 raw videos from the TaoLive platform and 3, 762 distorted videos compressed at 8 different CRF levels using H.265. Gao et al. [7] studied the impact of video enhancement algorithms on UGC videos and constructed the VDPVE dataset, which includes 184 low-quality videos and 1,211 videos enhanced by light/contrast/color, deblurring, stabilization algorithms. Wu et al. [53] introduced the MaxWell dataset with 4,543 videos labeled with multi-attribute scores on 16 dimensions. Lu et al. [24] introduced the KVQ dataset to further study the impact of complete video processing workflows, including pre-processing, transcoding, and enhancement, on video quality. The dataset consists of 600 user-upload social media videos and 3,600 processed videos.

In this paper, we focus on quality assessment for UGC videos processed by multiple video processing algorithms (called social media videos in this paper), which is more challenging to BVQA models because of their diverse distortions introduced during both capture and video editing/processing stages.

### 2.2. BVQA Models

As stated in Section 1, we can roughly divide the BVQA models into knowledge-driven methods and data-driven methods.

Knowledge-driven BVQA models [5,12,29,35,43,44] utilize carefully designed handcrafted features to quantify the video quality. For example, V-BLIINDS [35] utilizes spatiotemporal natural scene statistics (NSS) models to quantify the NSS features of frame differences and motion coherency characteristics, and then regresses these features to video scores by support vector regressor (SVR). Mittal et al. [29] propose a training-free blind VQA model named VIIDEO that exploits intrinsic statistics regularities of natural videos to quantify disturbances introduced due to distortions. TLVQM [12] extracts rich spatiotemporal features such as motion, jerkiness, blurriness, noise, blockiness, color, etc. from both high and low complexity levels. VIDEVAL [43] employs the sequential forward floating selection strategy to choose a set of quality-aware features from typical BI/VQA methods, followed by training an SVR model to regress them into the video quality. TLVQM and VIDEVAL demonstrate that leveraging rich qualityaware handcrafted features enables the BVQA model to achieve better performance. In this paper, we show that combining diverse quality-aware features extracted from DNNs with a base BVQA model (e.g. SimpleVQA) can also achieve superior performance. .

Data-driven BVQA methods $[14,15,19,20,38,40,49,51$,

58, 59] mainly leverage DNNs to extract the quality-aware features. For instance, Liu et al. [20] introduce a multi-task BVQA model, optimizing the 3D-CNN for quality assessment and compression distortion classification simultaneously. VSFA [15] first extracts semantic features from a pre-trained CNN model, followed by utilizing a gated recurrent unit (GRU) network to capture the temporal relationship among the semantic features of video frames. Yi et al. [58] propose an attention mechanism based BVQA model, which employs a non-local operator to handle uneven spatial distortion problems. Ying et al. [59] introduce a local-to-global region-based BVQA model, combing the quality-aware features extracted from a BIQA pre-trained and spatiotemporal features from a pre-trained action recognition network. Li et al. [14] also employ the IQA model pre-trained on multiple databases to extract quality-aware spatial features and the action recognition model to extract temporal features, subsequently utilizing a GRU network is used to regress spatial and temporal features into the quality scores. Sun et al. $[38,40]$ propose SimpleVQA, a BVQA framework that consists of a trainable spatial feature extraction module and a pre-trained motion feature extraction model. In this paper, we adopt SimpleVQA as our base model. Wu et al. [51] propose FAST-VQA, which samples spatio-temporal grid mini-cubes from original videos and trains a fragment attention network consisting of a Swin transformer and the gated relative position biases in an endto-end manner. Wu et al. [52] further propose DOVER, which integrates FAST-VQA with an aesthetics quality assessment branch to evaluate video quality from both technique and aesthetics perspectives. With the popularity of large multi-modality models (LMMs), some LMM-based quality assessment models [11,54-56] have been proposed to evaluate the image/video quality by providing predefined text prompts to LMMs.

Recently, there have been efforts to integrate various types of DNN features to enhance BVQA performance and provide explainability. For example, Wang et al. [49] propose a feature-rich BVQA model that assesses quality from three aspects including compression level, video content, and distortion type, with each aspect evaluated by a separate neural network. Liu et al. [19] extract seven types of features extracted by EfficientNet-b7 [41], ir-CSN-152 [42], CLIP [34], Swin Transformer-B [21], TimeSformer [1], Video Swin Transformer-B [22], and SlowFast [6] to represent content-aware, distortion-aware, and motion-aware features of videos. They incorporate these quality representations as supplementary supervisory information to train a lightweight BVQA model in a knowledge manner. These studies demonstrate the potential for BVQA models to benefit from various computer vision tasks. In this paper, we further demonstrate that BVQA models can achieve better performance with quality-aware pre-trained features.

## 3. Proposed Model

As depicted in Figure 2, our BVQA model builds upon SimpleVQA, incorporating Swin Transformer-B for learning spatial quality feature representation and leveraging the temporal path of SlowFast for modeling motion characteristics. We integrate three kinds of quality-aware features including LIQE, Q-Align, and FAST-VQA into SimpleVQA to enhance its quality-aware feature representation, thereby improving its capability to handle complex distortions of social media videos introduced during capture and video editing/processing procedures.

### 3.1. Video Pre-processing

Given a video $\boldsymbol{x}=\left\{\boldsymbol{x}_{i}\right\}_{i=0}^{N-1}$, where $\boldsymbol{x}_{i} \in \mathbb{R}^{H \times W \times 3}$ represents the $i$-th frame. Here, $H$ and $W$ denote the height and the width of each frame respectively, and $N$ is the total number of frames. The features extracted by our method can be categorized into three levels: spatial, temporal, and spatiotemporal. Therefore, we partition the video into three parts: key frames, video chunks, and the entire video. For key frames, we sample the first frame of every one-second video frame sequence as the key frame, denoted as:

$$
\begin{align*}
\boldsymbol{z} & =\left\{\boldsymbol{z}_{i}\right\}_{i=0}^{N_{z}-1} \\
N_{z} & =N / r  \tag{1}\\
\boldsymbol{z}_{i} & =\boldsymbol{x}_{i * r}
\end{align*}
$$

where $r$ represents the frame rate of the video $\boldsymbol{x}$. For video chunks, we split the video $\boldsymbol{x}$ into a series of video chunks:

$$
\begin{align*}
\mathcal{V} & =\left\{\boldsymbol{v}^{(i)}\right\}_{i=0}^{N_{z}-1} \\
\boldsymbol{v}^{(i)} & =\left\{\boldsymbol{x}_{s}\right\}_{s=i * r}^{(i+1) * r-1} \tag{2}
\end{align*}
$$

Specifically, each key frame corresponds to one video chunk. For the entire video, the video $\boldsymbol{x}$ is directly used as the input.

### 3.2. The Base Model

We adopt SimpleVQA [38] as our base model, which utilizes a trainable spatial quality analyzer to extract spatial quality-ware features and employs a fixed temporal quality analyzer to capture motion features. Recent study [40] suggests that most VQA datasets are dominated by spatial distortions and pose little challenge to the temporal quality analyzer. Therefore, we choose a high-performance backbone Swin Transformer-B [21] as our spatial quality analyzer. We drop out the classification head of Swin TransformerB and add a MHSA module [45] to guide the spatial quality analyzer to focus on salience regions of video frames that affect video quality. We finally apply global average pooling to obtain the spatial quality representation. We denote these procedures as:

$$
\begin{equation*}
\mathcal{F}_{i}^{s}=\operatorname{GP}\left(\operatorname{MHSA}\left(\operatorname{SwinB}\left(\boldsymbol{z}_{i}\right)\right)\right) \tag{3}
\end{equation*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_04_c4e7ba2112ab6606b875g-05.jpg?height=732&width=1678&top_left_y=244&top_left_x=213)

Figure 2. The framework of the proposed BVQA model. We use SimpleVQA as the base model, which consists of a Swin Transformer-B and a SlowFast. We extract three quality-aware features using LIQE, Q-Align, and FAST-VQA as the auxiliary features. These features are then concatenated and regressed into the quality score via a MLP network.

where GP, MHSA, and SwinB represent global average pooling, MHSA module, and Swin Transformer-B without the classification head operators. $\mathcal{F}_{i}^{s}$ is the spatial features of $i$-th key frames.

The temporal quality analyzer is designed to extract video motion information, which is important for detecting distortions such as jitter caused by unstable shooting equipment or lagging resulting from low bandwidth during streaming. Following the approaches [38,40], we use the fast pathway of SlowFast to extract motion features for each video chunk. We also remove the classification head of SlowFast and calculate the temporal features by global average pooling the last-stage feature maps:

$$
\begin{equation*}
\left.\mathcal{F}_{i}^{t}=\operatorname{GP}\left(\operatorname{SlowFast}\left(\boldsymbol{v}^{(i)}\right)\right)\right) \tag{4}
\end{equation*}
$$

where SlowFast denotes the SlowFast without the classification head. $\mathcal{F}_{i}^{t}$ is the temporal features of $i$-th video chunk.

### 3.3. LIQE Features

LIQE is a multi-task learning based visual-language model for BIQA. It employs the CLIP model, including an image encoder and a text encoder, to compute the cosine similarity between text features and image features. Specifically, it can take a text prompt $t(s, d, c)=$ "a photo of a $(n)$ $\{s\}$ with $\{d\}$ artifacts, which is of $\{c\}$ quality" and an image as the inputs, and calculate the cosine similarity between text features and image features as the probabilities to represent how well that the text prompt describes the test image. Subsequently, the probabilities can be used to infer the scene type, artifact type, and quality level of the test image.
Therefore, we utilize the probabilities from different types of text prompts as the features to represent the scene, artifact, and quality-level characteristics of video frames. Here, we consider nine scene categories: $s \in S=$ \{"animal", "cityscape", "human", "indoor scene", "landscape", "night scene", "plant", "still-life", and "others"\}, eleven distortion types: $d \in D=\{$ "blur", "color-related", "contrast", "JPEG compression", "JPEG2000 compression", "noise", "overexposure", "quantization", "underexposure", "spatially-localized", and "others"\}, and five quality levels: $c \in C=\{1,2,3,4,5\}=\{$ "bad", "poor", "fair", "good", "perfect" $\}$. So, in total, we have 495 text prompt candidates to compute the probabilities:

$$
\begin{equation*}
\left.\mathcal{F}_{i}^{\mathrm{LIQE}}=\operatorname{LIQE}\left(\boldsymbol{z}_{\boldsymbol{i}}, \boldsymbol{t}(\boldsymbol{s}, \boldsymbol{d}, \boldsymbol{c})\right)\right) \tag{5}
\end{equation*}
$$

where $\mathcal{F}_{i}^{\mathrm{LIQE}}$ represents the LIQE features of $i$-th key frames, which comprises 495 dimensions corresponding to the scene category, artifact type, and quality level characteristics.

### 3.4. Q-Align Features

Q-Align is a large multi-modality model designed for quality assessment tasks. Specifically, Q-Align is pretrained on multiple large-scale image/video quality assessment databases. The quality labels of the databases are first transformed into qualitative adjective descriptions (excellent, good, fair, poor, bad) and are then integrated into question-answer pairs for instruction fine-tuning of Q-Align. After training, Q-Align operates by taking in the prompt of "How is the quality of this image? |img|

The quality of the image is [SCORE_TOKEN]", where [SCORE_TOKEN] is the quality rating token responded by Q-Align and [SCORE_TOKEN] can be translated into the $\log$ probabilities to the predefined qualitative adjective descriptions.

However, to form a more comprehensive quality representation from the $\mathrm{Q}$-Align perspective, we extract the feature map from the last hidden layer of Q-Align rather than [SCORE_TOKEN] for analysis, which can be derived as:

$$
\begin{equation*}
\mathcal{F}_{i}^{\mathrm{Q}-\mathrm{Align}}=\operatorname{GP}\left(\mathrm{Q}-\operatorname{Align}\left(\boldsymbol{z}_{i}\right)\right) \tag{6}
\end{equation*}
$$

where $\mathcal{F}_{i}^{\mathrm{Q} \text {-Align }} \in R^{1 \times 4096}$ stands for the $\mathrm{Q}$-Align features of the $i$-th key frame, Q-Align $(\cdot)$ denotes the Q-Align last hidden layer feature map extraction process.

### 3.5. FAST-VQA Features

FAST-VQA is an efficient algorithm specially designed for BVQA. It notices that videos contain a high degree of spatio-temporal redundancy, and correspondingly proposes a grid mini-cude sampling (GMS) algorithm to pre-sample the video data before feeding them to the backbone, i.e. Video Swin Transformer Tiny (VSwin-T) [22] . For video as $\boldsymbol{x}$, the sampled fragments $\left(\boldsymbol{x}^{f}\right.$ ) are formulated as follows:

$$
\begin{align*}
& \left.\boldsymbol{x}_{i,\left[u \times S_{f}\right.}^{f}:(u+1) \times S_{f}, v \times S_{f}:(v+1) \times S_{f}\right]  \tag{7}\\
= & \left.\operatorname{RCrop}\left(\boldsymbol{x}_{i,\left[\frac{u \times H}{G_{f}}\right.}: \frac{(u+1) \times H}{G_{f}}, \frac{v \times W}{G_{f}}: \frac{(v+1) \times W}{G_{f}}\right], s_{f}\right) \tag{8}
\end{align*}
$$

The fragments are then fed into the VSwin-T to obtain the FAST-VQA features:

$$
\begin{equation*}
\mathcal{F}^{\mathrm{FAST}-\mathrm{VQA}}=\mathrm{FAST}-\mathrm{VQA}\left(\boldsymbol{x}^{f}\right) \tag{9}
\end{equation*}
$$

In this method, we extract FAST-VQA features pretrained from the LSVQ [59] database.

### 3.6. Quality Regression

After calculating these features, we concatenate these features into the final feature representation $\mathcal{F}_{i}$ :

$$
\begin{align*}
& \mathcal{F}_{i}=\operatorname{Cat}\left(\mathcal{F}_{i}^{s}, \mathcal{F}_{i}^{t}, \mathcal{F}_{i}^{\mathrm{LIQE}}, \mathcal{F}_{i}^{\mathrm{Q}-\mathrm{Align}}, \mathcal{F}_{i}^{\mathrm{FAST}-\mathrm{VQA}}\right) \\
& \mathcal{F}_{i}^{\mathrm{FAST}-\mathrm{VQA}}=\mathcal{F}^{\mathrm{FAST}-\mathrm{VQA}} \tag{10}
\end{align*}
$$

where Cat is the concatenation operator.

We then use a two-layer MLP network to regress $\mathcal{F}_{i}$ into local quality scores $\hat{q}_{i}$ :

$$
\begin{equation*}
\hat{q}_{i}=\operatorname{MLP}\left(\mathcal{F}_{i}\right) \tag{11}
\end{equation*}
$$

where MLP denotes the MLP operator and $\boldsymbol{q}_{i}$ is the quality score of $i$-th frame/chunk. We use the average pooling method to derive the global quality score $\hat{q}$ :

$$
\begin{equation*}
\hat{q}=\frac{1}{N_{z}} \sum_{i=0}^{N_{z}-1} \hat{q}_{i} \tag{12}
\end{equation*}
$$

### 3.7. Loss Function

Similar to $[40,51]$, we use the PLCC loss to optimize the proposed BVQA model:

$$
\begin{equation*}
\mathcal{L}=\left(1-\frac{\langle\hat{\boldsymbol{q}}-\operatorname{mean}(\hat{\boldsymbol{q}}), \boldsymbol{q}-\operatorname{mean}(\boldsymbol{q})\rangle}{\|\hat{\boldsymbol{q}}-\operatorname{mean}(\hat{\boldsymbol{q}})\|_{2}\|\boldsymbol{q}-\operatorname{mean}(\boldsymbol{q})\|_{2}}\right) / 2 \tag{13}
\end{equation*}
$$

where $\boldsymbol{q}$ and $\hat{\boldsymbol{q}}$ are the vectors of ground-truth and predicted quality scores of the images in a batch respectively, $\langle\cdot\rangle$ represents the inner product of two vectors, $\|\cdot\|$ denotes the norm operator for a vector, and mean is the average operator for a vector.

## 4. Experiment

### 4.1. Experimental Protocol

Test Datasets. We test our model on three VQA datasets: KVQ [24], TaoLive [64], and LIVE-WC [60], all of which focus on assessing the quality of streaming UGC videos. For KVQ, we train our model on the publicly released data from NTIRE 2024 Short-form UGC Video Quality Assessment Challenge ${ }^{2}$ and subsequently test the trained model on both validation and test sets. For TaoLive and LIVEWC, we conduct random splits of the videos with an $80 \%$ - $20 \%$ train-test ratio based on the video scenes, and repeat this process five times and report the average performance.

Implementation Details. As stated in Section 3, we utilize Swin Transformer-B [21] and SlowFast R50 [6] as the backbones of the spatial and temporal quality analyzers in the basic model. To improve the generalization ability of the basic model, we first train it on the LSVQ dataset [59], following the training strategy in [40]. Regarding the spatial quality analyzer, we resize the resolution of the minimum dimension of key frames as 384 while preserving their aspect ratios. During the training and test stages, the key frames are randomly and centrally cropped with a resolution of $384 \times 384$. As for the temporal quality analyzer, the resolution of the video chunks is resized to $224 \times 224$ without respecting the aspect ratio. For LIQE, Q-Align, and FASTVQA, we adhere to the original setups of these methods without making any alterations to extract the corresponding features. The Adam optimizer with the initial learning rate $1 \times 10^{-5}$ and batch size 6 is used to train the proposed model on a server with 2 NVIDIA RTX 3090. We decay the learning rate by a factor of 10 after 10 epochs and the total number of epochs is set as 30 .

Compared Models. We compare the proposed method with eight typical BVQA methods, including four knowledge-driven methods: NIQE [30], TLVQM [12], VIDEVAL [43], and RAPIQUE [44], and four data-driven methods: VSFA [15], SimpleVQA [38], FAST-VQA [52],[^2]

Table 1. Performance of the compared models and the proposed model on KVQ validation, KVQA test, TaoLive, and LIVE-WC datasets. The best-performing model is highlighted in each column

| BVQA Methods | KVQ Validation |  | KVQ Test |  | TaoLive |  | LIVE-WC |  |  |
| :---: | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | SRCC | PLCC | SRCC | PLCC | SRCC | PLCC | SRCC | PLCC |
|  | NIQE [30] | 0.239 | 0.241 | 0.272 | 0.281 | 0.331 | 0.327 | 0.245 | 0.241 |
|  | BRISQUE [28] | 0.472 | 0.480 | 0.489 | 0.493 | 0.764 | 0.767 | 0.794 | 0.797 |
|  | TLVQM [12] | 0.490 | 0.509 | 0.511 | 0.524 | 0.869 | 0.873 | 0.827 | 0.831 |
| Methods | VIDEAL [43] | 0.369 | 0.639 | 0.425 | 0.652 | 0.889 | 0.892 | 0.822 | 0.820 |
|  | RAPIQUE [44] | 0.803 | 0.801 | 0.815 | 0.818 | 0.841 | 0.838 | 0.867 | 0.866 |
|  | VSFA [15] | 0.830 | 0.834 | 0.843 | 0.840 | 0.904 | 0.903 | 0.857 | 0.857 |
|  | SimpleVQA [38] | 0.874 | 0.875 | 0.881 | 0.877 | $\mathbf{0 . 9 1 6}$ | 0.915 | 0.913 | 0.920 |
|  | FAST-VQA [52] | 0.864 | 0.865 | 0.871 | 0.870 | 0.876 | 0.881 | 0.849 | 0.852 |
|  | Q-Align [56] | 0.703 | 0.701 | 0.664 | 0.693 | 0.742 | 0.722 | 0.739 | 0.714 |
|  | Proposed | $\mathbf{0 . 9 1 4}$ | $\mathbf{0 . 9 1 8}$ | $\mathbf{0 . 9 2 6}$ | $\mathbf{0 . 9 2 4}$ | 0.912 | $\mathbf{0 . 9 1 8}$ | $\mathbf{0 . 9 5 5}$ | $\mathbf{0 . 9 5 5}$ |

Table 2. The results of NTIRE 2024 Challenge

| Team | Scores |
| :---: | :---: |
| SJTU MMLab (Proposed) | 0.9228 |
| IH-VQA | 0.9145 |
| TVQE | 0.9120 |
| BDVQAGroup | 0.9116 |
| VideoFusion | 0.8932 |

and Q-Align [56]. Except for Q-Align, we train other BVQA models for fair comparison.

Evaluation Criteria. We employ two criteria to evaluate the performance of VQA models: PLCC and Spearman rank-order correlation coefficient (SRCC). Note that PLCC assesses the prediction linearity of the VQA model, while SRCC evaluates the prediction monotonicity. An outstanding VQA model should achieve SRCC and PLCC values close to 1 . Before computing PLCC, we adhere to the procedure outlined in [46] to map model predictions to MOSs by a monotonic four-parameter logistic function to compensate for prediction nonlinearity.

### 4.2. Experimental Results

We list the experimental results in Table 1, from which we can obtain several conclusions. First, it is evident that all knowledge-driven methods perform poorly on three social media VQA datasets, suggesting that they lack the capability to effectively evaluate the quality of social media videos. Second, the proposed model achieves the best performance on both the KVQ and LIVE-WC datasets, surpassing competing BVQA methods by a substantial margin. This demonstrates that by incorporating rich quality-aware features, the proposed model has more powerful feature rep-
Table 3. The results of ablation studies on KVQ test set

| Base <br> Model | Q-Align | LIQE | FAST- <br> VQA | KVQ Test |  |
| :---: | :---: | :---: | :---: | :---: | :---: |
| SRCC | PLCC |  |  |  |  |
| $\sqrt{ }$ | $\times$ | $\sqrt{ }$ | $\sqrt{ }$ | 0.922 | 0.920 |
| $\sqrt{ }$ | $\sqrt{ }$ | $\times$ | $\sqrt{ }$ | 0.923 | 0.921 |
| $\sqrt{ }$ | $\sqrt{ }$ | $\sqrt{ }$ | $\times$ | 0.924 | 0.925 |
| $\sqrt{ }$ | $\sqrt{ }$ | $\sqrt{ }$ | $\sqrt{ }$ | 0.926 | 0.924 |

resentation capabilities for the complex BVQA task (e.g., BVQA for social media videos). Third, we observe that the proposed model achieves similar performance to SimpleVQA on TaoLive but outperforms other methods. The possible reason is that the videos in TaoLive mainly contain front-faces with diverse backgrounds. Moreover, the video processing method adopted in TaoLive only includes compression, which makes it simpler compared to the other two datasets. Therefore, even in the absence of diverse qualityaware features, SimpleVQA can still achieve state-of-theart performance, which also demonstrates the rationality of using SimpleVQA as the base model.

We also list the results of NTIRE Challenge in Table 2. To improve the robustness, we randomly split the public training set of KVQ with an $80 \%-20 \%$ ten times and use the ensemble results to compute the model performance. From Table 2, it is shown that the proposed model significantly outperforms other competing teams.

### 4.3. Ablation Studies

In this section, we investigate the effectiveness of features used in the proposed model. Specifically, we ablate Q-Align, LIQE, and FAST-VQA features from the proposed model respectively, and test them on the KVQ test set. The
experimental results are listed in Table 3. From Table 3, it is evident that regardless of which features are ablated, there is a performance degradation. When all features are integrated, the proposed model achieves the highest performance, which validates the effectiveness of extracted features.

## 5. Conclusion

In this paper, we attempt to enhance BVQA models with diverse quality-aware features and propose a strong BVQA model for social media videos. We use SimpleVQA as the base BVQA model and extract three kinds of qualityaware features from two BIQA models, LIQE and Q-Align, and one BVQA model, FAST-VQA. We simply concatenate these features with SimpleVQA and then regress them into the video quality score via a MLP network. Experimental results show that the proposed model achieves the best performance on three social media VQA datasets.

## 6. Acknowledgement

This work was supported in part by the National Natural Science Foundation of China under Grants 62071407, 62301316, 62225112, 62376282 and 62271312, the China Postdoctoral Science Foundation under Grants 2023TQ0212 and 2023M742298, the Postdoctoral Fellowship Program of CPSF under Grant GZC20231618, the Fundamental Research Funds for the Central Universities, the National Key R\&D Program of China (2021YFE0206700), the Science and Technology Commission of Shanghai Municipality (2021SHZDZX0102), and the Shanghai Committee of Science and Technology (22DZ2229005).

## References

[1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, volume 2, page 4, 2021. 4

[2] Chao Chen, Lark Kwon Choi, Gustavo De Veciana, Constantine Caramanis, Robert W Heath, and Alan C Bovik. Modeling the time-varying subjective quality of http video streams with rate adaptations. IEEE Transactions on Image Processing, 23(5):2206-2221, 2014. 3

[3] Francesca De Simone, Marco Tagliasacchi, Matteo Naccari, Stefano Tubaro, and Touradj Ebrahimi. A h. 264/avc video database for the evaluation of quality metrics. In 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 2430-2433. IEEE, 2010. 3

[4] Zhengfang Duanmu, Kai Zeng, Kede Ma, Abdul Rehman, and Zhou Wang. A quality-of-experience index for streaming video. IEEE Journal of Selected Topics in Signal Processing, 11(1):154-166, 2016. 3

[5] Joshua Peter Ebenezer, Zaixi Shang, Yongjun Wu, Hai Wei, Sriram Sethuraman, and Alan C Bovik. Chipqa:
No-reference video quality prediction via space-time chips. IEEE Transactions on Image Processing, 30:8059-8074, 2021. 1,3

[6] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202-6211, 2019. 2, 4, 6

[7] Yixuan Gao, Yuqin Cao, Tengchuan Kou, Wei Sun, Yunlong Dong, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai. Vdpve: Vqa dataset for perceptual video enhancement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1474-1483, 2023. 3

[8] Deepti Ghadiyaram, Alan C Bovik, Hojatollah Yeganeh, Roman Kordasiewicz, and Michael Gallant. Study of the effects of stalling events on the quality of experience of mobile streaming videos. In 2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP), pages 989993. IEEE, 2014. 3

[9] Deepti Ghadiyaram, Janice Pan, Alan C Bovik, Anush Krishna Moorthy, Prasanjit Panda, and Kai-Chieh Yang. Incapture mobile video distortions: A study of subjective behavior and objective algorithms. IEEE Transactions on Circuits and Systems for Video Technology, 28(9):2061-2077, 2017. 1, 3

[10] Vlad Hosu, Franz Hahn, Mohsen Jenadeleh, Hanhe Lin, Hui Men, Tams Szirnyi, Shujun Li, and Dietmar Saupe. The konstanz natural video database (konvid-1k). In 2017 Ninth international conference on quality of multimedia experience (QoMEX), pages 1-6. IEEE, 2017. 1, 3

[11] Zhipeng Huang, Zhizheng Zhang, Yiting Lu, Zheng-Jun Zha, Zhibo Chen, and Baining Guo. Visualcritic: Making $1 \mathrm{mms}$ perceive visual quality like humans. arXiv preprint arXiv:2403.12806, 2024. 4

[12] Jari Korhonen. Two-level approach for no-reference consumer video quality assessment. IEEE Transactions on Image Processing, 28(12):5923-5938, 2019. 1, 3, 6, 7

[13] Dae Yeol Lee, Somdyuti Paul, Christos G Bampis, Hyunsuk Ko, Jongho Kim, Se Yoon Jeong, Blake Homan, and Alan C Bovik. A subjective and objective study of spacetime subsampled video quality. IEEE Transactions on Image Processing, 31:934-948, 2021. 3

[14] Bowen Li, Weixia Zhang, Meng Tian, Guangtao Zhai, and Xianpei Wang. Blindly assess quality of in-the-wild videos via quality-aware pre-training and motion perception. IEEE Transactions on Circuits and Systems for Video Technology, 32(9):5944-5958, 2022. 1, 3, 4

[15] Dingquan Li, Tingting Jiang, and Ming Jiang. Quality assessment of in-the-wild videos. In Proceedings of the 27th ACM International Conference on Multimedia, pages 23512359, 2019. 1, 3, 4, 6, 7

[16] Xin Li, Kun Yuan, Yajing Pei, Yiting Lu, Ming Sun, Chao Zhou, Zhibo Chen, Radu Timofte, et al. Ntire 2024 challenge on short-form ugc video quality assessment: Methods and results. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2024. 3

[17] Yang Li, Shengbin Meng, Xinfeng Zhang, Meng Wang, Shiqi Wang, Yue Wang, and Siwei Ma. User-generated video
quality assessment: A subjective and objective study. IEEE Transactions on Multimedia, 25:154-166, 2021. 3

[18] Zhuoran Li, Zhengfang Duanmu, Wentao Liu, and Zhou Wang. Avc, hevc, vp9, avs2 or av1?-a comparative study of state-of-the-art video encoders on $4 \mathrm{k}$ videos. In Image Analysis and Recognition: 16th International Conference, ICIAR 2019, Waterloo, ON, Canada, August 27-29, 2019, Proceedings, Part I 16, pages 162-173. Springer, 2019. 3

[19] Hongbo Liu, Mingda Wu, Kun Yuan, Ming Sun, Yansong Tang, Chuanchuan Zheng, Xing Wen, and Xiu Li. Ada-dqa: Adaptive diverse quality-aware feature acquisition for video quality assessment. In Proceedings of the 31st ACM International Conference on Multimedia, pages 6695-6704, 2023. $1,3,4$

[20] Wentao Liu, Zhengfang Duanmu, and Zhou Wang. Endto-end blind quality assessment of compressed videos using deep neural networks. In ACM Multimedia, pages 546-554, 2018. 3, 4

[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012-10022, 2021. 2, 4, 6

[22] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3202-3211, 2022. 4, 6

[23] Wei Lu, Wei Sun, Zicheng Zhang, Danyang Tu, Xiongkuo Min, and Guangtao Zhai. Bh-vqa: Blind high frame rate video quality assessment. In 2023 IEEE International Conference on Multimedia and Expo (ICME), pages 2501-2506. IEEE, 2023. 1

[24] Yiting Lu, Xin Li, Yajing Pei, Kun Yuan, Qizhi Xie, Yunpeng Qu, Ming Sun, Chao Zhou, and Zhibo Chen. Kvq: Kaleidoscope video quality assessment for short-form videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 3, 6

[25] Alex Mackin, Fan Zhang, and David R Bull. A study of subjective video quality at various frame rates. In 2015 IEEE International Conference on Image Processing (ICIP), pages 3407-3411. IEEE, 2015. 3

[26] Pavan C Madhusudana, Xiangxu Yu, Neil Birkbeck, Yilin Wang, Balu Adsumilli, and Alan C Bovik. Subjective and objective quality assessment of high frame rate videos. IEEE Access, 9:108069-108082, 2021. 3

[27] Xiongkuo Min, Huiyu Duan, Wei Sun, Yucheng Zhu, and Guangtao Zhai. Perceptual video quality assessment: A survey. arXiv preprint arXiv:2402.03413, 2024. 1

[28] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in the spatial domain. IEEE Transactions on image processing, 21(12):4695-4708, 2012.7

[29] Anish Mittal, Michele A Saad, and Alan C Bovik. A completely blind video integrity oracle. IEEE Transactions on Image Processing, 25(1):289-300, 2015. 3

[30] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a "completely blind" image quality analyzer. IEEE Signal processing letters, 20(3):209-212, 2012. 6, 7
[31] Anush Krishna Moorthy, Lark Kwon Choi, Alan Conrad Bovik, and Gustavo De Veciana. Video quality assessment on mobile devices: Subjective, behavioral and objective studies. IEEE Journal of Selected Topics in Signal Processing, 6(6):652-671, 2012. 3

[32] Rasoul Mohammadi Nasiri, Jiheng Wang, Abdul Rehman, Shiqi Wang, and Zhou Wang. Perceptual quality assessment of high frame rate video. In 2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP), pages 1-6. IEEE, 2015. 3

[33] Mikko Nuutinen, Toni Virtanen, Mikko Vaahteranoksa, Tero Vuori, Pirkko Oittinen, and Jukka Hkkinen. Cvd2014-a database for evaluating no-reference video quality assessment algorithms. IEEE Transactions on Image Processing, 25(7):3073-3086, 2016. 1, 3

[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 4

[35] Michele A Saad, Alan C Bovik, and Christophe Charrier. Blind prediction of natural video quality. IEEE Transactions on Image Processing, 23(3):1352-1365, 2014. 1, 3

[36] Kalpana Seshadrinathan, Rajiv Soundararajan, Alan Conrad Bovik, and Lawrence K Cormack. Study of subjective and objective quality assessment of video. IEEE transactions on Image Processing, 19(6):1427-1441, 2010. 3

[37] Zeina Sinno and Alan Conrad Bovik. Large-scale study of perceptual video quality. IEEE Transactions on Image Processing, 28(2):612-627, 2018. 1, 3

[38] Wei Sun, Xiongkuo Min, Wei Lu, and Guangtao Zhai. A deep learning based no-reference quality assessment model for ugc videos. In Proceedings of the 30th ACM International Conference on Multimedia, pages 856-865, 2022. 1, $3,4,5,6,7$

[39] Wei Sun, Tao Wang, Xiongkuo Min, Fuwang Yi, and Guangtao Zhai. Deep learning based full-reference and no-reference quality assessment models for compressed ugc videos. In 2021 IEEE International Conference on Multimedia \& Expo Workshops (ICMEW), pages 1-6. IEEE, 2021. 1

[40] Wei Sun, Wen Wen, Xiongkuo Min, Long Lan, Guangtao Zhai, and Kede Ma. Analysis of video quality datasets via design of minimalistic video quality models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1, $3,4,5,6$

[41] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105-6114. PMLR, 2019. 4

[42] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classification with channel-separated convolutional networks. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5552-5561, 2019. 4

[43] Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C Bovik. Ugc-vqa: Benchmarking blind video
quality assessment for user generated content. IEEE Transactions on Image Processing, 30:4449-4464, 2021. 1, 3, 6, 7

[44] Zhengzhong Tu, Xiangxu Yu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C Bovik. Rapique: Rapid and accurate video quality prediction of user generated content. IEEE Open Journal of Signal Processing, 2:425-440, 2021. $1,3,6,7$

[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2, 4

[46] VQEG. Final report from the Video Quality Experts Group on the validation of objective models of video quality assessment, 2000. 7

[47] Phong V Vu and Damon M Chandler. Vis 3: An algorithm for video quality assessment via analysis of spatial and spatiotemporal slices. Journal of Electronic Imaging, 23(1):013016-013016, 2014. 3

[48] Yilin Wang, Sasi Inguva, and Balu Adsumilli. Youtube ugc dataset for video compression research. In 2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP), pages 1-5. IEEE, 2019. 1, 3

[49] Yilin Wang, Junjie Ke, Hossein Talebi, Joong Gon Yim, Neil Birkbeck, Balu Adsumilli, Peyman Milanfar, and Feng Yang. Rich features for perceptual quality assessment of ugc videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1343513444, 2021. 1, 3, 4

[50] Wen Wen, Mu Li, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang, and Kede Ma. Modular blind video quality assessment. arXiv preprint arXiv:2402.19276, 2024. 2

[51] Haoning Wu, Chaofeng Chen, Jingwen Hou, Liang Liao, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Fastvqa: Efficient end-to-end video quality assessment with fragment sampling. In European conference on computer vision, pages 538-554. Springer, 2022. 1, 2, 3, 4, 6

[52] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20144-20154, 2023. 4, 6, 7

[53] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Towards explainable in-the-wild video quality assessment: A database and a language-prompted approach. In Proceedings of the 31st ACM International Conference on Multimedia. ACM, 2023. 3

[54] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023. 4

[55] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Kaixin Xu, Chunyi Li, Jingwen Hou, Guangtao Zhai, et al. Q-instruct: Improving low-level visual abilities for multi-modality foundation models. arXiv preprint arXiv:2311.06783, 2023. 4

[56] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, Yan Qiong, Min Xiongkuo, Zhai Guangtao, and Lin Weisi. Q-align: Teaching $1 \mathrm{mms}$ for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. 1, 2, 4, 7

[57] Fengchuang Xing, Yuan-Gen Wang, Hanpin Wang, Leida Li, and Guopu Zhu. Starvqa: Space-time attention for video quality assessment. In 2022 IEEE International Conference on Image Processing (ICIP), pages 2326-2330. IEEE, 2022. 1

[58] Fuwang Yi, Mianyi Chen, Wei Sun, Xiongkuo Min, Yuan Tian, and Guangtao Zhai. Attention based network for noreference ugc video quality assessment. In 2021 IEEE International Conference on Image Processing (ICIP), pages 1414-1418. IEEE, 2021. 1, 3, 4

[59] Zhenqiang Ying, Maniratnam Mandal, Deepti Ghadiyaram, and Alan Bovik. Patch-vq:'patching up'the video quality problem. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1401914029, 2021. 1, 3, 4, 6

[60] Xiangxu Yu, Neil Birkbeck, Yilin Wang, Christos G Bampis, Balu Adsumilli, and Alan C Bovik. Predicting the quality of compressed videos with pre-existing distortions. IEEE Transactions on Image Processing, 30:7511-7526, 2021. 3, 6

[61] Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang, and Kede Ma. Blind image quality assessment via visionlanguage correspondence: A multitask learning perspective. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14071-14081, 2023. 2

[62] Zicheng Zhang, Wei Lu, Wei Sun, Xiongkuo Min, Tao Wang, and Guangtao Zhai. Surveillance video quality assessment based on quality related retraining. In 2022 IEEE International Conference on Image Processing (ICIP), pages 4278-4282. IEEE, 2022. 2

[63] Zicheng Zhang, Wei Sun, Yucheng Zhu, Xiongkuo Min, Wei $\mathrm{Wu}$, Ying Chen, and Guangtao Zhai. Evaluating point cloud from moving camera videos: A no-reference metric. IEEE Transactions on Multimedia, 2023. 2

[64] Zicheng Zhang, Wei Wu, Wei Sun, Danyang Tu, Wei Lu, Xiongkuo Min, Ying Chen, and Guangtao Zhai. Md-vqa: Multi-dimensional quality assessment for ugc live videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1746-1755, 2023. 3, 6

[65] Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, and Guangtao Zhai. Geometry-aware video quality assessment for dynamic digital human. In 2023 IEEE International Conference on Image Processing (ICIP), pages 1365-1369. IEEE, 2023. 2


[^0]:    *These authors contributed equally to this work.

    ${ }^{\dagger}$ Corresponding authors.

[^1]:    ${ }^{1}$ We use the social media video to refer to UGC videos represented on social media applications like Kwai and TikTok.

[^2]:    ${ }^{2}$ https://codalab.lisn.upsaclay.fr/competitions/17638

</end of paper 3>


